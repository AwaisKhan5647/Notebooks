{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6397681-479d-4454-928a-1705ecd849c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mawais\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Processing files:   0%|                                                                      | 0/25380 [00:00<?, ?it/s]C:\\Users\\mawais\\AppData\\Local\\Temp\\ipykernel_17988\\2706483279.py:52: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(y=windowed_segment, sr=sr)[0]\n",
      "Processing files: 100%|███████████████████████████████████████████████████████| 25380/25380 [15:53:36<00:00,  2.25s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import torch\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize paths and parameters\n",
    "model_name = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Wav2Vec2Model.from_pretrained(model_name).to(device)\n",
    "# model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
    "\n",
    "train_audio_path = r\"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\train\\\\con_wav\\\\*.wav\"\n",
    "segment_label_path = r\"F:\\Awais_data\\Datasets\\PartialSpoof\\database_segment_labels\\database\\segment_labels\\train_seglab_0.16.npy\"\n",
    "utterance_label_path = r\"F:\\Awais_data\\Datasets\\PartialSpoof\\protocols\\PartialSpoof_LA_cm_protocols\\PartialSpoof.LA.cm.train.trl.txt\"\n",
    "save_path = r\"C:\\Notebooks\\rrl_source\\dataset_raw\\merge\\segment_features\\train\\\\\"\n",
    "\n",
    "window_size = 0.16  # Segment length in seconds\n",
    "hop_size = 0.15     # Frame shift in seconds\n",
    "extract_utterance_level = True  # Set to True for utterance-level, False for segment-level\n",
    "\n",
    "# Hann window function\n",
    "def hann_window(length):\n",
    "    return 0.5 * (1 - np.cos(2 * np.pi * np.arange(length) / (length - 1)))\n",
    "\n",
    "# Load segment-level or utterance-level labels\n",
    "def load_labels(label_file, utterance_level):\n",
    "    if utterance_level:\n",
    "        labels = {}\n",
    "        with open(label_file, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' ')\n",
    "                if len(parts) >= 5:\n",
    "                    file_id = parts[1].strip()\n",
    "                    label = parts[-1].strip()\n",
    "                    labels[file_id] = 0 if label == 'spoof' else 1\n",
    "    else:\n",
    "        labels = np.load(label_file, allow_pickle=True).item()\n",
    "    return labels\n",
    "\n",
    "# Handcrafted feature extraction\n",
    "def extract_handcrafted_features(segment, sr):\n",
    "    windowed_segment = segment * hann_window(len(segment))\n",
    "    mfcc = librosa.feature.mfcc(y=windowed_segment, sr=sr, n_mfcc=13).mean(axis=1)\n",
    "    delta_mfcc = librosa.feature.delta(mfcc)\n",
    "    tempo = librosa.beat.tempo(y=windowed_segment, sr=sr)[0]\n",
    "    chroma = librosa.feature.chroma_stft(y=windowed_segment, sr=sr).mean(axis=1)\n",
    "    zcr = librosa.feature.zero_crossing_rate(windowed_segment).mean()\n",
    "    energy = librosa.feature.rms(y=windowed_segment).mean()\n",
    "    pitches, _ = librosa.core.piptrack(y=windowed_segment, sr=sr)\n",
    "    pitch = np.mean(pitches[pitches > 0]) if len(pitches[pitches > 0]) > 0 else 0\n",
    "    tempogram = librosa.feature.tempogram(y=windowed_segment, sr=sr).mean(axis=1)[1:]\n",
    "    downsampled_tempogram = tempogram[::int(np.ceil(len(tempogram) / 18))]\n",
    "    features = np.concatenate((mfcc, delta_mfcc, [tempo], chroma, [zcr], [energy], [pitch], downsampled_tempogram))\n",
    "    return features\n",
    "\n",
    "def extract_wav2vec_features(segment, sr): \n",
    "    if len(segment.shape) > 1:   # Ensure the segment is a 1D array and not a 2D array\n",
    "        segment = segment.flatten()  # Flatten it to 1D if needed\n",
    "    \n",
    "    inputs = feature_extractor(segment, sampling_rate=sr, return_tensors=\"pt\", padding=False)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()} \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)  # Extract the last hidden state and return it\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "# Main processing function\n",
    "def process_dataset(audio_files, labels, utterance_level):\n",
    "    all_features = []\n",
    "    for audio_file in tqdm(audio_files, desc=\"Processing files\"):\n",
    "        file_id = os.path.basename(audio_file).replace('.wav', '')\n",
    "        if file_id not in labels:\n",
    "            continue\n",
    "\n",
    "        y, sr = librosa.load(audio_file, sr=16000)\n",
    "        if utterance_level:\n",
    "            label = labels[file_id]\n",
    "            handcrafted_features = extract_handcrafted_features(y, sr)\n",
    "            wav2vec_features = extract_wav2vec_features(y, sr)\n",
    "            combined_features = np.concatenate(([file_id], handcrafted_features, wav2vec_features, [label]))\n",
    "            all_features.append(combined_features)\n",
    "        else:\n",
    "            segment_labels = labels[file_id]\n",
    "            segment_length = int(window_size * sr)\n",
    "            hop_length = int(hop_size * sr)\n",
    "\n",
    "            for i, seg_label in enumerate(segment_labels):\n",
    "                start = i * hop_length\n",
    "                end = start + segment_length\n",
    "                segment = (\n",
    "                    np.pad(y[start:], (0, segment_length - len(y[start:])), mode='edge')\n",
    "                if end > len(y)\n",
    "                else y[start:end]\n",
    "                )\n",
    "\n",
    "                # Extract features\n",
    "                handcrafted_features = extract_handcrafted_features(segment, sr)\n",
    "                wav2vec_features = extract_wav2vec_features(segment, sr)\n",
    "                combined_features = np.concatenate(([file_id], handcrafted_features, wav2vec_features, [seg_label]))\n",
    "                all_features.append(combined_features)\n",
    "\n",
    "    return np.array(all_features)\n",
    "\n",
    "# Load labels and process dataset\n",
    "labels = load_labels(utterance_label_path if extract_utterance_level else segment_label_path, extract_utterance_level)\n",
    "audio_files = glob(train_audio_path)\n",
    "# audio_files = audio_files[:1]\n",
    "features = process_dataset(audio_files, labels, extract_utterance_level)\n",
    "\n",
    "# Save features in parts\n",
    "part_size = 80000\n",
    "for i in range(0, len(features), part_size):\n",
    "    part_features = features[i:i + part_size]\n",
    "    part_name = f\"{save_path}train_{'utterance' if extract_utterance_level else 'segment'}_merged_part_{i // part_size + 1}.csv\"\n",
    "    pd.DataFrame(part_features).to_csv(part_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71f351e-d50d-4233-b50c-7923dd96420e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
