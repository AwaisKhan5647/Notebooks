{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b5c5b3-e837-42b3-86db-7b8268fc1d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def convert_csv_to_data(csv_file_path):\n",
    "    # Load the CSV file into a DataFrame (no header assumed here)\n",
    "    df = pd.read_csv(csv_file_path, header=None)  \n",
    "    \n",
    "    # Generate the output .data file path\n",
    "    data_file_path = csv_file_path.replace('.csv', '.data')\n",
    "    \n",
    "    # Format columns based on conditions\n",
    "    formatted_columns = []\n",
    "    for col in df.columns:\n",
    "        if col >= 1024 and col <= 1025:\n",
    "            # Retain full precision for columns 44 to 59 (no formatting change)\n",
    "            formatted_column = df[col]\n",
    "        else:\n",
    "            # Apply 3 decimal places for other columns\n",
    "            formatted_column = df[col].map(lambda x: '%.3f' % x if pd.notnull(x) else '')\n",
    "        \n",
    "        formatted_columns.append(formatted_column)\n",
    "    \n",
    "    # Concatenate all formatted columns back into a DataFrame\n",
    "    formatted_df = pd.concat(formatted_columns, axis=1)\n",
    "    \n",
    "    # Save the formatted DataFrame to a .data file\n",
    "    formatted_df.to_csv(data_file_path, index=False, header=False, sep=',', quoting=None)\n",
    "    \n",
    "    print(f\"Converted {csv_file_path} to {data_file_path}\")\n",
    "\n",
    "# Example usage\n",
    "csv_file = \"C:\\\\Notebooks\\\\rrl_source\\\\dataset_raw\\\\train_segment_W2V2_full.csv\"  # Path to the CSV file\n",
    "convert_csv_to_data(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2da8ff-35a8-4667-9c45-15147b6a76f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory path where you want to save the file\n",
    "directory_path = \"C:\\\\Notebooks\\\\rrl_source\\\\dataset\\\\\"  # Change this to your desired path\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "# Define the full file path\n",
    "file_path = os.path.join(directory_path, \"train_segment_W2V2_full.INFO\")\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path, \"w\") as file:\n",
    "    # Write 1024 continuous features\n",
    "    for i in range(1, 1025):\n",
    "        file.write(f\"{i} continuous\\n\")\n",
    "    \n",
    "    # Write the remaining lines\n",
    "    file.write(\"class discrete\\n\")\n",
    "    file.write(\"LABEL_POS -1\\n\")\n",
    "\n",
    "print(f\"{file_path} has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b14f650-4ab2-44de-883a-1979612d0aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved C:\\Notebooks\\rrl_source\\dataset_raw\\train_segment_Wav2Vec2_part1.csv with 80000 samples.\n",
      "Saved C:\\Notebooks\\rrl_source\\dataset_raw\\train_segment_Wav2Vec2_part2.csv with 80000 samples.\n",
      "Saved C:\\Notebooks\\rrl_source\\dataset_raw\\train_segment_Wav2Vec2_part3.csv with 80000 samples.\n",
      "Finished processing and saving parts.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the large CSV file\n",
    "file_path = r\"C:\\Notebooks\\rrl_source\\dataset_raw\\train_segment_Wav2Vec2_1024.csv\"\n",
    "\n",
    "# Parameters for chunking\n",
    "chunk_size = 150000  # Number of rows to read in each chunk, adjust if needed\n",
    "output_size = 80000  # Total rows per output file\n",
    "samples_per_label = output_size // 2  # Samples per label (40,000 for each label)\n",
    "\n",
    "# Initialize counters\n",
    "part_num = 1\n",
    "\n",
    "# Create an iterator for reading the CSV file in chunks\n",
    "csv_iterator = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "\n",
    "for chunk in csv_iterator:\n",
    "    # Drop any rows with NaN in the 'Label' column (if present)\n",
    "    chunk = chunk.dropna(subset=['Label'])\n",
    "    \n",
    "    # Separate data by label\n",
    "    ones = chunk[chunk['Label'] == 1]\n",
    "    zeros = chunk[chunk['Label'] == 0]\n",
    "    \n",
    "    # Check if we have enough samples for both labels\n",
    "    if len(ones) < samples_per_label or len(zeros) < samples_per_label:\n",
    "        continue  # Skip this chunk if it doesn't have enough samples\n",
    "    \n",
    "    # Sample 40,000 from each label to create a balanced set\n",
    "    balanced_chunk = pd.concat([ones.sample(samples_per_label, random_state=42),\n",
    "                                zeros.sample(samples_per_label, random_state=42)])\n",
    "    \n",
    "    # Shuffle the balanced data\n",
    "    balanced_chunk = balanced_chunk.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Save to a new CSV file\n",
    "    output_file = rf\"C:\\Notebooks\\rrl_source\\dataset_raw\\train_segment_Wav2Vec2_part{part_num}.csv\"\n",
    "    balanced_chunk.to_csv(output_file, index=False)\n",
    "    print(f\"Saved {output_file} with {len(balanced_chunk)} samples.\")\n",
    "    \n",
    "    part_num += 1\n",
    "\n",
    "print(\"Finished processing and saving parts.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd4cb3de-a77c-4f15-be34-5f2bec5755ed",
   "metadata": {},
   "source": [
    "DATA and INFO file generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef75fee8-a462-4243-a07e-17321d3d8786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted C:\\Notebooks\\rrl_source\\Spectnet_model_Halftruth_embedding\\feature_emb_256_norm.csv to C:\\Notebooks\\rrl_source\\Spectnet_model_Halftruth_embedding\\feature_emb_256_norm.data\n",
      "C:\\Notebooks\\rrl_source\\Spectnet_model_Halftruth_embedding\\feature_emb_256_norm.INFO has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def convert_csv_to_data_and_info(csv_file_path):\n",
    "    # Load CSV file, removing header if it exists\n",
    "    df = pd.read_csv(csv_file_path, header=0)\n",
    "    \n",
    "    # Remove FILEID column if present\n",
    "    if 'FILEID' in df.columns[0] or df.columns[0].lower() == 'fileid' or df.columns[0].lower() == 'FileID':\n",
    "        df = df.iloc[:, 1:]\n",
    "        \n",
    "    # Remove polarity column if present in the last position\n",
    "    if df.columns[-1].lower() in ['polarity', 'positive', 'negative']:\n",
    "        df = df.iloc[:, :-1]\n",
    "    \n",
    "    # Generate the output .data file path\n",
    "    data_file_path = csv_file_path.replace('.csv', '.data')\n",
    "    \n",
    "    # Format all feature columns to 3 decimal places, ensure label is integer (0 or 1)\n",
    "    formatted_df = df.copy()\n",
    "    for col in df.columns[:-1]:  # Apply formatting to all except the label column\n",
    "        formatted_df[col] = df[col].map(lambda x: '%.3f' % x if pd.notnull(x) else '')\n",
    "    \n",
    "    # Ensure the label column is integer without floating point\n",
    "    formatted_df.iloc[:, -1] = df.iloc[:, -1].astype(int)\n",
    "    \n",
    "    # Save the formatted DataFrame to a .data file\n",
    "    formatted_df.to_csv(data_file_path, index=False, header=False, sep=',', float_format='%.3f')\n",
    "    print(f\"Converted {csv_file_path} to {data_file_path}\")\n",
    "    \n",
    "    # Generate the .info file\n",
    "    num_features = len(df.columns) - 1  # Subtract label column\n",
    "    info_file_path = csv_file_path.replace('.csv', '.INFO')\n",
    "    with open(info_file_path, \"w\") as file:\n",
    "        # Write continuous types for all features\n",
    "        for i in range(1, num_features + 1):\n",
    "            file.write(f\"{i} continuous\\n\")\n",
    "        # Write the label type as discrete\n",
    "        file.write(\"class discrete\\n\")\n",
    "        file.write(\"LABEL_POS -1\\n\")\n",
    "    \n",
    "    print(f\"{info_file_path} has been created successfully.\")\n",
    "\n",
    "# Example usage\n",
    "csv_file = \"C:\\\\Notebooks\\\\rrl_source\\\\Spectnet_model_Halftruth_embedding\\\\feature_emb_256_norm.csv\"  # Path to the CSV file\n",
    "convert_csv_to_data_and_info(csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2016002-5955-4b24-8b0f-0dc50b5b2439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 files to merge: ['F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Specnet\\\\train\\\\embeddings\\\\extracted_train_segment_merged_part_1.csv', 'F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Specnet\\\\train\\\\embeddings\\\\extracted_train_segment_merged_part_2.csv', 'F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Specnet\\\\train\\\\embeddings\\\\extracted_train_segment_merged_part_3.csv', 'F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Specnet\\\\train\\\\embeddings\\\\extracted_train_segment_merged_part_4.csv', 'F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Specnet\\\\train\\\\embeddings\\\\extracted_train_segment_merged_part_5.csv', 'F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Specnet\\\\train\\\\embeddings\\\\extracted_train_segment_merged_part_6.csv', 'F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Specnet\\\\train\\\\embeddings\\\\extracted_train_segment_merged_part_7.csv']\n",
      "Merged data shape: (558407, 259)\n",
      "Merged data saved temporarily to F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\train_segment_specnet_emb_merged.csv\n",
      "Removing FILEID column: fileid\n",
      "Converted merged data to F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\train_segment_specnet_emb.data\n",
      "F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\train_segment_specnet_emb.INFO has been created successfully.\n",
      "Temporary merged CSV removed: F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\train_segment_specnet_emb_merged.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Function to process and convert the merged CSV to .data and .info files\n",
    "def convert_csv_to_data_and_info(csv_file_path, output_prefix):\n",
    "    # Load the merged CSV file\n",
    "    df = pd.read_csv(csv_file_path, header=0)\n",
    "    \n",
    "    # Remove the FILEID column if present\n",
    "    if df.columns[0].lower() in ['fileid', 'file_id']:\n",
    "        print(f\"Removing FILEID column: {df.columns[0]}\")\n",
    "        df = df.iloc[:, 1:]\n",
    "        \n",
    "    # Generate the output .data file path\n",
    "    data_file_path = f\"{output_prefix}.data\"\n",
    "    \n",
    "    # Format all feature columns to 3 decimal places, ensure label is integer (0 or 1)\n",
    "    formatted_df = df.copy()\n",
    "    for col in df.columns[:-1]:  # Format all except the label column\n",
    "        formatted_df[col] = df[col].map(lambda x: f\"{x:.3f}\" if pd.notnull(x) else '')\n",
    "    \n",
    "    # Ensure the label column is integer\n",
    "    formatted_df.iloc[:, -1] = df.iloc[:, -1].astype(int)\n",
    "    \n",
    "    # Save the formatted DataFrame to a .data file\n",
    "    formatted_df.to_csv(data_file_path, index=False, header=False, sep=',', float_format='%.3f')\n",
    "    print(f\"Converted merged data to {data_file_path}\")\n",
    "    \n",
    "    # Generate the .info file\n",
    "    num_features = len(df.columns) - 1  # Subtract label column\n",
    "    info_file_path = f\"{output_prefix}.INFO\"\n",
    "    with open(info_file_path, \"w\") as file:\n",
    "        # Write continuous types for all features\n",
    "        for i in range(1, num_features + 1):\n",
    "            file.write(f\"{i} continuous\\n\")\n",
    "        # Write the label type as discrete\n",
    "        file.write(\"class discrete\\n\")\n",
    "        file.write(\"LABEL_POS -1\\n\")\n",
    "    \n",
    "    print(f\"{info_file_path} has been created successfully.\")\n",
    "\n",
    "# Merging all cleaned parts\n",
    "def merge_cleaned_csv_files(input_dir, output_prefix):\n",
    "    # Find all cleaned CSV files (cleaned_part_1.csv to cleaned_part_12.csv)\n",
    "    csv_files = [os.path.join(input_dir, f\"extracted_train_segment_merged_part_{i}.csv\") for i in range(1, 13)]\n",
    "    \n",
    "    # Verify that files exist\n",
    "    existing_files = [file for file in csv_files if os.path.exists(file)]\n",
    "    if not existing_files:\n",
    "        raise FileNotFoundError(\"No cleaned CSV files found in the specified directory.\")\n",
    "    print(f\"Found {len(existing_files)} files to merge: {existing_files}\")\n",
    "    \n",
    "    # Read and merge all CSVs\n",
    "    data_frames = [pd.read_csv(file) for file in existing_files]\n",
    "    merged_df = pd.concat(data_frames, ignore_index=True)\n",
    "    print(f\"Merged data shape: {merged_df.shape}\")\n",
    "    \n",
    "    # Save the merged file temporarily\n",
    "    merged_csv_path = f\"{output_prefix}_merged.csv\"\n",
    "    merged_df.to_csv(merged_csv_path, index=False)\n",
    "    print(f\"Merged data saved temporarily to {merged_csv_path}\")\n",
    "    \n",
    "    # Process the merged CSV to generate .data and .info files\n",
    "    convert_csv_to_data_and_info(merged_csv_path, output_prefix)\n",
    "\n",
    "    # Remove the temporary merged CSV\n",
    "    os.remove(merged_csv_path)\n",
    "    print(f\"Temporary merged CSV removed: {merged_csv_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_directory = r\"F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\"  # Directory with cleaned parts\n",
    "output_file_prefix = os.path.join(input_directory, \"train_segment_specnet_emb\")  # Prefix for .data and .info files\n",
    "\n",
    "merge_cleaned_csv_files(input_directory, output_file_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2aefcae-56dc-4197-960d-57fb16e6be12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leading commas removed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define the file path\n",
    "file_path = r\"C:\\Notebooks\\rrl_source\\dataset\\train_segment_specnet_emb.data\"\n",
    "\n",
    "# Open the file, process, and save the updated content\n",
    "with open(file_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Remove the leading comma from each line\n",
    "updated_lines = [line.lstrip(',') for line in lines]\n",
    "\n",
    "# Write the updated lines back to the file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.writelines(updated_lines)\n",
    "\n",
    "print(\"Leading commas removed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5159b047-aa31-41fb-9441-337a90a0035d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mawais\\AppData\\Local\\Temp\\ipykernel_6432\\2845746301.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  effective_features['Label'] = labels.values  # Add label as the last column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective features extraction and saving completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "save_path = r\"C:\\\\Notebooks\\\\rrl_source\\\\dataset_raw\\\\merge\\\\segment\\\\\"\n",
    "extract_utterance_level = False\n",
    "\n",
    "# Load the features file\n",
    "features_file = f\"{save_path}train_{'utterance' if extract_utterance_level else 'segment'}_merged_part_1.csv\"\n",
    "data = pd.read_csv(features_file)\n",
    "\n",
    "# Exclude the FileID column and separate features and labels\n",
    "file_ids = data.iloc[:, 0]  # First column with audio names (FileID)\n",
    "features = data.iloc[:, 1:-1]  # All columns except the first (FileID) and last (label)\n",
    "labels = data.iloc[:, -1].astype(int)  # The last column is assumed to be the label\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Perform feature selection based on mutual information\n",
    "mi_scores = mutual_info_classif(features_scaled, labels)\n",
    "feature_importance = sorted(enumerate(mi_scores), key=lambda x: x[1], reverse=True)\n",
    "selected_feature_indices = [idx for idx, score in feature_importance if score > 0.01]  # Adjust threshold as needed\n",
    "\n",
    "# Extract effective features\n",
    "effective_features = features.iloc[:, selected_feature_indices]\n",
    "\n",
    "# Add the FileID and label columns back to the effective features\n",
    "effective_features.insert(0, 'FileID', file_ids)  # Add FileID as the first column\n",
    "effective_features['Label'] = labels.values  # Add label as the last column\n",
    "\n",
    "# Save the effective features to a new CSV file\n",
    "effective_features_save_name = f\"{save_path}effective.csv\"\n",
    "effective_features.to_csv(effective_features_save_name, index=False)\n",
    "\n",
    "print(\"Effective features extraction and saving completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72cf31cc-4d70-441a-82e1-42ae599eeae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset saved to C:\\Notebooks\\rrl_source\\dataset_raw\\merge\\segment\\train_segment_merged_part_1_balance.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def balance_dataset(input_csv, output_csv='C:\\\\Notebooks\\\\rrl_source\\\\dataset_raw\\\\merge\\\\segment\\\\train_segment_merged_part_1_balance.csv'):\n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(input_csv)\n",
    "    \n",
    "    # Check if the label column exists and identify the classes\n",
    "    if 'Label' not in data.columns:\n",
    "        raise ValueError(\"The CSV file must contain a 'label' column with 1 and 0 as classes.\")\n",
    "    \n",
    "    # Separate the data based on class labels\n",
    "    class_0 = data[data['Label'] == 0]\n",
    "    class_1 = data[data['Label'] == 1]\n",
    "    \n",
    "    # Determine the smaller class size\n",
    "    min_class_size = min(len(class_0), len(class_1))\n",
    "    \n",
    "    # Randomly sample to balance the classes\n",
    "    balanced_class_0 = class_0.sample(n=min_class_size, random_state=42)\n",
    "    balanced_class_1 = class_1.sample(n=min_class_size, random_state=42)\n",
    "    \n",
    "    # Concatenate the balanced classes\n",
    "    balanced_data = pd.concat([balanced_class_0, balanced_class_1])\n",
    "    \n",
    "    # Shuffle the data to mix the classes\n",
    "    balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Save the balanced data to a new CSV file\n",
    "    balanced_data.to_csv(output_csv, index=False)\n",
    "    print(f\"Balanced dataset saved to {output_csv}\")\n",
    "\n",
    "# Usage example\n",
    "balance_dataset('C:\\\\Notebooks\\\\rrl_source\\\\dataset_raw\\\\merge\\\\segment\\\\train_segment_merged_part_1.csv')  # Replace 'input.csv' with the path to your CSV file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1beabd6-375a-4460-b9e3-ff84e896bdef",
   "metadata": {},
   "source": [
    "data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cf66a28-a57c-4794-bb9a-ad6fe47c9ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 files to process.\n",
      "Merged data shape: (558407, 259)\n",
      "Number of columns with all zero values: 179\n",
      "Columns with all zero values: ['feature_0', 'feature_1', 'feature_2', 'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9', 'feature_10', 'feature_12', 'feature_15', 'feature_17', 'feature_20', 'feature_21', 'feature_23', 'feature_24', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_40', 'feature_42', 'feature_44', 'feature_46', 'feature_48', 'feature_49', 'feature_50', 'feature_52', 'feature_54', 'feature_55', 'feature_57', 'feature_58', 'feature_59', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_73', 'feature_74', 'feature_76', 'feature_77', 'feature_79', 'feature_80', 'feature_82', 'feature_83', 'feature_84', 'feature_85', 'feature_87', 'feature_88', 'feature_89', 'feature_90', 'feature_91', 'feature_93', 'feature_95', 'feature_97', 'feature_99', 'feature_102', 'feature_103', 'feature_104', 'feature_106', 'feature_107', 'feature_108', 'feature_109', 'feature_112', 'feature_113', 'feature_114', 'feature_115', 'feature_116', 'feature_117', 'feature_118', 'feature_119', 'feature_122', 'feature_123', 'feature_124', 'feature_125', 'feature_126', 'Unnamed: 130', 'Unnamed: 131', 'Unnamed: 132', 'Unnamed: 135', 'Unnamed: 136', 'Unnamed: 137', 'Unnamed: 140', 'Unnamed: 142', 'Unnamed: 144', 'Unnamed: 145', 'Unnamed: 146', 'Unnamed: 147', 'Unnamed: 149', 'Unnamed: 152', 'Unnamed: 153', 'Unnamed: 154', 'Unnamed: 155', 'Unnamed: 156', 'Unnamed: 157', 'Unnamed: 159', 'Unnamed: 161', 'Unnamed: 162', 'Unnamed: 163', 'Unnamed: 164', 'Unnamed: 165', 'Unnamed: 166', 'Unnamed: 168', 'Unnamed: 170', 'Unnamed: 171', 'Unnamed: 173', 'Unnamed: 174', 'Unnamed: 178', 'Unnamed: 179', 'Unnamed: 181', 'Unnamed: 184', 'Unnamed: 185', 'Unnamed: 186', 'Unnamed: 187', 'Unnamed: 189', 'Unnamed: 190', 'Unnamed: 191', 'Unnamed: 192', 'Unnamed: 193', 'Unnamed: 195', 'Unnamed: 198', 'Unnamed: 199', 'Unnamed: 200', 'Unnamed: 201', 'Unnamed: 202', 'Unnamed: 203', 'Unnamed: 204', 'Unnamed: 206', 'Unnamed: 207', 'Unnamed: 209', 'Unnamed: 210', 'Unnamed: 211', 'Unnamed: 212', 'Unnamed: 213', 'Unnamed: 214', 'Unnamed: 215', 'Unnamed: 216', 'Unnamed: 217', 'Unnamed: 218', 'Unnamed: 219', 'Unnamed: 220', 'Unnamed: 222', 'Unnamed: 223', 'Unnamed: 225', 'Unnamed: 226', 'Unnamed: 227', 'Unnamed: 230', 'Unnamed: 231', 'Unnamed: 233', 'Unnamed: 234', 'Unnamed: 236', 'Unnamed: 237', 'Unnamed: 240', 'Unnamed: 243', 'Unnamed: 244', 'Unnamed: 245', 'Unnamed: 246', 'Unnamed: 247', 'Unnamed: 248', 'Unnamed: 249', 'Unnamed: 250', 'Unnamed: 253', 'Unnamed: 255']\n",
      "Number of duplicate columns: 178\n",
      "Duplicate columns: ['feature_103', 'Unnamed: 226', 'feature_57', 'feature_106', 'Unnamed: 202', 'Unnamed: 170', 'Unnamed: 246', 'feature_32', 'feature_113', 'feature_80', 'Unnamed: 165', 'feature_66', 'feature_7', 'feature_36', 'Unnamed: 250', 'feature_69', 'Unnamed: 236', 'Unnamed: 206', 'Unnamed: 145', 'Unnamed: 199', 'feature_46', 'Unnamed: 164', 'feature_97', 'feature_65', 'Unnamed: 216', 'Unnamed: 248', 'feature_88', 'feature_61', 'Unnamed: 132', 'feature_37', 'feature_1', 'Unnamed: 144', 'feature_126', 'Unnamed: 191', 'feature_4', 'Unnamed: 178', 'feature_58', 'feature_124', 'feature_5', 'feature_108', 'Unnamed: 255', 'feature_17', 'Unnamed: 217', 'Unnamed: 156', 'feature_2', 'Unnamed: 159', 'Unnamed: 157', 'Unnamed: 203', 'feature_115', 'Unnamed: 219', 'feature_64', 'Unnamed: 200', 'Unnamed: 240', 'feature_82', 'Unnamed: 166', 'Unnamed: 153', 'Unnamed: 247', 'feature_23', 'feature_40', 'feature_71', 'feature_79', 'Unnamed: 136', 'Unnamed: 210', 'Unnamed: 204', 'Unnamed: 222', 'feature_122', 'feature_49', 'feature_76', 'feature_34', 'Unnamed: 149', 'feature_67', 'Unnamed: 168', 'feature_50', 'Unnamed: 249', 'feature_27', 'feature_30', 'Unnamed: 147', 'feature_118', 'feature_91', 'feature_74', 'feature_90', 'feature_15', 'Unnamed: 171', 'Unnamed: 135', 'feature_119', 'Unnamed: 187', 'feature_112', 'Unnamed: 227', 'feature_77', 'Unnamed: 223', 'feature_83', 'feature_29', 'Unnamed: 195', 'Unnamed: 214', 'Unnamed: 181', 'Unnamed: 198', 'Unnamed: 184', 'Unnamed: 163', 'feature_73', 'Unnamed: 192', 'feature_89', 'feature_125', 'Unnamed: 189', 'feature_99', 'feature_70', 'feature_12', 'feature_44', 'feature_28', 'feature_8', 'feature_85', 'feature_35', 'Unnamed: 146', 'Unnamed: 173', 'Unnamed: 130', 'Unnamed: 207', 'feature_68', 'Unnamed: 190', 'Unnamed: 161', 'feature_20', 'Unnamed: 193', 'Unnamed: 201', 'feature_117', 'Unnamed: 218', 'feature_63', 'feature_123', 'Unnamed: 152', 'Unnamed: 211', 'feature_21', 'feature_104', 'feature_38', 'feature_48', 'Unnamed: 245', 'Unnamed: 154', 'Unnamed: 140', 'Unnamed: 142', 'feature_9', 'Unnamed: 162', 'feature_107', 'feature_102', 'Unnamed: 179', 'feature_59', 'feature_52', 'Unnamed: 137', 'feature_24', 'feature_62', 'feature_31', 'feature_114', 'feature_87', 'feature_95', 'Unnamed: 155', 'feature_54', 'Unnamed: 212', 'Unnamed: 186', 'feature_116', 'Unnamed: 243', 'Unnamed: 225', 'feature_10', 'Unnamed: 230', 'feature_93', 'Unnamed: 231', 'Unnamed: 220', 'Unnamed: 131', 'feature_109', 'feature_6', 'feature_33', 'Unnamed: 237', 'Unnamed: 253', 'Unnamed: 215', 'Unnamed: 209', 'Unnamed: 213', 'feature_42', 'feature_84', 'Unnamed: 185', 'Unnamed: 234', 'Unnamed: 244', 'Unnamed: 174', 'Unnamed: 233', 'feature_55']\n",
      "Columns 'file_id' or 'label' not found for duplicate detection.\n",
      "Data summary:\n",
      "       Unnamed: 1  feature_0  feature_1  feature_2      feature_3  feature_4  \\\n",
      "count         0.0   558407.0   558407.0   558407.0  558407.000000   558407.0   \n",
      "mean          NaN        0.0        0.0        0.0       0.323973        0.0   \n",
      "std           NaN        0.0        0.0        0.0       0.217819        0.0   \n",
      "min           NaN        0.0        0.0        0.0       0.000000        0.0   \n",
      "25%           NaN        0.0        0.0        0.0       0.128300        0.0   \n",
      "50%           NaN        0.0        0.0        0.0       0.313829        0.0   \n",
      "75%           NaN        0.0        0.0        0.0       0.527077        0.0   \n",
      "max           NaN        0.0        0.0        0.0       0.971637        0.0   \n",
      "\n",
      "       feature_5  feature_6  feature_7  feature_8  ...  Unnamed: 249  \\\n",
      "count   558407.0   558407.0   558407.0   558407.0  ...      558407.0   \n",
      "mean         0.0        0.0        0.0        0.0  ...           0.0   \n",
      "std          0.0        0.0        0.0        0.0  ...           0.0   \n",
      "min          0.0        0.0        0.0        0.0  ...           0.0   \n",
      "25%          0.0        0.0        0.0        0.0  ...           0.0   \n",
      "50%          0.0        0.0        0.0        0.0  ...           0.0   \n",
      "75%          0.0        0.0        0.0        0.0  ...           0.0   \n",
      "max          0.0        0.0        0.0        0.0  ...           0.0   \n",
      "\n",
      "       Unnamed: 250   Unnamed: 251   Unnamed: 252  Unnamed: 253  \\\n",
      "count      558407.0  558407.000000  558407.000000      558407.0   \n",
      "mean            0.0       0.127659       0.002761           0.0   \n",
      "std             0.0       0.157271       0.013913           0.0   \n",
      "min             0.0       0.000000       0.000000           0.0   \n",
      "25%             0.0       0.000000       0.000000           0.0   \n",
      "50%             0.0       0.016857       0.000000           0.0   \n",
      "75%             0.0       0.258868       0.000000           0.0   \n",
      "max             0.0       0.587824       0.190664           0.0   \n",
      "\n",
      "        Unnamed: 254  Unnamed: 255   Unnamed: 256   Unnamed: 257  \\\n",
      "count  558407.000000      558407.0  558407.000000  558407.000000   \n",
      "mean        0.000050           0.0       0.886449       0.452515   \n",
      "std         0.003363           0.0       0.173243       0.170584   \n",
      "min         0.000000           0.0       0.000000       0.000000   \n",
      "25%         0.000000           0.0       0.772998       0.336285   \n",
      "50%         0.000000           0.0       0.900489       0.484279   \n",
      "75%         0.000000           0.0       1.000634       0.570126   \n",
      "max         0.412174           0.0       1.538716       0.935525   \n",
      "\n",
      "               label  \n",
      "count  558407.000000  \n",
      "mean        0.456531  \n",
      "std         0.498107  \n",
      "min         0.000000  \n",
      "25%         0.000000  \n",
      "50%         0.000000  \n",
      "75%         1.000000  \n",
      "max         1.000000  \n",
      "\n",
      "[8 rows x 258 columns]\n",
      "Cleaned data shape: (558407, 80)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mawais\\AppData\\Local\\Temp\\ipykernel_8888\\3134352016.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_data[\"label\"] = label_column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\cleaned_part_1.csv\n",
      "Saved F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\cleaned_part_2.csv\n",
      "Saved F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\cleaned_part_3.csv\n",
      "Saved F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\cleaned_part_4.csv\n",
      "Saved F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\cleaned_part_5.csv\n",
      "Saved F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\cleaned_part_6.csv\n",
      "Saved F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\cleaned_part_7.csv\n",
      "Saved F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\cleaned_part_8.csv\n",
      "Saved F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\cleaned_part_9.csv\n",
      "Saved F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\cleaned_part_10.csv\n",
      "Saved F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\cleaned_part_11.csv\n",
      "Saved F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\cleaned_part_12.csv\n",
      "Data cleaning and saving completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define the directory where CSV files are stored\n",
    "csv_dir =  r\"F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_path = csv_dir  # Save in the same directory\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Read all CSV files\n",
    "all_files = glob.glob(os.path.join(csv_dir, \"*.csv\"))\n",
    "print(f\"Found {len(all_files)} files to process.\")\n",
    "\n",
    "# Combine all CSVs into a single DataFrame\n",
    "data_frames = []\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    data_frames.append(df)\n",
    "merged_data = pd.concat(data_frames, ignore_index=True)\n",
    "print(f\"Merged data shape: {merged_data.shape}\")\n",
    "\n",
    "# ---- Data Analysis ----\n",
    "# Identify columns with all zero values\n",
    "zero_columns = [col for col in merged_data.columns if (merged_data[col] == 0).all()]\n",
    "print(f\"Number of columns with all zero values: {len(zero_columns)}\")\n",
    "if zero_columns:\n",
    "    print(f\"Columns with all zero values: {zero_columns}\")\n",
    "\n",
    "# Check for duplicate columns (columns with identical values)\n",
    "duplicate_columns = set()\n",
    "for i, col1 in enumerate(merged_data.columns):\n",
    "    for col2 in merged_data.columns[i + 1:]:\n",
    "        if merged_data[col1].equals(merged_data[col2]):\n",
    "            duplicate_columns.add(col2)\n",
    "print(f\"Number of duplicate columns: {len(duplicate_columns)}\")\n",
    "if duplicate_columns:\n",
    "    print(f\"Duplicate columns: {list(duplicate_columns)}\")\n",
    "\n",
    "# Identify duplicate rows based on all features except 'file_id' and 'label'\n",
    "if \"file_id\" in merged_data.columns and \"label\" in merged_data.columns:\n",
    "    duplicates = merged_data[merged_data.duplicated(subset=merged_data.columns.difference([\"file_id\", \"label\"]), keep=False)]\n",
    "    print(f\"Number of duplicate rows: {duplicates.shape[0]}\")\n",
    "    if not duplicates.empty:\n",
    "        print(\"Sample duplicate rows (file_id and label):\")\n",
    "        print(duplicates[[\"file_id\", \"label\"]].head(10))  # Display the first 10 duplicate rows\n",
    "else:\n",
    "    print(\"Columns 'file_id' or 'label' not found for duplicate detection.\")\n",
    "\n",
    "# Summary of data\n",
    "print(\"Data summary:\")\n",
    "print(merged_data.describe())\n",
    "\n",
    "# ---- Data Cleaning ----\n",
    "# Remove columns with all zero values\n",
    "non_zero_columns = merged_data.loc[:, (merged_data != 0).any(axis=0)]\n",
    "cleaned_data = non_zero_columns\n",
    "\n",
    "# Ensure the label column is at the end\n",
    "if \"label\" in cleaned_data.columns:\n",
    "    label_column = cleaned_data.pop(\"label\")\n",
    "    cleaned_data[\"label\"] = label_column\n",
    "\n",
    "print(f\"Cleaned data shape: {cleaned_data.shape}\")\n",
    "\n",
    "# ---- Save Cleaned Data in Parts ----\n",
    "part_size = 50000\n",
    "for i in range(0, len(cleaned_data), part_size):\n",
    "    part_data = cleaned_data.iloc[i:i + part_size]\n",
    "    part_name = os.path.join(output_path, f\"cleaned_part_{i // part_size + 1}.csv\")\n",
    "    part_data.to_csv(part_name, index=False)\n",
    "    print(f\"Saved {part_name}\")\n",
    "\n",
    "print(\"Data cleaning and saving completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f43cac5-2351-4e7c-b155-cb85e070d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Define the directory where CSV files are stored\n",
    "csv_dir = r\"F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_path = csv_dir  # Save in the same directory\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Read all CSV files\n",
    "all_files = glob.glob(os.path.join(csv_dir, \"*.csv\"))\n",
    "print(f\"Found {len(all_files)} files to process.\")\n",
    "\n",
    "# Combine all CSVs into a single DataFrame\n",
    "data_frames = []\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    data_frames.append(df)\n",
    "merged_data = pd.concat(data_frames, ignore_index=True)\n",
    "print(f\"Merged data shape: {merged_data.shape}\")\n",
    "\n",
    "# ---- Data Analysis ----\n",
    "# Identify columns with all zero values\n",
    "zero_columns = [col for col in merged_data.columns if (merged_data[col] == 0).all()]\n",
    "print(f\"Number of columns with all zero values: {len(zero_columns)}\")\n",
    "\n",
    "# Drop zero-value columns\n",
    "merged_data.drop(columns=zero_columns, inplace=True)\n",
    "\n",
    "# Drop columns with NaN values\n",
    "nan_columns = merged_data.columns[merged_data.isna().any()].tolist()\n",
    "print(f\"Number of columns with NaN values: {len(nan_columns)}\")\n",
    "merged_data.dropna(axis=1, inplace=True)\n",
    "\n",
    "# Drop duplicate columns\n",
    "duplicate_columns = set()\n",
    "for i, col1 in enumerate(merged_data.columns):\n",
    "    for col2 in merged_data.columns[i + 1:]:\n",
    "        if merged_data[col1].equals(merged_data[col2]):\n",
    "            duplicate_columns.add(col2)\n",
    "print(f\"Number of duplicate columns: {len(duplicate_columns)}\")\n",
    "merged_data.drop(columns=list(duplicate_columns), inplace=True)\n",
    "\n",
    "# ---- Feature Selection ----\n",
    "# Ensure file_id and label columns are present\n",
    "if \"fileid\" not in merged_data.columns or \"label\" not in merged_data.columns:\n",
    "    raise ValueError(\"Columns 'file_id' and 'label' must be present in the dataset.\")\n",
    "\n",
    "# Separate features, file_id, and label\n",
    "file_ids = merged_data[\"fileid\"]\n",
    "labels = merged_data[\"label\"]\n",
    "features = merged_data.drop(columns=[\"fileid\", \"label\"])\n",
    "\n",
    "# Normalize features for mutual information computation\n",
    "scaler = MinMaxScaler()\n",
    "normalized_features = scaler.fit_transform(features)\n",
    "\n",
    "# Compute mutual information\n",
    "mutual_info = mutual_info_classif(normalized_features, labels, discrete_features=False)\n",
    "feature_importance = pd.Series(mutual_info, index=features.columns).sort_values(ascending=False)\n",
    "top_features = feature_importance.head(64).index.tolist()\n",
    "\n",
    "print(f\"Selected top 64 features based on mutual information: {top_features}\")\n",
    "\n",
    "# Keep only selected features along with file_id and label\n",
    "selected_data = pd.concat([file_ids, features[top_features], labels], axis=1)\n",
    "\n",
    "# ---- Save Cleaned Data ----\n",
    "output_file = os.path.join(output_path, \"selected_66d_data.csv\")\n",
    "selected_data.to_csv(output_file, index=False)\n",
    "print(f\"Saved selected features to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "296bff0b-d7a6-43b8-aa3c-8beb566b21b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 files to process.\n",
      "Merged data shape: (558407, 259)\n",
      "Number of columns with all zero values: 179\n",
      "Number of columns with NaN values: 1\n",
      "Number of duplicate columns: 0\n",
      "Selected top 64 features based on variance: ['Unnamed: 242', 'Unnamed: 158', 'feature_100', 'Unnamed: 139', 'feature_75', 'Unnamed: 188', 'feature_92', 'Unnamed: 133', 'Unnamed: 138', 'Unnamed: 176', 'feature_72', 'feature_18', 'feature_22', 'Unnamed: 197', 'Unnamed: 228', 'feature_3', 'feature_127', 'feature_41', 'Unnamed: 150', 'feature_101', 'Unnamed: 256', 'Unnamed: 257', 'feature_94', 'Unnamed: 251', 'feature_105', 'feature_26', 'Unnamed: 160', 'feature_43', 'feature_60', 'feature_56', 'feature_121', 'Unnamed: 208', 'feature_11', 'feature_120', 'Unnamed: 229', 'Unnamed: 205', 'feature_47', 'Unnamed: 172', 'Unnamed: 224', 'feature_39', 'feature_96', 'Unnamed: 239', 'feature_78', 'feature_45', 'Unnamed: 235', 'feature_111', 'feature_53', 'Unnamed: 180', 'Unnamed: 194', 'Unnamed: 252', 'Unnamed: 143', 'Unnamed: 141', 'Unnamed: 151', 'Unnamed: 167', 'feature_14', 'Unnamed: 232', 'feature_51', 'feature_98', 'feature_19', 'Unnamed: 254', 'Unnamed: 134', 'Unnamed: 177', 'Unnamed: 182', 'feature_25']\n",
      "Cleaned data shape: (558407, 66)\n",
      "Saved F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\cleaned_part_1.csv\n",
      "Saved F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\cleaned_part_2.csv\n",
      "Saved F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\cleaned_part_3.csv\n",
      "Saved F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\cleaned_part_4.csv\n",
      "Saved F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\cleaned_part_5.csv\n",
      "Saved F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\cleaned_part_6.csv\n",
      "Saved F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\cleaned_part_7.csv\n",
      "Saved F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\cleaned_part_8.csv\n",
      "Saved F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\cleaned_part_9.csv\n",
      "Saved F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\cleaned_part_10.csv\n",
      "Saved F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\cleaned_part_11.csv\n",
      "Saved F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\\cleaned_part_12.csv\n",
      "Data cleaning and saving completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define the directory where CSV files are stored\n",
    "csv_dir = r\"F:\\Awais_data\\Datasets\\PartialSpoof\\Specnet\\train\\embeddings\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_path = csv_dir  # Save in the same directory\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Read all CSV files\n",
    "all_files = glob.glob(os.path.join(csv_dir, \"*.csv\"))\n",
    "print(f\"Found {len(all_files)} files to process.\")\n",
    "\n",
    "# Combine all CSVs into a single DataFrame\n",
    "data_frames = []\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    data_frames.append(df)\n",
    "merged_data = pd.concat(data_frames, ignore_index=True)\n",
    "print(f\"Merged data shape: {merged_data.shape}\")\n",
    "\n",
    "# ---- Data Analysis ----\n",
    "# Identify and remove columns with all zero values\n",
    "zero_columns = [col for col in merged_data.columns if (merged_data[col] == 0).all()]\n",
    "print(f\"Number of columns with all zero values: {len(zero_columns)}\")\n",
    "merged_data.drop(columns=zero_columns, inplace=True)\n",
    "\n",
    "# Remove columns with NaN values\n",
    "nan_columns = merged_data.columns[merged_data.isna().any()].tolist()\n",
    "print(f\"Number of columns with NaN values: {len(nan_columns)}\")\n",
    "merged_data.dropna(axis=1, inplace=True)\n",
    "\n",
    "# Remove duplicate columns\n",
    "duplicate_columns = set()\n",
    "for i, col1 in enumerate(merged_data.columns):\n",
    "    for col2 in merged_data.columns[i + 1:]:\n",
    "        if merged_data[col1].equals(merged_data[col2]):\n",
    "            duplicate_columns.add(col2)\n",
    "print(f\"Number of duplicate columns: {len(duplicate_columns)}\")\n",
    "merged_data.drop(columns=list(duplicate_columns), inplace=True)\n",
    "\n",
    "# Keep the \"file_id\" and \"label\" columns for later\n",
    "if \"fileid\" not in merged_data.columns or \"label\" not in merged_data.columns:\n",
    "    raise ValueError(\"Columns 'fileid' and 'label' must be present in the dataset.\")\n",
    "\n",
    "file_id = merged_data.pop(\"fileid\")\n",
    "label = merged_data.pop(\"label\")\n",
    "\n",
    "# Select the top 64 distinct columns based on variance\n",
    "variance = merged_data.var()\n",
    "top_features = variance.nlargest(64).index\n",
    "print(f\"Selected top 64 features based on variance: {top_features.tolist()}\")\n",
    "\n",
    "# Combine the cleaned data\n",
    "cleaned_data = pd.concat([file_id, merged_data[top_features], label], axis=1)\n",
    "print(f\"Cleaned data shape: {cleaned_data.shape}\")\n",
    "\n",
    "# ---- Save Cleaned Data in Parts ----\n",
    "part_size = 50000\n",
    "for i in range(0, len(cleaned_data), part_size):\n",
    "    part_data = cleaned_data.iloc[i:i + part_size]\n",
    "    part_name = os.path.join(output_path, f\"cleaned_part_{i // part_size + 1}.csv\")\n",
    "    part_data.to_csv(part_name, index=False)\n",
    "    print(f\"Saved {part_name}\")\n",
    "\n",
    "print(\"Data cleaning and saving completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6310cfc3-2cba-4e95-801c-1031803dd386",
   "metadata": {},
   "source": [
    "data shuffle and normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68892949-19fe-4809-a901-4f447ba0a087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized and shuffled data saved to C:\\Notebooks\\rrl_source\\Spectnet_model_Halftruth_embedding\\normalized_shuffled_output.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Input and output CSV file paths\n",
    "input_csv = \"C:\\\\Notebooks\\\\rrl_source\\\\Spectnet_model_Halftruth_embedding\\\\feature_emb_256.csv\"  # Replace with your input file path\n",
    "output_csv = \"C:\\\\Notebooks\\\\rrl_source\\\\Spectnet_model_Halftruth_embedding\\\\normalized_shuffled_output.csv\"  # Replace with your desired output file path\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Separate the columns\n",
    "file_ids = df.iloc[:, 0]  # File IDs (first column)\n",
    "labels = df.iloc[:, -1]  # Labels (last column)\n",
    "features = df.iloc[:, 1:-1]  # Feature columns\n",
    "\n",
    "# Normalize features to [0, 1] range\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "features_normalized = scaler.fit_transform(features)\n",
    "\n",
    "# Create a new DataFrame with normalized features\n",
    "normalized_df = pd.DataFrame(features_normalized, columns=features.columns)\n",
    "normalized_df.insert(0, \"FileID\", file_ids)  # Add FileID as the first column\n",
    "normalized_df[\"Label\"] = labels  # Add Label as the last column\n",
    "\n",
    "# Shuffle the DataFrame rows\n",
    "shuffled_df = shuffle(normalized_df, random_state=42)\n",
    "\n",
    "# Save the shuffled DataFrame to a new CSV file\n",
    "shuffled_df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Normalized and shuffled data saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62a119e-8bc7-4e58-aef0-53533bf4b77f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
