{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2570a453-63e4-4821-b69f-59b2c67041d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mawais\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Processing files:   0%|                                                                      | 0/24844 [00:00<?, ?it/s]C:\\Users\\mawais\\AppData\\Local\\Temp\\ipykernel_16844\\1193749285.py:51: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(y=windowed_segment, sr=sr)[0]\n",
      "C:\\Users\\mawais\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\librosa\\core\\pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n",
      "Processing files: 100%|███████████████████████████████████████████████████████| 24844/24844 [19:22:34<00:00,  2.81s/it]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 72.5 GiB for an array with shape (560279, 1086) and data type <U32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 118\u001b[0m\n\u001b[0;32m    116\u001b[0m audio_files \u001b[38;5;241m=\u001b[39m glob(dev_audio_path)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# audio_files = audio_files[:1]\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_utterance_level\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Save features in parts\u001b[39;00m\n\u001b[0;32m    121\u001b[0m part_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m80000\u001b[39m\n",
      "Cell \u001b[1;32mIn[1], line 110\u001b[0m, in \u001b[0;36mprocess_dataset\u001b[1;34m(audio_files, labels, utterance_level)\u001b[0m\n\u001b[0;32m    107\u001b[0m             combined_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(([file_id], handcrafted_features, wav2vec_features, [seg_label]))\n\u001b[0;32m    108\u001b[0m             all_features\u001b[38;5;241m.\u001b[39mappend(combined_features)\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_features\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 72.5 GiB for an array with shape (560279, 1086) and data type <U32"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import torch\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize paths and parameters\n",
    "model_name = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Wav2Vec2Model.from_pretrained(model_name).to(device)\n",
    "\n",
    "dev_audio_path = r\"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\dev\\\\con_wav\\\\*.wav\"\n",
    "segment_label_path = r\"F:\\Awais_data\\Datasets\\PartialSpoof\\database_segment_labels\\database\\segment_labels\\dev_seglab_0.16.npy\"\n",
    "utterance_label_path = r\"F:\\Awais_data\\Datasets\\PartialSpoof\\protocols\\PartialSpoof_LA_cm_protocols\\PartialSpoof.LA.cm.dev.trl.txt\"\n",
    "save_path = r\"C:\\Notebooks\\rrl_source\\dataset_raw\\merge\\segmented_features\\dev\\\\\"\n",
    "\n",
    "window_size = 0.16  # Segment length in seconds\n",
    "hop_size = 0.16     # Frame shift in seconds\n",
    "extract_utterance_level = False  # Set to True for utterance-level, False for segment-level\n",
    "\n",
    "# Hann window function\n",
    "def hann_window(length):\n",
    "    return 0.5 * (1 - np.cos(2 * np.pi * np.arange(length) / (length - 1)))\n",
    "\n",
    "# Load segment-level or utterance-level labels\n",
    "def load_labels(label_file, utterance_level):\n",
    "    if utterance_level:\n",
    "        labels = {}\n",
    "        with open(label_file, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' ')\n",
    "                if len(parts) >= 5:\n",
    "                    file_id = parts[1].strip()\n",
    "                    label = parts[-1].strip()\n",
    "                    labels[file_id] = 0 if label == 'spoof' else 1\n",
    "    else:\n",
    "        labels = np.load(label_file, allow_pickle=True).item()\n",
    "    return labels\n",
    "\n",
    "# Handcrafted feature extraction\n",
    "def extract_handcrafted_features(segment, sr):\n",
    "    windowed_segment = segment * hann_window(len(segment))\n",
    "    mfcc = librosa.feature.mfcc(y=windowed_segment, sr=sr, n_mfcc=13).mean(axis=1)\n",
    "    delta_mfcc = librosa.feature.delta(mfcc)\n",
    "    tempo = librosa.beat.tempo(y=windowed_segment, sr=sr)[0]\n",
    "    chroma = librosa.feature.chroma_stft(y=windowed_segment, sr=sr).mean(axis=1)\n",
    "    zcr = librosa.feature.zero_crossing_rate(windowed_segment).mean()\n",
    "    energy = librosa.feature.rms(y=windowed_segment).mean()\n",
    "    pitches, _ = librosa.core.piptrack(y=windowed_segment, sr=sr)\n",
    "    pitch = np.mean(pitches[pitches > 0]) if len(pitches[pitches > 0]) > 0 else 0\n",
    "    tempogram = librosa.feature.tempogram(y=windowed_segment, sr=sr).mean(axis=1)[1:]\n",
    "    downsampled_tempogram = tempogram[::int(np.ceil(len(tempogram) / 18))]\n",
    "    features = np.concatenate((mfcc, delta_mfcc, [tempo], chroma, [zcr], [energy], [pitch], downsampled_tempogram))\n",
    "    return features\n",
    "\n",
    "def extract_wav2vec_features(segment, sr): \n",
    "    if len(segment.shape) > 1:   # Ensure the segment is a 1D array and not a 2D array\n",
    "        segment = segment.flatten()  # Flatten it to 1D if needed\n",
    "    \n",
    "    inputs = feature_extractor(segment, sampling_rate=sr, return_tensors=\"pt\", padding=False)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()} \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)  # Extract the last hidden state and return it\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "# Main processing function\n",
    "def process_dataset(audio_files, labels, utterance_level):\n",
    "    all_features = []\n",
    "    for audio_file in tqdm(audio_files, desc=\"Processing files\"):\n",
    "        file_id = os.path.basename(audio_file).replace('.wav', '')\n",
    "        if file_id not in labels:\n",
    "            continue\n",
    "\n",
    "        y, sr = librosa.load(audio_file, sr=16000)\n",
    "        if utterance_level:\n",
    "            label = labels[file_id]\n",
    "            handcrafted_features = extract_handcrafted_features(y, sr)\n",
    "            wav2vec_features = extract_wav2vec_features(y, sr)\n",
    "            combined_features = np.concatenate(([file_id], handcrafted_features, wav2vec_features, [label]))\n",
    "            all_features.append(combined_features)\n",
    "        else:\n",
    "            segment_labels = labels[file_id]\n",
    "            segment_length = int(window_size * sr)\n",
    "            hop_length = int(hop_size * sr)\n",
    "\n",
    "            for i, seg_label in enumerate(segment_labels):\n",
    "                start = i * hop_length\n",
    "                end = start + segment_length\n",
    "                segment = (\n",
    "                    np.pad(y[start:], (0, segment_length - len(y[start:])), mode='edge')\n",
    "                if end > len(y)\n",
    "                else y[start:end]\n",
    "                )\n",
    "\n",
    "                # Extract features\n",
    "                handcrafted_features = extract_handcrafted_features(segment, sr)\n",
    "                wav2vec_features = extract_wav2vec_features(segment, sr)\n",
    "                combined_features = np.concatenate(([file_id], handcrafted_features, wav2vec_features, [seg_label]))\n",
    "                all_features.append(combined_features)\n",
    "\n",
    "    return np.array(all_features)\n",
    "\n",
    "\n",
    "\n",
    "# Load labels and process dataset\n",
    "labels = load_labels(utterance_label_path if extract_utterance_level else segment_label_path, extract_utterance_level)\n",
    "audio_files = glob(dev_audio_path)\n",
    "# audio_files = audio_files[:1]\n",
    "features = process_dataset(audio_files, labels, extract_utterance_level)\n",
    "\n",
    "# Save features in parts\n",
    "part_size = 80000\n",
    "for i in range(0, len(features), part_size):\n",
    "    part_features = features[i:i + part_size]\n",
    "    part_name = f\"{save_path}dev_{'utterance' if extract_utterance_level else 'segment'}_merged_part_{i // part_size + 1}.csv\"\n",
    "    pd.DataFrame(part_features).to_csv(part_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4b43bfc-c9e3-4488-b70e-2489c9dcbf9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Notebooks'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5956efcf-15b3-4419-a441-1d17acb8e863",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfeatures\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20b098f2-10d3-4ba6-9ce1-8a92d5f3a399",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfeatures\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0527c9-422d-44b4-9069-daf16e6c9bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
