{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "edb78c52-869a-42a3-96aa-1bd6692c8026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25380\n",
      "Number of spoof files: 20518\n",
      "Number of real files: 4862\n",
      "Length of segment_labels: 17\n",
      "Indexing at: 24\n",
      "Skipping index 24 because it's out of bounds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|                                                                         | 0/20 [00:00<?, ?it/s]C:\\Users\\mawais\\AppData\\Local\\Temp\\ipykernel_12968\\2889203000.py:56: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(y=windowed_segment, sr=sr)[0]\n",
      "Processing files:   0%|                                                                         | 0/20 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 17 is out of bounds for axis 0 with size 17",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 207\u001b[0m\n\u001b[0;32m    204\u001b[0m selected_audio_files \u001b[38;5;241m=\u001b[39m selected_spoof_files \u001b[38;5;241m+\u001b[39m selected_real_files\n\u001b[0;32m    206\u001b[0m \u001b[38;5;66;03m# Process the dataset\u001b[39;00m\n\u001b[1;32m--> 207\u001b[0m features, labels \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselected_audio_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_utterance_level\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# Split dataset into train, validation, and test sets (80-10-10 split)\u001b[39;00m\n\u001b[0;32m    210\u001b[0m X_train, X_temp, y_train, y_temp \u001b[38;5;241m=\u001b[39m train_test_split(features, labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[1;32mIn[33], line 129\u001b[0m, in \u001b[0;36mprocess_dataset\u001b[1;34m(audio_files, labels, utterance_level)\u001b[0m\n\u001b[0;32m    127\u001b[0m             combined_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((handcrafted_features, wav2vec_features))\n\u001b[0;32m    128\u001b[0m             all_features\u001b[38;5;241m.\u001b[39mappend(combined_features)\n\u001b[1;32m--> 129\u001b[0m             all_labels\u001b[38;5;241m.\u001b[39mappend(\u001b[43msegment_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(all_features), np\u001b[38;5;241m.\u001b[39marray(all_labels)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 17 is out of bounds for axis 0 with size 17"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import librosa\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize paths and parameters\n",
    "model_name = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "model = Wav2Vec2Model.from_pretrained(model_name)\n",
    "\n",
    "train_audio_path = r\"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\train\\\\con_wav\\\\*.wav\"\n",
    "segment_label_path = r\"F:\\Awais_data\\Datasets\\PartialSpoof\\database_segment_labels\\database\\segment_labels\\train_seglab_0.16.npy\"\n",
    "utterance_label_path = r\"F:\\Awais_data\\Datasets\\PartialSpoof\\protocols\\PartialSpoof_LA_cm_protocols\\PartialSpoof.LA.cm.train.trl.txt\"\n",
    "save_path = r\"C:\\\\Notebooks\\\\rrl_source\\\\dataset_raw\\\\merge\\\\new\\\\\"\n",
    "\n",
    "window_size = 0.16 # Segment length in seconds\n",
    "hop_size = 0.16    # Frame shift in seconds\n",
    "extract_utterance_level = False  # Set to True for utterance-level, False for segment-level\n",
    "\n",
    "# Hann window function\n",
    "def hann_window(length):\n",
    "    return 0.5 * (1 - np.cos(2 * np.pi * np.arange(length) / (length - 1)))\n",
    "\n",
    "# Load segment-level or utterance-level labels\n",
    "def load_labels(label_file, utterance_level):\n",
    "    labels = {}\n",
    "    \n",
    "    if utterance_level:\n",
    "        # For utterance-level labels, we parse the label file and map the class labels to file_ids\n",
    "        with open(label_file, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' ')\n",
    "                if len(parts) >= 5:  # Assuming the line structure is consistent\n",
    "                    file_id = parts[1].strip()  # Extract the file_id\n",
    "                    label = parts[-1].strip()   # Extract the label (e.g., \"spoof\" or \"bonafide\")\n",
    "                    # Assign 0 for spoof, 1 for real/bonafide\n",
    "                    labels[file_id] = 0 if label == 'spoof' else 1\n",
    "    else:\n",
    "        # If working with segment-level labels\n",
    "        labels = np.load(label_file, allow_pickle=True).item()\n",
    "\n",
    "    return labels\n",
    "\n",
    "def extract_handcrafted_features(segment, sr, target_size=60):\n",
    "    windowed_segment = segment * hann_window(len(segment))\n",
    "    mfcc = librosa.feature.mfcc(y=windowed_segment, sr=sr, n_mfcc=13).mean(axis=1)\n",
    "    delta_mfcc = librosa.feature.delta(mfcc)\n",
    "    tempo = librosa.beat.tempo(y=windowed_segment, sr=sr)[0]\n",
    "    chroma = librosa.feature.chroma_stft(y=windowed_segment, sr=sr).mean(axis=1)\n",
    "    zcr = librosa.feature.zero_crossing_rate(windowed_segment).mean()\n",
    "    energy = librosa.feature.rms(y=windowed_segment).mean()\n",
    "    pitches, _ = librosa.core.piptrack(y=windowed_segment, sr=sr)\n",
    "    pitch = np.mean(pitches[pitches > 0]) if len(pitches[pitches > 0]) > 0 else 0\n",
    "    tempogram = librosa.feature.tempogram(y=windowed_segment, sr=sr).mean(axis=1)\n",
    "    downsampled_tempogram = tempogram[::int(np.ceil(len(tempogram) / 18))]\n",
    "    \n",
    "    features = np.concatenate((mfcc, delta_mfcc, [tempo], chroma, [zcr], [energy], [pitch], downsampled_tempogram))\n",
    "    \n",
    "    # Pad or truncate features to the target size\n",
    "    if len(features) < target_size:\n",
    "        features = np.pad(features, (0, target_size - len(features)), mode='constant')\n",
    "    elif len(features) > target_size:\n",
    "        features = features[:target_size]\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Wav2Vec2 feature extraction function\n",
    "def extract_wav2vec_features(segment, sr, target_size=1024):\n",
    "    if len(segment.shape) > 1:   # Ensure the segment is a 1D array and not a 2D array\n",
    "        segment = segment.flatten()  # Flatten it to 1D if needed\n",
    "    \n",
    "    inputs = feature_extractor(segment, sampling_rate=sr, return_tensors=\"pt\", padding=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)  # Extract the last hidden state and return it\n",
    "    wav2vec_features = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    \n",
    "    # Pad or truncate to fixed size (1024)\n",
    "    if len(wav2vec_features) < target_size:\n",
    "        wav2vec_features = np.pad(wav2vec_features, (0, target_size - len(wav2vec_features)), mode='constant')\n",
    "    elif len(wav2vec_features) > target_size:\n",
    "        wav2vec_features = wav2vec_features[:target_size]\n",
    "    \n",
    "    return wav2vec_features\n",
    "\n",
    "# Main processing function\n",
    "def process_dataset(audio_files, labels, utterance_level):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    for audio_file in tqdm(audio_files, desc=\"Processing files\"):\n",
    "        file_id = os.path.basename(audio_file).replace('.wav', '')\n",
    "        if file_id not in labels:\n",
    "            print('no file id exist')\n",
    "            continue\n",
    "\n",
    "        y, sr = librosa.load(audio_file, sr=16000)\n",
    "        y = librosa.util.fix_length(y, size=int(4 * sr))  # Truncate or pad to 4 seconds\n",
    "  # Truncate or pad to 4 seconds\n",
    "        num_frames = int(4 / hop_size)\n",
    "        \n",
    "        if utterance_level:\n",
    "            label = labels[file_id]\n",
    "            for _ in range(num_frames):\n",
    "                handcrafted_features = extract_handcrafted_features(y, sr)\n",
    "                wav2vec_features = extract_wav2vec_features(y, sr)\n",
    "                combined_features = np.concatenate((handcrafted_features, wav2vec_features))\n",
    "                all_features.append(combined_features)\n",
    "                all_labels.append(label)\n",
    "        else:\n",
    "            segment_labels = labels[file_id]\n",
    "            segment_length = int(window_size * sr)\n",
    "            hop_length = int(hop_size * sr)\n",
    "            for i in range(num_frames):\n",
    "                start = i * hop_length\n",
    "                end = start + segment_length\n",
    "                segment = y[start:end]\n",
    "                handcrafted_features = extract_handcrafted_features(segment, sr)\n",
    "                wav2vec_features = extract_wav2vec_features(segment, sr)\n",
    "                combined_features = np.concatenate((handcrafted_features, wav2vec_features))\n",
    "                all_features.append(combined_features)\n",
    "                all_labels.append(segment_labels[i])\n",
    "\n",
    "    return np.array(all_features), np.array(all_labels)\n",
    "\n",
    "# Load labels and process dataset for 100 spoof and 100 real audios\n",
    "labels = load_labels(utterance_label_path if extract_utterance_level else segment_label_path, extract_utterance_level)\n",
    "\n",
    "audio_files = glob(train_audio_path)\n",
    "print(len(audio_files))\n",
    "\n",
    "# Extract file IDs (for matching with labels)\n",
    "audio_file_ids = [os.path.basename(file).replace('.wav', '') for file in audio_files]\n",
    "# print(audio_file_ids[:10])  # Print first 10 file IDs\n",
    "\n",
    "# Now, match the files based on file_id and their labels\n",
    "# spoof_files = [file for file in audio_files if os.path.basename(file).replace('.wav', '') in labels and labels[os.path.basename(file).replace('.wav', '')] == 0]\n",
    "# print(len(spoof_files))\n",
    "\n",
    "# real_files = [file for file in audio_files if os.path.basename(file).replace('.wav', '') in labels and labels[os.path.basename(file).replace('.wav', '')] == 1]\n",
    "# print(len(real_files))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spoof_files = []\n",
    "real_files = []\n",
    "for file in audio_files:\n",
    "    file_id = os.path.basename(file).replace('.wav', '')\n",
    "    \n",
    "    if file_id not in labels:\n",
    "        print(f\"File ID {file_id} not found in labels.\")\n",
    "        continue\n",
    "    \n",
    "    if extract_utterance_level:\n",
    "        # For utterance-level labels\n",
    "        label = labels[file_id]\n",
    "        if label == 0:\n",
    "            spoof_files.append(file)\n",
    "        elif label == 1:\n",
    "            real_files.append(file)\n",
    "    else:\n",
    "        # For segment-level labels\n",
    "        segment_labels = labels[file_id]\n",
    "        # Here, check for segment-level processing logic\n",
    "        if int(segment_labels[0]) == 0:\n",
    "            spoof_files.append(file)\n",
    "        elif int(segment_labels[0]) == 1:\n",
    "            real_files.append(file)\n",
    "\n",
    "print(f\"Number of spoof files: {len(spoof_files)}\")\n",
    "print(f\"Number of real files: {len(real_files)}\")\n",
    "# selected_spoof_files = spoof_files  # Use all available spoof files\n",
    "# selected_real_files = real_files  # Use all available real files\n",
    "\n",
    "print(f\"Length of segment_labels: {len(segment_labels)}\")\n",
    "print(f\"Indexing at: {i}\")\n",
    "if i >= len(segment_labels):\n",
    "    print(f\"Skipping index {i} because it's out of bounds.\")\n",
    "else:\n",
    "    all_labels.append(segment_labels[i])\n",
    "\n",
    "# Select 100 spoof audios from the start and 100 real audios from the end\n",
    "selected_spoof_files = spoof_files[:10]  # Selecting the first 100 spoof files\n",
    "selected_real_files = real_files[-10:]  # Selecting the last 100 real files\n",
    "\n",
    "# Print names and labels of selected audios\n",
    "for audio_file in selected_spoof_files:\n",
    "    file_id = os.path.basename(audio_file).replace('.wav', '')\n",
    "    # print(f\"Selected spoof audio: {file_id} - Label: {labels[file_id]}\")\n",
    "\n",
    "for audio_file in selected_real_files:\n",
    "    file_id = os.path.basename(audio_file).replace('.wav', '')\n",
    "    # print(f\"Selected real audio: {file_id} - Label: {labels[file_id]}\")\n",
    "\n",
    "# Combine the selected files\n",
    "selected_audio_files = selected_spoof_files + selected_real_files\n",
    "\n",
    "# Process the dataset\n",
    "features, labels = process_dataset(selected_audio_files, labels, extract_utterance_level)\n",
    "\n",
    "# Split dataset into train, validation, and test sets (80-10-10 split)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features (important for MLP performance)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train an MLP classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(512, 256), max_iter=300, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = mlp.predict(X_test)\n",
    "y_pred_prob = mlp.predict_proba(X_test)[:, 1]  # Probabilities for ROC AUC and EER\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# EER calculation\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]  # EER is where fpr = 1 - tpr\n",
    "\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"EER: {eer:.4f}\")\n",
    "\n",
    "# Save evaluation results to a .txt file\n",
    "evaluation_file = \"C:\\\\Notebooks\\\\rrl_source\\\\dataset_raw\\\\merge\\\\new\\\\evaluation_results.txt\"\n",
    "with open(evaluation_file, \"w\") as file:\n",
    "    file.write(f\"Confusion Matrix:\\n{cm}\\n\")\n",
    "    file.write(f\"ROC AUC: {roc_auc:.4f}\\n\")\n",
    "    file.write(f\"EER: {eer:.4f}\\n\")\n",
    "\n",
    "# Save frame-level predictions to a .csv file\n",
    "predictions_df = pd.DataFrame({\n",
    "    'File ID': selected_audio_files,\n",
    "    'Prediction': y_pred,\n",
    "    'Prediction Score': y_pred_prob\n",
    "})\n",
    "predictions_file = \"C:\\\\Notebooks\\\\rrl_source\\\\dataset_raw\\\\merge\\\\new\\\\frame_level_predictions.csv\"\n",
    "predictions_df.to_csv(predictions_file, index=False)\n",
    "\n",
    "print(f\"Evaluation results and predictions saved to {evaluation_file} and {predictions_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7af1a81a-c4ba-4b56-b76f-cea913538a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio File: LA_T_6741908\n",
      "Number of frames generated: 15\n",
      "Number of labels in the segment label file: 15\n",
      "Labels: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]...\n",
      "--------------------------------------------------\n",
      "Audio File: CON_T_0008091\n",
      "Number of frames generated: 18\n",
      "Number of labels in the segment label file: 18\n",
      "Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]...\n",
      "--------------------------------------------------\n",
      "Audio File: CON_T_0005915\n",
      "Number of frames generated: 31\n",
      "Number of labels in the segment label file: 31\n",
      "Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from glob import glob\n",
    "\n",
    "# Function to load segment-level or utterance-level labels\n",
    "def load_labels(label_file, utterance_level):\n",
    "    labels = {}\n",
    "    \n",
    "    if utterance_level:\n",
    "        # For utterance-level labels, parse the label file and map file_ids to class labels\n",
    "        with open(label_file, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' ')\n",
    "                if len(parts) >= 5:  # Assuming the line structure is consistent\n",
    "                    file_id = parts[1].strip()  # Extract file_id\n",
    "                    label = parts[-1].strip()   # Extract label (\"spoof\" or \"bonafide\")\n",
    "                    labels[file_id] = 0 if label == 'spoof' else 1\n",
    "    else:\n",
    "        # For segment-level labels, load from the numpy file\n",
    "        labels = np.load(label_file, allow_pickle=True).item()\n",
    "\n",
    "    return labels\n",
    "\n",
    "# Function to calculate the number of frames for a given audio based on window_size and hop_size\n",
    "def get_num_frames(audio_length, window_size, hop_size, sr):\n",
    "    segment_length = int(window_size * sr)\n",
    "    hop_length = int(hop_size * sr)\n",
    "    return int((audio_length - segment_length) / hop_length) + 1\n",
    "\n",
    "# Paths for dataset and labels\n",
    "train_audio_path = r\"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\train\\\\con_wav\\\\*.wav\"\n",
    "segment_label_path = r\"F:\\Awais_data\\Datasets\\PartialSpoof\\database_segment_labels\\database\\segment_labels\\train_seglab_0.16.npy\"\n",
    "utterance_label_path = r\"F:\\Awais_data\\Datasets\\PartialSpoof\\protocols\\PartialSpoof_LA_cm_protocols\\PartialSpoof.LA.cm.train.trl.txt\"\n",
    "\n",
    "# Parameters\n",
    "window_size = 0.16  # Segment length in seconds\n",
    "hop_size = 0.15    # Frame shift in seconds\n",
    "extract_utterance_level = True  # Set to True for utterance-level, False for segment-level\n",
    "\n",
    "# Load labels\n",
    "labels = load_labels(utterance_label_path if extract_utterance_level else segment_label_path, extract_utterance_level)\n",
    "\n",
    "# Load 3 random audio files from the dataset\n",
    "audio_files = glob(train_audio_path)\n",
    "random_files = np.random.choice(audio_files, 3, replace=False)\n",
    "\n",
    "# Process each audio file\n",
    "for audio_file in random_files:\n",
    "    file_id = os.path.basename(audio_file).replace('.wav', '')\n",
    "    \n",
    "    if file_id not in labels:\n",
    "        print(f\"File ID {file_id} not found in labels\")\n",
    "        continue\n",
    "\n",
    "    y, sr = librosa.load(audio_file, sr=16000)\n",
    "    audio_length = len(y)\n",
    "\n",
    "    # Get the number of frames for this audio file\n",
    "    num_frames = get_num_frames(audio_length, window_size, hop_size, sr)\n",
    "\n",
    "    # Get the corresponding labels for the audio file\n",
    "    if extract_utterance_level:\n",
    "        label = labels[file_id]\n",
    "        segment_labels = [label] * num_frames\n",
    "    else:\n",
    "        segment_labels = labels[file_id]\n",
    "\n",
    "    # Print the details\n",
    "    print(f\"Audio File: {file_id}\")\n",
    "    print(f\"Number of frames generated: {num_frames}\")\n",
    "    print(f\"Number of labels in the segment label file: {len(segment_labels)}\")\n",
    "    print(f\"Labels: {segment_labels[:50]}...\")  # Print first 5 labels as an example\n",
    "    print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f4cbd0-60dd-4cc4-83e9-cc0ef49c2b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa506ead-66c0-4039-a323-70967433a047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mawais\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25380\n",
      "22800\n",
      "2580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|                                                                      | 0/25380 [00:00<?, ?it/s]C:\\Users\\mawais\\AppData\\Local\\Temp\\ipykernel_21532\\2792128954.py:70: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(y=windowed_segment, sr=sr)[0]\n",
      "Processing files: 100%|███████████████████████████████████████████████████████| 25380/25380 [39:58:28<00:00,  5.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[22727     0]\n",
      " [    0  2653]]\n",
      "ROC AUC: 1.0000\n",
      "EER: 0.0000\n",
      "Evaluation results and predictions saved to C:\\Notebooks\\rrl_source\\dataset_raw\\merge\\new\\evaluation_results.txt and C:\\Notebooks\\rrl_source\\dataset_raw\\merge\\new\\frame_level_predictions.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import librosa\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "# Determine if a GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize paths and parameters\n",
    "model_name = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "model = Wav2Vec2Model.from_pretrained(model_name).to(device)\n",
    "\n",
    "train_audio_path = r\"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\train\\\\con_wav\\\\*.wav\"\n",
    "segment_label_path = r\"F:\\Awais_data\\Datasets\\PartialSpoof\\database_segment_labels\\database\\segment_labels\\train_seglab_0.16.npy\"\n",
    "utterance_label_path = r\"F:\\Awais_data\\Datasets\\PartialSpoof\\protocols\\PartialSpoof_LA_cm_protocols\\PartialSpoof.LA.cm.train.trl.txt\"\n",
    "save_path = r\"C:\\\\Notebooks\\\\rrl_source\\\\dataset_raw\\\\merge\\\\new\\\\\"\n",
    "\n",
    "window_size = 0.16 # Segment length in seconds\n",
    "hop_size = 0.16    # Frame shift in seconds\n",
    "extract_utterance_level = True  # Set to True for utterance-level, False for segment-level\n",
    "\n",
    "# Hann window function\n",
    "def hann_window(length):\n",
    "    return 0.5 * (1 - np.cos(2 * np.pi * np.arange(length) / (length - 1)))\n",
    "\n",
    "# Load segment-level or utterance-level labels\n",
    "def load_labels(label_file, utterance_level):\n",
    "    labels = {}\n",
    "    \n",
    "    if utterance_level:\n",
    "        # For utterance-level labels, we parse the label file and map the class labels to file_ids\n",
    "        with open(label_file, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' ')\n",
    "                if len(parts) >= 5:  # Assuming the line structure is consistent\n",
    "                    file_id = parts[1].strip()  # Extract the file_id\n",
    "                    label = parts[-1].strip()   # Extract the label (e.g., \"spoof\" or \"bonafide\")\n",
    "                    # Assign 0 for spoof, 1 for real/bonafide\n",
    "                    labels[file_id] = 0 if label == 'spoof' else 1\n",
    "    else:\n",
    "        # If working with segment-level labels\n",
    "        labels = np.load(label_file, allow_pickle=True).item()\n",
    "\n",
    "    return labels\n",
    "\n",
    "def extract_handcrafted_features(segment, sr, target_size=60):\n",
    "    windowed_segment = segment * hann_window(len(segment))\n",
    "    mfcc = librosa.feature.mfcc(y=windowed_segment, sr=sr, n_mfcc=13).mean(axis=1)\n",
    "    delta_mfcc = librosa.feature.delta(mfcc)\n",
    "    # tempo = librosa.feature.rhythm.tempo(y=windowed_segment, sr=sr)[0]\n",
    "    tempo = librosa.beat.tempo(y=windowed_segment, sr=sr)[0]\n",
    "    chroma = librosa.feature.chroma_stft(y=windowed_segment, sr=sr).mean(axis=1)\n",
    "    zcr = librosa.feature.zero_crossing_rate(windowed_segment).mean()\n",
    "    energy = librosa.feature.rms(y=windowed_segment).mean()\n",
    "    pitches, _ = librosa.core.piptrack(y=windowed_segment, sr=sr)\n",
    "    pitch = np.mean(pitches[pitches > 0]) if len(pitches[pitches > 0]) > 0 else 0\n",
    "    tempogram = librosa.feature.tempogram(y=windowed_segment, sr=sr).mean(axis=1)\n",
    "    downsampled_tempogram = tempogram[::int(np.ceil(len(tempogram) / 18))]\n",
    "    \n",
    "    features = np.concatenate((mfcc, delta_mfcc, [tempo], chroma, [zcr], [energy], [pitch], downsampled_tempogram))\n",
    "    \n",
    "    # Pad or truncate features to the target size\n",
    "    if len(features) < target_size:\n",
    "        features = np.pad(features, (0, target_size - len(features)), mode='constant')\n",
    "    elif len(features) > target_size:\n",
    "        features = features[:target_size]    \n",
    "    return features\n",
    "\n",
    "def extract_wav2vec_features(segment, sr, target_size=1024):\n",
    "    if len(segment.shape) > 1:   # Ensure the segment is a 1D array and not a 2D array\n",
    "        segment = segment.flatten()  # Flatten it to 1D if needed\n",
    "    \n",
    "    inputs = feature_extractor(segment, sampling_rate=sr, return_tensors=\"pt\", padding=False).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)  # Extract the last hidden state\n",
    "    wav2vec_features = outputs.last_hidden_state.mean(dim=1).cpu().squeeze().numpy()\n",
    "    \n",
    "    # Pad or truncate to fixed size (1024)\n",
    "    if len(wav2vec_features) < target_size:\n",
    "        wav2vec_features = np.pad(wav2vec_features, (0, target_size - len(wav2vec_features)), mode='constant')\n",
    "    elif len(wav2vec_features) > target_size:\n",
    "        wav2vec_features = wav2vec_features[:target_size]\n",
    "    \n",
    "    return wav2vec_features\n",
    "\n",
    "# Main processing function\n",
    "def process_dataset(audio_files, labels, utterance_level):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    for audio_file in tqdm(audio_files, desc=\"Processing files\"):\n",
    "        file_id = os.path.basename(audio_file).replace('.wav', '')\n",
    "        if file_id not in labels:\n",
    "            print('no file id exist')\n",
    "            continue\n",
    "\n",
    "        y, sr = librosa.load(audio_file, sr=16000)\n",
    "        y = librosa.util.fix_length(y, size=int(4 * sr))  # Truncate or pad to 4 seconds\n",
    "  # Truncate or pad to 4 seconds\n",
    "        num_frames = int(4 / hop_size)\n",
    "        \n",
    "        if utterance_level:\n",
    "            label = labels[file_id]\n",
    "            for _ in range(num_frames):\n",
    "                handcrafted_features = extract_handcrafted_features(y, sr)\n",
    "                wav2vec_features = extract_wav2vec_features(y, sr)\n",
    "                combined_features = np.concatenate((handcrafted_features, wav2vec_features))\n",
    "                all_features.append(combined_features)\n",
    "                all_labels.append(label)\n",
    "        else:\n",
    "            segment_labels = labels[file_id]\n",
    "            segment_length = int(window_size * sr)\n",
    "            hop_length = int(hop_size * sr)\n",
    "            for i in range(num_frames):\n",
    "                start = i * hop_length\n",
    "                end = start + segment_length\n",
    "                segment = y[start:end]\n",
    "                handcrafted_features = extract_handcrafted_features(segment, sr)\n",
    "                wav2vec_features = extract_wav2vec_features(segment, sr)\n",
    "                combined_features = np.concatenate((handcrafted_features, wav2vec_features))\n",
    "                all_features.append(combined_features)\n",
    "                all_labels.append(segment_labels[i])\n",
    "\n",
    "    return np.array(all_features), np.array(all_labels)\n",
    "\n",
    "# Load labels and process dataset for 100 spoof and 100 real audios\n",
    "labels = load_labels(utterance_label_path if extract_utterance_level else segment_label_path, extract_utterance_level)\n",
    "\n",
    "audio_files = glob(train_audio_path)\n",
    "print(len(audio_files))\n",
    "\n",
    "# Extract file IDs (for matching with labels)\n",
    "audio_file_ids = [os.path.basename(file).replace('.wav', '') for file in audio_files]\n",
    "# print(audio_file_ids[:10])  # Print first 10 file IDs\n",
    "\n",
    "# Now, match the files based on file_id and their labels\n",
    "spoof_files = [file for file in audio_files if os.path.basename(file).replace('.wav', '') in labels and labels[os.path.basename(file).replace('.wav', '')] == 0]\n",
    "print(len(spoof_files))\n",
    "\n",
    "real_files = [file for file in audio_files if os.path.basename(file).replace('.wav', '') in labels and labels[os.path.basename(file).replace('.wav', '')] == 1]\n",
    "print(len(real_files))\n",
    "\n",
    "# Select 100 spoof audios from the start and 100 real audios from the end\n",
    "# selected_spoof_files = spoof_files[:50]  # Selecting the first 100 spoof files\n",
    "# selected_real_files = real_files[-25:]  # Selecting the last 100 real files\n",
    "\n",
    "selected_spoof_files = spoof_files  # Use all available spoof files\n",
    "selected_real_files = real_files  # Use all available real files\n",
    "\n",
    "# Print names and labels of selected audios\n",
    "for audio_file in selected_spoof_files:\n",
    "    file_id = os.path.basename(audio_file).replace('.wav', '')\n",
    "    # print(f\"Selected spoof audio: {file_id} - Label: {labels[file_id]}\")\n",
    "\n",
    "for audio_file in selected_real_files:\n",
    "    file_id = os.path.basename(audio_file).replace('.wav', '')\n",
    "    # print(f\"Selected real audio: {file_id} - Label: {labels[file_id]}\")\n",
    "\n",
    "# Combine the selected files\n",
    "selected_audio_files = selected_spoof_files + selected_real_files\n",
    "\n",
    "# Process the dataset\n",
    "features, labels = process_dataset(selected_audio_files, labels, extract_utterance_level)\n",
    "\n",
    "# Split dataset into train, validation, and test sets (80-10-10 split)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features (important for MLP performance)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train an MLP classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(512, 256), max_iter=300, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = mlp.predict(X_test)\n",
    "y_pred_prob = mlp.predict_proba(X_test)[:, 1]  # Probabilities for ROC AUC and EER\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# EER calculation\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]  # EER is where fpr = 1 - tpr\n",
    "\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"EER: {eer:.4f}\")\n",
    "\n",
    "# Save evaluation results to a .txt file\n",
    "evaluation_file = \"C:\\\\Notebooks\\\\rrl_source\\\\dataset_raw\\\\merge\\\\new\\\\evaluation_results.txt\"\n",
    "with open(evaluation_file, \"w\") as file:\n",
    "    file.write(f\"Confusion Matrix:\\n{cm}\\n\")\n",
    "    file.write(f\"ROC AUC: {roc_auc:.4f}\\n\")\n",
    "    file.write(f\"EER: {eer:.4f}\\n\")\n",
    "\n",
    "# Save frame-level predictions to a .csv file\n",
    "predictions_df = pd.DataFrame({\n",
    "    'File ID': selected_audio_files,\n",
    "    'Prediction': y_pred,\n",
    "    'Prediction Score': y_pred_prob\n",
    "})\n",
    "predictions_file = \"C:\\\\Notebooks\\\\rrl_source\\\\dataset_raw\\\\merge\\\\new\\\\frame_level_predictions.csv\"\n",
    "predictions_df.to_csv(predictions_file, index=False)\n",
    "\n",
    "print(f\"Evaluation results and predictions saved to {evaluation_file} and {predictions_file}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e70db56e-ff64-42b9-acbf-7840f6cc7420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mawais\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'F:\\\\\\\\Awais_data\\\\\\\\Datasets\\\\\\\\PartialSpoof\\\\\\\\protocols\\\\\\\\PartialSpoof_LA_cm_protocols\\\\\\\\PartialSpoof.LA.cm.train.trl.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 144\u001b[0m\n\u001b[0;32m    136\u001b[0m test_files \u001b[38;5;241m=\u001b[39m glob(test_audio_path)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Load and balance datasets\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# train_files = balance_dataset(glob(train_audio_path), train_labels, max_per_class=1200)\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# val_files = balance_dataset(glob(val_audio_path), val_labels, max_per_class=2500)\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# test_files = balance_dataset(glob(test_audio_path), test_labels, max_per_class=2500)\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Load labels\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_utterance_label_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutterance_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m val_labels \u001b[38;5;241m=\u001b[39m load_labels(val_utterance_label_path, utterance_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    146\u001b[0m test_labels \u001b[38;5;241m=\u001b[39m load_labels(test_utterance_label_path, utterance_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[1], line 45\u001b[0m, in \u001b[0;36mload_labels\u001b[1;34m(label_file, utterance_level)\u001b[0m\n\u001b[0;32m     43\u001b[0m labels \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m utterance_level:\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabel_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[0;32m     47\u001b[0m             parts \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'F:\\\\\\\\Awais_data\\\\\\\\Datasets\\\\\\\\PartialSpoof\\\\\\\\protocols\\\\\\\\PartialSpoof_LA_cm_protocols\\\\\\\\PartialSpoof.LA.cm.train.trl.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import librosa\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Determine if a GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize paths and parameters\n",
    "model_name = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "model = Wav2Vec2Model.from_pretrained(model_name).to(device)\n",
    "\n",
    "train_audio_path = r\"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\train\\\\con_wav\\\\*.wav\"\n",
    "train_utterance_label_path = r\"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\protocols\\\\PartialSpoof_LA_cm_protocols\\\\PartialSpoof.LA.cm.train.trl.txt\"\n",
    "train_segment_label_path = r\"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\database_segment_labels\\\\database\\\\segment_labels\\\\train_seglab_0.16.npy\"\n",
    "\n",
    "val_audio_path = r\"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\dev\\\\con_wav\\\\*.wav\"\n",
    "val_utterance_label_path = r\"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\protocols\\\\PartialSpoof_LA_cm_protocols\\\\PartialSpoof.LA.cm.dev.trl.txt\"\n",
    "val_segment_label_path = r\"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\database_segment_labels\\\\database\\\\segment_labels\\\\dev_seglab_0.16.npy\"\n",
    "\n",
    "test_audio_path = r\"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\eval\\\\con_wav\\\\*.wav\"\n",
    "test_utterance_label_path = r\"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\protocols\\\\PartialSpoof_LA_cm_protocols\\\\PartialSpoof.LA.cm.eval.trl.txt\"\n",
    "test_segment_label_path = r\"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\database_segment_labels\\\\database\\\\segment_labels\\\\eval_seglab_0.16.npy\"\n",
    "\n",
    "window_size = 0.16  # Segment length in seconds\n",
    "hop_size = 0.16     # Frame shift in seconds\n",
    "   # Target feature size\n",
    "\n",
    "# Function to load labels\n",
    "def load_labels(label_file, utterance_level):\n",
    "    labels = {}\n",
    "    if utterance_level:\n",
    "        with open(label_file, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' ')\n",
    "                if len(parts) >= 5:  # Assuming the line structure is consistent\n",
    "                    file_id = parts[1].strip()  # Extract the file_id\n",
    "                    label = parts[-1].strip()   # Extract the label (e.g., \"spoof\" or \"bonafide\")\n",
    "                    labels[file_id] = 0 if label == 'spoof' else 1\n",
    "    else:\n",
    "        labels = np.load(label_file, allow_pickle=True).item()\n",
    "    return labels\n",
    "\n",
    "# Function to balance the dataset\n",
    "def balance_dataset(audio_files, labels, max_per_class):\n",
    "    spoof_files = [file for file in audio_files if os.path.basename(file).replace('.wav', '') in labels and labels[os.path.basename(file).replace('.wav', '')] == 0]\n",
    "    real_files = [file for file in audio_files if os.path.basename(file).replace('.wav', '') in labels and labels[os.path.basename(file).replace('.wav', '')] == 1]\n",
    "    balanced_spoof_files = spoof_files[:max_per_class]\n",
    "    balanced_real_files = real_files[:max_per_class]\n",
    "    return balanced_spoof_files + balanced_real_files\n",
    "\n",
    "# Main processing function\n",
    "def process_dataset(audio_files, labels, utterance_level):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    for audio_file in tqdm(audio_files, desc=\"Processing files\"):\n",
    "        file_id = os.path.basename(audio_file).replace('.wav', '')\n",
    "        if file_id not in labels:\n",
    "            continue\n",
    "        y, sr = librosa.load(audio_file, sr=16000)\n",
    "        y = librosa.util.fix_length(y, size=int(4 * sr))  # Truncate or pad to 4 seconds\n",
    "        if utterance_level:\n",
    "            label = labels[file_id]\n",
    "            handcrafted_features = extract_handcrafted_features(y, sr)\n",
    "            wav2vec_features = extract_wav2vec_features(y, sr)\n",
    "            combined_features = np.concatenate((handcrafted_features, wav2vec_features))\n",
    "            all_features.append(combined_features)\n",
    "            all_labels.append(label)\n",
    "    return np.array(all_features), np.array(all_labels)\n",
    "\n",
    "# Handcrafted and Wav2Vec feature extraction functions (as defined previously)\n",
    "def extract_handcrafted_features(segment, sr, target_size=60):\n",
    "    windowed_segment = segment * hann_window(len(segment))\n",
    "    mfcc = librosa.feature.mfcc(y=windowed_segment, sr=sr, n_mfcc=13).mean(axis=1)\n",
    "    delta_mfcc = librosa.feature.delta(mfcc)\n",
    "    # tempo = librosa.feature.rhythm.tempo(y=windowed_segment, sr=sr)[0]\n",
    "    tempo = librosa.beat.tempo(y=windowed_segment, sr=sr)[0]\n",
    "    chroma = librosa.feature.chroma_stft(y=windowed_segment, sr=sr).mean(axis=1)\n",
    "    zcr = librosa.feature.zero_crossing_rate(windowed_segment).mean()\n",
    "    energy = librosa.feature.rms(y=windowed_segment).mean()\n",
    "    pitches, _ = librosa.core.piptrack(y=windowed_segment, sr=sr)\n",
    "    pitch = np.mean(pitches[pitches > 0]) if len(pitches[pitches > 0]) > 0 else 0\n",
    "    tempogram = librosa.feature.tempogram(y=windowed_segment, sr=sr).mean(axis=1)\n",
    "    downsampled_tempogram = tempogram[::int(np.ceil(len(tempogram) / 18))]\n",
    "    \n",
    "    features = np.concatenate((mfcc, delta_mfcc, [tempo], chroma, [zcr], [energy], [pitch], downsampled_tempogram))\n",
    "    \n",
    "    # Pad or truncate features to the target size\n",
    "    if len(features) < target_size:\n",
    "        features = np.pad(features, (0, target_size - len(features)), mode='constant')\n",
    "    elif len(features) > target_size:\n",
    "        features = features[:target_size]    \n",
    "    return features\n",
    "\n",
    "def extract_wav2vec_features(segment, sr, target_size=1024):\n",
    "    if len(segment.shape) > 1:   # Ensure the segment is a 1D array and not a 2D array\n",
    "        segment = segment.flatten()  # Flatten it to 1D if needed\n",
    "    \n",
    "    inputs = feature_extractor(segment, sampling_rate=sr, return_tensors=\"pt\", padding=False).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)  # Extract the last hidden state\n",
    "    wav2vec_features = outputs.last_hidden_state.mean(dim=1).cpu().squeeze().numpy()\n",
    "    \n",
    "    # Pad or truncate to fixed size (1024)\n",
    "    if len(wav2vec_features) < target_size:\n",
    "        wav2vec_features = np.pad(wav2vec_features, (0, target_size - len(wav2vec_features)), mode='constant')\n",
    "    elif len(wav2vec_features) > target_size:\n",
    "        wav2vec_features = wav2vec_features[:target_size]\n",
    "    \n",
    "    return wav2vec_features\n",
    "\n",
    "def calculate_eer(y_true, y_scores):\n",
    "    # Calculate false acceptance rate (FAR) and false rejection rate (FRR)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = thresholds[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    return eer, eer_threshold\n",
    "\n",
    "# Load complete unbalanced datasets\n",
    "train_files = glob(train_audio_path)\n",
    "val_files = glob(val_audio_path)\n",
    "test_files = glob(test_audio_path)\n",
    "\n",
    "# Load and balance datasets\n",
    "# train_files = balance_dataset(glob(train_audio_path), train_labels, max_per_class=1200)\n",
    "# val_files = balance_dataset(glob(val_audio_path), val_labels, max_per_class=2500)\n",
    "# test_files = balance_dataset(glob(test_audio_path), test_labels, max_per_class=2500)\n",
    "\n",
    "# Load labels\n",
    "train_labels = load_labels(train_utterance_label_path, utterance_level=True)\n",
    "val_labels = load_labels(val_utterance_label_path, utterance_level=True)\n",
    "test_labels = load_labels(test_utterance_label_path, utterance_level=True)\n",
    "\n",
    "# Process datasets\n",
    "X_train, y_train = process_dataset(train_files, train_labels, utterance_level=False)\n",
    "X_val, y_val = process_dataset(val_files, val_labels, utterance_level=False)\n",
    "X_test, y_test = process_dataset(test_files, test_labels, utterance_level=False)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(512, 256), max_iter=300, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Validate the model\n",
    "y_val_pred = mlp.predict(X_val)\n",
    "y_val_pred_prob = mlp.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Test the model\n",
    "y_test_pred = mlp.predict(X_test)\n",
    "y_test_pred_prob = mlp.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "val_cm = confusion_matrix(y_val, y_val_pred)\n",
    "test_cm = confusion_matrix(y_test, y_test_pred)\n",
    "val_roc_auc = roc_auc_score(y_val, y_val_pred_prob)\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_pred_prob)\n",
    "\n",
    "# EER Computation\n",
    "val_eer, val_threshold = calculate_eer(y_val, y_val_pred_prob)\n",
    "test_eer, test_threshold = calculate_eer(y_test, y_test_pred_prob)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Validation Confusion Matrix:\\n{val_cm}\")\n",
    "print(f\"Validation ROC AUC: {val_roc_auc:.4f}\")\n",
    "print(f\"Validation EER: {val_eer:.4f}, Threshold: {val_threshold:.4f}\\n\")\n",
    "print(f\"Test Confusion Matrix:\\n{test_cm}\")\n",
    "print(f\"Test ROC AUC: {test_roc_auc:.4f}\")\n",
    "print(f\"Test EER: {test_eer:.4f}, Threshold: {test_threshold:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0117d4-abe5-4c14-8935-14a7b281eb67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
