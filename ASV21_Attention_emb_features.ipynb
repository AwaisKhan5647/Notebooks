{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b841ef34-8662-4878-b60d-21c7e1fcc9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import resnet18\n",
    "import torch.nn.functional as F  \n",
    "import torchvision.models as models  # Import this line to access pre-trained models\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40ff9df2-6763-4974-8d99-1faf1415b014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found .npy files. Loading and merging...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m max_files_to_merge \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Specify the number of files to merge\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Merge the files\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m merged_data \u001b[38;5;241m=\u001b[39m \u001b[43mmerge_files_with_limit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_files_to_merge\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Verify the result\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMerged data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmerged_data\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[28], line 31\u001b[0m, in \u001b[0;36mmerge_files_with_limit\u001b[1;34m(directory, output_file, max_files, prefix)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Reshape the 3D data to 2D: (samples * segments, features)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m reshaped_list \u001b[38;5;241m=\u001b[39m [data\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m data_list]\n\u001b[1;32m---> 31\u001b[0m merged_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreshaped_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Convert to DataFrame for saving\u001b[39;00m\n\u001b[0;32m     34\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(merged_data)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\numpy\\core\\shape_base.py:289\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    288\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[1;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def merge_files_with_limit(directory, output_file, max_files, prefix=\"W2V_DF_hidden_states_features\"):\n",
    "    \"\"\"\n",
    "    Merge a specific number of .npy or .csv files based on availability.\n",
    "\n",
    "    Parameters:\n",
    "        directory (str): Directory containing the files.\n",
    "        output_file (str): Path to save the merged file.\n",
    "        max_files (int): Maximum number of files to merge.\n",
    "        prefix (str): File prefix to search for.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Merged data.\n",
    "    \"\"\"\n",
    "    # Identify .npy files with the specified prefix\n",
    "    npy_files = [os.path.join(directory, f) for f in os.listdir(directory)\n",
    "                 if f.startswith(prefix) and f.endswith('.npy')]\n",
    "    \n",
    "    if npy_files:\n",
    "        print(\"Found .npy files. Loading and merging...\")\n",
    "        # Limit the number of files to merge\n",
    "        npy_files = npy_files[:max_files]\n",
    "        # Load and concatenate the limited number of .npy files\n",
    "        data_list = [np.load(file) for file in npy_files]\n",
    "        \n",
    "        # Reshape the 3D data to 2D: (samples * segments, features)\n",
    "        reshaped_list = [data.reshape(-1, data.shape[-1]) for data in data_list]\n",
    "        merged_data = np.vstack(reshaped_list)\n",
    "        \n",
    "        # Convert to DataFrame for saving\n",
    "        merged_df = pd.DataFrame(merged_data)\n",
    "        merged_df.to_csv(output_file, index=False)\n",
    "        return merged_df\n",
    "    else:\n",
    "        print(\".npy files not found. Checking for .csv files...\")\n",
    "        # Identify .csv files with the specified prefix\n",
    "        csv_files = [os.path.join(directory, f) for f in os.listdir(directory) \n",
    "                     if f.startswith(prefix) and f.endswith('.csv')]\n",
    "        \n",
    "        if not csv_files:\n",
    "            raise FileNotFoundError(\"No .npy or .csv files found with the specified prefix.\")\n",
    "        \n",
    "        # Limit the number of files to merge\n",
    "        csv_files = csv_files[:max_files]\n",
    "        # Load and concatenate the limited number of .csv files\n",
    "        df_list = [pd.read_csv(file) for file in csv_files]\n",
    "        merged_df = pd.concat(df_list, ignore_index=True)\n",
    "        # Save the merged DataFrame to a single CSV file\n",
    "        merged_df.to_csv(output_file, index=False)\n",
    "        return merged_df\n",
    "\n",
    "# Directory and output file paths\n",
    "csv_dir = r\"F:\\Awais_data\\Datasets\\ASV21\\Features\\SSL\\W2V\\DF\\Hidden\"\n",
    "output_file = \"merged_features_ASV21.csv\"\n",
    "max_files_to_merge = 1  # Specify the number of files to merge\n",
    "\n",
    "# Merge the files\n",
    "merged_data = merge_files_with_limit(csv_dir, output_file, max_files_to_merge)\n",
    "\n",
    "# Verify the result\n",
    "print(f\"Merged data shape: {merged_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1762abc-edf0-4833-b7b0-d0e0c07de3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- 2. Preprocess Data ---------\n",
    "def preprocess_data(df):\n",
    "    # Extract features and labels\n",
    "    file_names = df.iloc[:, 0]  # File names (not used for training)\n",
    "    labels = df.iloc[:, -1].values  # Labels (1=real, 0=fake)\n",
    "    features = df.iloc[:, 1:-1].values  # Features (1x1084)\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    features_normalized = scaler.fit_transform(features)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(features_normalized, labels, test_size=0.4, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, features_normalized, labels\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, features, labels = preprocess_data(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5c4e497-5cd0-4bdf-ad48-4148325c6815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- 3. Dataset and Dataloader ---------\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = AudioDataset(X_train, y_train)\n",
    "val_dataset = AudioDataset(X_val, y_val)\n",
    "test_dataset = AudioDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a3b8cb3-efec-48ed-82e3-9ddec03e4fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, num_classes=2, num_heads=8):\n",
    "        super(AttentionModel, self).__init__()\n",
    "\n",
    "        # Embedding layer to transform input features into the desired embedding dimension\n",
    "        self.embedding = nn.Linear(input_dim, embedding_dim)\n",
    "\n",
    "        # Multi-Head Self Attention Layer\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "        # Feed Forward Network after Attention\n",
    "        self.fc1 = nn.Linear(embedding_dim, embedding_dim * 2)\n",
    "        self.fc2 = nn.Linear(embedding_dim * 2, num_classes)\n",
    "\n",
    "        # Layer Normalization for stability\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding input to match the attention input size\n",
    "        x = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "        # Adding a batch dimension for attention mechanism\n",
    "        x = x.unsqueeze(1)  # Add a sequence length dimension [batch_size, 1, embedding_dim]\n",
    "\n",
    "        # Attention Layer (self-attention)\n",
    "        attn_output, attn_weights = self.attention(x, x, x)  # (batch_size, seq_len, embedding_dim), (batch_size, seq_len, seq_len)\n",
    "        \n",
    "        # Layer normalization after attention\n",
    "        x = self.layer_norm(attn_output + x)\n",
    "\n",
    "        # Feed Forward Network\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x.squeeze(1), attn_weights  # Squeeze to get batch_size, num_classes output\n",
    "\n",
    "input_dim = 512\n",
    "embedding_dim = 128\n",
    "num_classes = 2\n",
    "# Instantiate the model\n",
    "model = AttentionModel(input_dim=input_dim, embedding_dim=embedding_dim, num_classes=num_classes).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1297bdcd-f220-4691-998e-50d630ccddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- NT-Xent Loss ---------\n",
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, z_i, z_j):\n",
    "        z_i = nn.functional.normalize(z_i, dim=1)\n",
    "        z_j = nn.functional.normalize(z_j, dim=1)\n",
    "        sim_matrix = torch.mm(z_i, z_j.t()) / self.temperature\n",
    "        labels = torch.arange(z_i.size(0)).cuda()\n",
    "        loss = nn.CrossEntropyLoss()(sim_matrix, labels)\n",
    "        return loss\n",
    "\n",
    "\n",
    "criterion = NTXentLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9eba488d-79e4-41c9-bece-f8b9bc1f1dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    " #--------- 6. Optimizer ---------\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2da1112a-27ed-49ac-968b-c622fb5daf4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x510 and 512x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 82\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, EER: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meer\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     76\u001b[0m             \u001b[38;5;66;03m# Confusion Matrix Plot\u001b[39;00m\n\u001b[0;32m     77\u001b[0m             \u001b[38;5;66;03m# plot_confusion_matrix(cm, classes=[\"Fake\", \"Real\"], title=f'Confusion Matrix Epoch {epoch+1}')\u001b[39;00m\n\u001b[0;32m     78\u001b[0m             \u001b[38;5;66;03m# plt.savefig(f'confusion_matrix_epoch_{epoch+1}.png')\u001b[39;00m\n\u001b[0;32m     79\u001b[0m             \u001b[38;5;66;03m# plt.close()\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Training the model with validation evaluation\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m \u001b[43mtrain_contrastive_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 60\u001b[0m, in \u001b[0;36mtrain_contrastive_model\u001b[1;34m(model, criterion, optimizer, train_loader, val_loader, epochs)\u001b[0m\n\u001b[0;32m     57\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m logits, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     63\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()(logits, y)  \u001b[38;5;66;03m# Using cross-entropy for classification\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[22], line 23\u001b[0m, in \u001b[0;36mAttentionModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Embedding input to match the attention input size\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch_size, seq_len, embedding_dim]\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Adding a batch dimension for attention mechanism\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Add a sequence length dimension [batch_size, 1, embedding_dim]\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x510 and 512x128)"
     ]
    }
   ],
   "source": [
    "def calculate_eer(fpr, tpr):\n",
    "    # Calculate EER (Equal Error Rate) from the FPR and TPR\n",
    "    eer_index = np.nanargmin(np.abs(fpr - (1 - tpr)))  # Find where the FPR and TPR are closest\n",
    "    eer = (fpr[eer_index] + (1 - tpr[eer_index])) / 2\n",
    "    return eer\n",
    "\n",
    "def evaluate_model(model, val_loader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():  # No gradients needed for evaluation\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            logits, _ = model(x)  # Forward pass\n",
    "            preds = torch.softmax(logits, dim=1)[:, 1]  # Assuming binary classification, take the positive class\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "\n",
    "    # Calculate AUC\n",
    "    auc_score = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "    # Calculate Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, (all_preds > 0.5).astype(int))\n",
    "\n",
    "    # Compute ROC curve and AUC\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_preds)\n",
    "    eer = calculate_eer(fpr, tpr)\n",
    "\n",
    "    return auc_score, cm, eer\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "def train_contrastive_model(model, criterion, optimizer, train_loader, val_loader=None, epochs=50):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            logits, _ = model(x)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = nn.CrossEntropyLoss()(logits, y)  # Using cross-entropy for classification\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        if val_loader:\n",
    "            # Evaluate model on validation set after each epoch\n",
    "            auc_score, cm, eer = evaluate_model(model, val_loader, device)\n",
    "            print(f\"Validation AUC: {auc_score:.4f}, EER: {eer:.4f}\")\n",
    "\n",
    "            # Confusion Matrix Plot\n",
    "            # plot_confusion_matrix(cm, classes=[\"Fake\", \"Real\"], title=f'Confusion Matrix Epoch {epoch+1}')\n",
    "            # plt.savefig(f'confusion_matrix_epoch_{epoch+1}.png')\n",
    "            # plt.close()\n",
    "\n",
    "# Training the model with validation evaluation\n",
    "train_contrastive_model(model, criterion, optimizer, train_loader, val_loader, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052d199e-8bf3-47e3-8a54-0eb540131085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- 8. t-SNE Visualization ---------\n",
    "def plot_tsne(features, labels, title, save_path):\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_features = tsne.fit_transform(features)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(tsne_features[:, 0], tsne_features[:, 1], c=labels, cmap='viridis', s=10)\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "# t-SNE before training\n",
    "plot_tsne(features[:80000], labels[:80000], \"t-SNE Before Training\", \"tsne_before.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa487f8b-3079-439e-a984-ea7962fef9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE after training\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f57c40-1e2b-4391-a026-fa3b2eeb0c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use torch.float32 dtype for input\n",
    "with torch.no_grad():\n",
    "    final_embeddings, _ = model(torch.tensor(features[:80000], dtype=torch.float32).cuda())  # Get only the embeddings (ignore attention_weights)\n",
    "    final_embeddings = final_embeddings.cpu().numpy()  # Convert embeddings to CPU for visualization\n",
    "\n",
    "plot_tsne(final_embeddings, labels[:80000], \"t-SNE After Training\", \"tsne_after.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c9ea0-d1f8-41b1-91b5-3a90a109ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f228b0-f2a5-4ee5-9ee0-05cce2ec829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_summary(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name}: {param.size()}\")\n",
    "\n",
    "model_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478c5cb5-03bb-4fb7-8f18-ad84268198c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58557512-d855-47e2-9170-eca270cec624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "# Define local directory to save the model\n",
    "\n",
    "save_directory = r\"C:\\Notebooks\\rrl_source\\Spectnet_model_ASV21_embedding\"  # Replace with your preferred path\n",
    "os.makedirs(save_directory, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "# Define file paths for model and weights\n",
    "model_path = os.path.join(save_directory, \"specnet_ASV21_model.pth\")\n",
    "weights_path = os.path.join(save_directory, \"specnet_ASV21_weights.pth\")\n",
    "\n",
    "# Save the trained SpecNet model and weights\n",
    "torch.save(model.state_dict(), weights_path)  # Save weights\n",
    "torch.save(model, model_path)                # Save the entire model\n",
    "\n",
    "print(f\"Trained model saved at: {model_path}\")\n",
    "print(f\"Trained model weights saved at: {weights_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6626e6-6712-41d7-b130-ac57e4815ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "# Define the paths to the saved model and weights\n",
    "os.chdir(r'C:\\Notebooks\\rrl_source\\Spectnet_model_ASV21_embedding')\n",
    "\n",
    "model_path = \".\\specnet_Halftruth_model.pth\"  # Replace with your saved model path\n",
    "weights_path = \".\\specnet_Halftruth_weights.pth\"  # Replace with your weights path\n",
    "\n",
    "def load_model_from_local(model_path, weights_path, device=\"cuda\"):\n",
    "    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load the full model\n",
    "    loaded_model = torch.load(model_path, map_location=device)\n",
    "    loaded_model.eval()  # Set the model to evaluation mode\n",
    "    print(\"Model and weights successfully loaded from local directory.\")\n",
    "    return loaded_model\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\"  # Specify the device to load the model\n",
    "loaded_model = load_model_from_local(model_path, weights_path, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95b56856-9565-49fc-8bc0-9ef6f8d9c3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mawais\\AppData\\Local\\Temp\\ipykernel_27548\\126609002.py:83: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.\\\\specnet_Halftruth_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 88\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel loaded successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m---> 88\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Process the CSV and save embeddings\u001b[39;00m\n\u001b[0;32m     91\u001b[0m process_features_and_save_embeddings(input_csv, output_csv_128, output_csv_256, loaded_model, device)\n",
      "Cell \u001b[1;32mIn[26], line 83\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(model_path, device)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(model_path, device):\n\u001b[0;32m     82\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(device \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 83\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel loaded successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.\\\\specnet_Halftruth_model.pth'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Define a custom dataset\n",
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, features):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx]\n",
    "\n",
    "# Function to extract embeddings\n",
    "def extract_embeddings(features, model, device=\"cuda\"):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    dataset = FeatureDataset(features)\n",
    "    dataloader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    embeddings_128 = []\n",
    "    embeddings_256 = []\n",
    "\n",
    "    with torch.no_grad():  # No gradients required for inference\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            # Forward pass through the model\n",
    "            embedded = model.embedding(batch)  # Output of the embedding layer (128 features)\n",
    "            attention_output, _ = model.attention(embedded.unsqueeze(1), embedded.unsqueeze(1), embedded.unsqueeze(1))\n",
    "            normed_output = model.layer_norm(attention_output.squeeze(1))  # Output of LayerNorm (128 features)\n",
    "            embeddings_128.append(normed_output.cpu().numpy())\n",
    "            \n",
    "            fc1_output = model.fc1(normed_output)  # Output of fc1 (256 features)\n",
    "            embeddings_256.append(fc1_output.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(embeddings_128), np.vstack(embeddings_256)\n",
    "\n",
    "# Main function\n",
    "def process_features_and_save_embeddings(input_csv, output_csv_128, output_csv_256, model, device=\"cuda\"):\n",
    "    # Load the feature data\n",
    "    data = pd.read_csv(input_csv)\n",
    "    \n",
    "    # Extract necessary columns\n",
    "    file_ids = data.iloc[:, 0].values  # FileID\n",
    "    features = data.iloc[:, 1:-1].values  # Features\n",
    "    labels = data.iloc[:, -1].values  # Label\n",
    "\n",
    "    # Extract embeddings\n",
    "    embeddings_128, embeddings_256 = extract_embeddings(features, model, device)\n",
    "\n",
    "    # Create DataFrames for 128 and 256 features\n",
    "    embedding_columns_128 = [f\"Feature{i+1}\" for i in range(embeddings_128.shape[1])]\n",
    "    df_embeddings_128 = pd.DataFrame(embeddings_128, columns=embedding_columns_128)\n",
    "    df_embeddings_128.insert(0, \"FileID\", file_ids)\n",
    "    df_embeddings_128[\"Label\"] = labels\n",
    "\n",
    "    embedding_columns_256 = [f\"Feature{i+1}\" for i in range(embeddings_256.shape[1])]\n",
    "    df_embeddings_256 = pd.DataFrame(embeddings_256, columns=embedding_columns_256)\n",
    "    df_embeddings_256.insert(0, \"FileID\", file_ids)\n",
    "    df_embeddings_256[\"Label\"] = labels\n",
    "\n",
    "    # Save the updated DataFrames to CSV files\n",
    "    df_embeddings_128.to_csv(output_csv_128, index=False)\n",
    "    df_embeddings_256.to_csv(output_csv_256, index=False)\n",
    "    print(f\"Embeddings saved to {output_csv_128} and {output_csv_256}\")\n",
    "\n",
    "# Paths\n",
    "input_csv = r\"C:\\Notebooks\\rrl_source\\Spectnet_model_ASV21_embedding\\features.csv\"  # Input file\n",
    "output_csv_128 = r\"C:\\Notebooks\\rrl_source\\Spectnet_model_ASV21_embedding\\feature_emb_128.csv\"  # 128 features file\n",
    "output_csv_256 = r\"C:\\Notebooks\\rrl_source\\Spectnet_model_ASV21_embedding\\feature_emb_256.csv\"  # 256 features file\n",
    "model_path = r\".\\specnet_Halftruth_model.pth\"  # Model path\n",
    "device = \"cuda\"\n",
    "\n",
    "# Load the model\n",
    "def load_model(model_path, device):\n",
    "    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "    model = torch.load(model_path, map_location=device)\n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully.\")\n",
    "    return model\n",
    "\n",
    "loaded_model = load_model(model_path, device)\n",
    "\n",
    "# Process the CSV and save embeddings\n",
    "process_features_and_save_embeddings(input_csv, output_csv_128, output_csv_256, loaded_model, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66410467-7bb3-4d92-98ef-790184e8bb93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2590923e-d0b0-431b-949e-9a0ac9cf57ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52a2bb4-1f17-4b66-acb5-c99cc0f5e608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab01de6-7f6c-4aa9-a3d8-7b37479ce37b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a17169-8497-422c-a9ac-ffad5330cab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd165d4-06c0-4b41-ad93-e6d7556ef8bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a960109-4130-471f-a075-cba7d9ffbb25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f596be60-a2be-4dc0-8c80-5f475774242f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aad6ca4-65b6-47ff-aa44-fb396c317d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb95f74-ff7c-4f34-b2eb-e674080e2eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf1fc87-65e8-4d99-8e81-4246091ceb7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fb5852-de34-46e0-89ee-41bd67e45b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
