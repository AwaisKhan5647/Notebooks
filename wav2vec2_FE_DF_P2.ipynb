{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "803cc5a5-40a1-4f29-9f69-560e0c395a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "268e9afb-5272-4a6f-a311-42657511aadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mawais\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# input_audio, sample_rate = librosa.load(\"/content/bla.wav\",  sr=16000)\n",
    "\n",
    "model_name = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "model = Wav2Vec2Model.from_pretrained(model_name)\n",
    "min_duration = 4.0  # Minimum duration for padding/truncation\n",
    "# i= feature_extractor(input_audio, return_tensors=\"pt\", sampling_rate=sample_rate)\n",
    "# with torch.no_grad():\n",
    "#   o= model(i.input_values)\n",
    "# print(o.keys())\n",
    "# print(o.last_hidden_state.shape)\n",
    "# print(o.extract_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "407caabf-71f6-4eee-854b-1ae0ee69ce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Wav2Vec2 model and feature extractor\n",
    "# model_name = \"facebook/wav2vec2-large-960h\"\n",
    "# feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "# model = Wav2Vec2Model.from_pretrained(model_name)\n",
    "# min_duration = 4.0  # Minimum duration for padding/truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f366e0d8-8d9f-4310-be26-dfc0995f2f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Determine if a GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1acff14b-9d86-4de5-8025-148a6d556e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Model(\n",
       "  (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): Wav2Vec2LayerNormConvLayer(\n",
       "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_projection): Wav2Vec2FeatureProjection(\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "    (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "      (conv): ParametrizedConv1d(\n",
       "        1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "        (parametrizations): ModuleDict(\n",
       "          (weight): ParametrizationList(\n",
       "            (0): _WeightNorm()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (padding): Wav2Vec2SamePadLayer()\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "        (attention): Wav2Vec2Attention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Wav2Vec2FeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move model to the GPU\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b76b628-c04a-4597-9135-07615c9db2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_file_path, device, model, feature_extractor, min_duration=4.0):\n",
    "    # Load and preprocess the audio file\n",
    "    audio, sr = librosa.load(audio_file_path, sr=16000)\n",
    "    duration = librosa.get_duration(y=audio, sr=sr)\n",
    "    if duration < min_duration:\n",
    "        pad_samples = int((min_duration - duration) * sr)\n",
    "        audio = np.pad(audio, (0, pad_samples), mode='constant')\n",
    "    elif duration > min_duration:\n",
    "        audio = audio[:int(min_duration * sr)]\n",
    "\n",
    "    audio_reshaped = np.reshape(audio, (1, -1))\n",
    "    \n",
    "    # Extract features using the feature extractor\n",
    "    input_values = feature_extractor(audio_reshaped, return_tensors=\"pt\", padding=True, sampling_rate=16000).input_values\n",
    "\n",
    "    # Move tensors to the GPU\n",
    "    input_values = input_values.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the model\n",
    "        outputs = model(input_values)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        features_last_cnn_layer = outputs.extract_features\n",
    "\n",
    "    # Move tensors back to CPU and convert to numpy arrays\n",
    "    hidden_states = hidden_states.cpu().numpy().squeeze()\n",
    "    features_last_cnn_layer = features_last_cnn_layer.cpu().numpy().squeeze()\n",
    "    \n",
    "    return hidden_states, features_last_cnn_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20e952e3-fff0-4fef-ab3e-a04c02f9b1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read labels\n",
    "def read_labels(labels_file):\n",
    "    labels_dict = {}\n",
    "    with open(labels_file, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            audio_name = parts[1]\n",
    "            label = parts[5]  # Assuming the label is at the 5th index\n",
    "            label = 1 if label == 'spoof' else 0\n",
    "            labels_dict[audio_name] = label\n",
    "    return labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdfb04b8-0ffd-4c5a-bfe0-7c2a55da3fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  65%|███████████████████████████████                 | 396708/611829 [19:00:29<7:36:12,  7.86it/s]C:\\Users\\mawais\\AppData\\Local\\Temp\\ipykernel_15092\\3272786532.py:3: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sr = librosa.load(audio_file_path, sr=16000)\n",
      "C:\\Users\\mawais\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\librosa\\core\\audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "Extracting features:  65%|██████████████████████████████▍                | 396708/611829 [19:00:29<10:18:26,  5.80it/s]\n"
     ]
    },
    {
     "ename": "NoBackendError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\librosa\\core\\audio.py:175\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 175\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\librosa\\core\\audio.py:221\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# Load the target number of frames, and transpose to match librosa form\u001b[39;00m\n\u001b[1;32m--> 221\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43msf_desc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_duration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y, sr_native\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\soundfile.py:895\u001b[0m, in \u001b[0;36mSoundFile.read\u001b[1;34m(self, frames, dtype, always_2d, fill_value, out)\u001b[0m\n\u001b[0;32m    894\u001b[0m         frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(out)\n\u001b[1;32m--> 895\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_array_io\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mread\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m>\u001b[39m frames:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\soundfile.py:1344\u001b[0m, in \u001b[0;36mSoundFile._array_io\u001b[1;34m(self, action, array, frames)\u001b[0m\n\u001b[0;32m   1343\u001b[0m cdata \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mcast(ctype \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m, array\u001b[38;5;241m.\u001b[39m__array_interface__[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 1344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cdata_io\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\soundfile.py:1354\u001b[0m, in \u001b[0;36mSoundFile._cdata_io\u001b[1;34m(self, action, data, ctype, frames)\u001b[0m\n\u001b[0;32m   1353\u001b[0m frames \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file, data, frames)\n\u001b[1;32m-> 1354\u001b[0m \u001b[43m_error_check\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_errorcode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\soundfile.py:1407\u001b[0m, in \u001b[0;36m_error_check\u001b[1;34m(err, prefix)\u001b[0m\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39mprefix)\n",
      "\u001b[1;31mLibsndfileError\u001b[0m: Error : flac decoder lost sync.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNoBackendError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m hidden_states, features_last_cnn_layer \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Store features and label if feature extraction was successful\u001b[39;00m\n\u001b[0;32m     29\u001b[0m hidden_states_list\u001b[38;5;241m.\u001b[39mappend(hidden_states)\n",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(audio_file_path, device, model, feature_extractor, min_duration)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_features\u001b[39m(audio_file_path, device, model, feature_extractor, min_duration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4.0\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Load and preprocess the audio file\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     audio, sr \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     duration \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mget_duration(y\u001b[38;5;241m=\u001b[39maudio, sr\u001b[38;5;241m=\u001b[39msr)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m duration \u001b[38;5;241m<\u001b[39m min_duration:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\librosa\\core\\audio.py:183\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n\u001b[0;32m    180\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    182\u001b[0m     )\n\u001b[1;32m--> 183\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\librosa\\util\\decorators.py:59\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDeprecated as of librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mIt will be removed in librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[0;32m     58\u001b[0m )\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\librosa\\core\\audio.py:239\u001b[0m, in \u001b[0;36m__audioread_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    236\u001b[0m     reader \u001b[38;5;241m=\u001b[39m path\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43maudioread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[0;32m    242\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\audioread\\__init__.py:132\u001b[0m, in \u001b[0;36maudio_open\u001b[1;34m(path, backends)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# All backends failed!\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m NoBackendError()\n",
      "\u001b[1;31mNoBackendError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    audio_path = \"F:\\\\Awais_data\\\\Datasets\\\\ASV21\\\\ASVspoof2021_DF_eval\\\\flac\"\n",
    "    labels_file = \"F:\\\\Awais_data\\\\Datasets\\\\ASV21\\\\ASVspoof2021_DF_eval\\\\ASVspoof2021.DF.cm.eval.trl.txt.txt\"\n",
    "    output_dir = \"F:\\\\Awais_data\\\\Datasets\\\\ASV21\\\\Features\\\\SSL\\\\\"\n",
    "\n",
    "    # Read labels\n",
    "    labels_dict = read_labels(labels_file)\n",
    "\n",
    "    # Parameters for saving\n",
    "    chunk_size = 51000\n",
    "    file_counter = 1\n",
    "    hidden_states_list = []\n",
    "    features_last_cnn_layer_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Iterate over the labeled audio files\n",
    "    for audio_name, label in tqdm(labels_dict.items(), desc=\"Extracting features\"):\n",
    "        audio_file = os.path.join(audio_path, audio_name + \".flac\")\n",
    "\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(audio_file):\n",
    "            print(f\"File '{audio_file}' not found. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Extract features\n",
    "        hidden_states, features_last_cnn_layer = extract_features(audio_file, device, model, feature_extractor)\n",
    "\n",
    "        # Store features and label if feature extraction was successful\n",
    "        hidden_states_list.append(hidden_states)\n",
    "        features_last_cnn_layer_list.append(features_last_cnn_layer)\n",
    "        labels_list.append(label)\n",
    "\n",
    "        # Save to files in chunks\n",
    "        if len(hidden_states_list) >= chunk_size:\n",
    "            # Convert lists to numpy arrays\n",
    "            hidden_states_array = np.array(hidden_states_list)\n",
    "            features_last_cnn_layer_array = np.array(features_last_cnn_layer_list)\n",
    "            labels_array = np.array(labels_list)\n",
    "\n",
    "            # Save features and labels to numpy files\n",
    "            np.save(os.path.join(output_dir, f\"XLSR_DF_hidden_states_features{file_counter:02d}.npy\"), hidden_states_array)\n",
    "            np.save(os.path.join(output_dir, f\"XLSR_DF_features_last_cnn_layer{file_counter:02d}.npy\"), features_last_cnn_layer_array)\n",
    "            np.save(os.path.join(output_dir, f\"XLSR_DF_labels{file_counter:02d}.npy\"), labels_array)\n",
    "\n",
    "            # Clear the lists and increment file counter\n",
    "            hidden_states_list = []\n",
    "            features_last_cnn_layer_list = []\n",
    "            labels_list = []\n",
    "            file_counter += 1\n",
    "\n",
    "    # Save any remaining samples\n",
    "    if hidden_states_list:\n",
    "        hidden_states_array = np.array(hidden_states_list)\n",
    "        features_last_cnn_layer_array = np.array(features_last_cnn_layer_list)\n",
    "        labels_array = np.array(labels_list)\n",
    "\n",
    "        np.save(os.path.join(output_dir, f\"XLSR_DF_hidden_states_features{file_counter:02d}.npy\"), hidden_states_array)\n",
    "        np.save(os.path.join(output_dir, f\"XLSR_DF_features_last_cnn_layer{file_counter:02d}.npy\"), features_last_cnn_layer_array)\n",
    "        np.save(os.path.join(output_dir, f\"XLSR_DF_labels{file_counter:02d}.npy\"), labels_array)\n",
    "\n",
    "    print(\"Feature extraction and saving complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebacc18d-9994-4a02-918b-d9f10b241cbc",
   "metadata": {},
   "source": [
    "For Resuming the downlaod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c67c684-2658-45ea-b460-a266492ff83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████████████████████████████████████████████| 611829/611829 [11:36:21<00:00, 14.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction and saving complete!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    audio_path = \"F:\\\\Awais_data\\\\Datasets\\\\ASV21\\\\ASVspoof2021_DF_eval\\\\flac\"\n",
    "    labels_file = \"F:\\\\Awais_data\\\\Datasets\\\\ASV21\\\\ASVspoof2021_DF_eval\\\\ASVspoof2021.DF.cm.eval.trl.txt.txt\"\n",
    "    output_dir = \"F:\\\\Awais_data\\\\Datasets\\\\ASV21\\\\Features\\\\SSL\\\\\"\n",
    "\n",
    "    # Read labels\n",
    "    labels_dict = read_labels(labels_file)\n",
    "\n",
    "    # Determine last processed file index by checking existing numpy files\n",
    "    existing_files = os.listdir(output_dir)\n",
    "    processed_files = set()\n",
    "    for file in existing_files:\n",
    "        if file.startswith(\"XLSR_DF_hidden_states_features\"):\n",
    "            file_number = int(file.split(\"features\")[1].split(\".npy\")[0])\n",
    "            processed_files.add(file_number)\n",
    "\n",
    "    # Parameters for saving\n",
    "    chunk_size = 51000\n",
    "    hidden_states_list = []\n",
    "    features_last_cnn_layer_list = []\n",
    "    labels_list = []\n",
    "    file_counter = max(processed_files) + 1 if processed_files else 1\n",
    "\n",
    "    # Iterate over the labeled audio files\n",
    "    for i, (audio_name, label) in enumerate(tqdm(labels_dict.items(), desc=\"Extracting features\"), start=1):\n",
    "        if i <= (file_counter - 1) * chunk_size:\n",
    "            continue  # Skip already processed files\n",
    "\n",
    "        audio_file = os.path.join(audio_path, audio_name + \".flac\")\n",
    "\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(audio_file):\n",
    "            print(f\"File '{audio_file}' not found. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Extract features\n",
    "        hidden_states, features_last_cnn_layer = extract_features(audio_file, device, model, feature_extractor)\n",
    "\n",
    "        # Store features and label if feature extraction was successful\n",
    "        hidden_states_list.append(hidden_states)\n",
    "        features_last_cnn_layer_list.append(features_last_cnn_layer)\n",
    "        labels_list.append(label)\n",
    "\n",
    "        # Save to files in chunks\n",
    "        if len(hidden_states_list) >= chunk_size:\n",
    "            # Convert lists to numpy arrays\n",
    "            hidden_states_array = np.array(hidden_states_list)\n",
    "            features_last_cnn_layer_array = np.array(features_last_cnn_layer_list)\n",
    "            labels_array = np.array(labels_list)\n",
    "\n",
    "            # Save features and labels to numpy files\n",
    "            np.save(os.path.join(output_dir, f\"XLSR_DF_hidden_states_features{file_counter:02d}.npy\"), hidden_states_array)\n",
    "            np.save(os.path.join(output_dir, f\"XLSR_DF_features_last_cnn_layer{file_counter:02d}.npy\"), features_last_cnn_layer_array)\n",
    "            np.save(os.path.join(output_dir, f\"XLSR_DF_labels{file_counter:02d}.npy\"), labels_array)\n",
    "\n",
    "            # Clear the lists and increment file counter\n",
    "            hidden_states_list = []\n",
    "            features_last_cnn_layer_list = []\n",
    "            labels_list = []\n",
    "            file_counter += 1\n",
    "\n",
    "    # Save any remaining samples\n",
    "    if hidden_states_list:\n",
    "        hidden_states_array = np.array(hidden_states_list)\n",
    "        features_last_cnn_layer_array = np.array(features_last_cnn_layer_list)\n",
    "        labels_array = np.array(labels_list)\n",
    "\n",
    "        np.save(os.path.join(output_dir, f\"XLSR_DF_hidden_states_features{file_counter:02d}.npy\"), hidden_states_array)\n",
    "        np.save(os.path.join(output_dir, f\"XLSR_DF_features_last_cnn_layer{file_counter:02d}.npy\"), features_last_cnn_layer_array)\n",
    "        np.save(os.path.join(output_dir, f\"XLSR_DF_labels{file_counter:02d}.npy\"), labels_array)\n",
    "\n",
    "    print(\"Feature extraction and saving complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924505db-094a-40a1-81a0-7b4a3afa13a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     audio_path = \"F:\\\\Awais_data\\\\Datasets\\\\ASV21\\\\ASVspoof2021_LA_eval\\\\flac\"\n",
    "#     labels_file = \"F:\\\\Awais_data\\\\Datasets\\\\ASV21\\\\ASVspoof2021_LA_eval\\\\ASVspoof2021.LA.cm.eval.trl.txt.txt\"\n",
    "#     output_hidden_states_file = \"F:\\\\Awais_data\\\\Datasets\\\\ASV21\\\\Features\\\\SSL\\\\W2V_LA_hidden_states_features.npy\"\n",
    "#     output_features_last_cnn_layer_file = \"F:\\\\Awais_data\\\\Datasets\\\\ASV21\\\\Features\\\\SSL\\\\W2V_LA_features_last_cnn_layer.npy\"\n",
    "#     output_labels_file = \"F:\\\\Awais_data\\\\Datasets\\\\ASV21\\\\Features\\\\SSL\\\\W2V_LA_labels.npy\"\n",
    "\n",
    "#     # Read labels\n",
    "#     labels_dict = read_labels(labels_file)\n",
    "\n",
    "#     # Extract features and store labels\n",
    "#     hidden_states_list = []\n",
    "#     features_last_cnn_layer_list = []\n",
    "#     labels_list = []\n",
    "\n",
    "#     # Iterate over the labeled audio files\n",
    "#     for audio_name, label in tqdm(labels_dict.items(), desc=\"Extracting features\"):\n",
    "#         audio_file = os.path.join(audio_path, audio_name + \".flac\")\n",
    "\n",
    "#         # Check if file exists\n",
    "#         if not os.path.exists(audio_file):\n",
    "#             print(f\"File '{audio_file}' not found. Skipping...\")\n",
    "#             continue\n",
    "\n",
    "#         # Extract features\n",
    "#         hidden_states, features_last_cnn_layer = extract_features(audio_file, device, model, feature_extractor)\n",
    "\n",
    "#         # Store features and label if feature extraction was successful\n",
    "#         hidden_states_list.append(hidden_states)\n",
    "#         features_last_cnn_layer_list.append(features_last_cnn_layer)\n",
    "#         labels_list.append(label)\n",
    "\n",
    "#     # Convert lists to numpy arrays\n",
    "#     hidden_states_array = np.array(hidden_states_list)\n",
    "#     features_last_cnn_layer_array = np.array(features_last_cnn_layer_list)\n",
    "#     labels_array = np.array(labels_list)\n",
    "\n",
    "#     # Save features and labels to numpy files\n",
    "#     np.save(output_hidden_states_file, hidden_states_array)\n",
    "#     np.save(output_features_last_cnn_layer_file, features_last_cnn_layer_array)\n",
    "#     np.save(output_labels_file, labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd6266c-f00e-4cf3-8227-1fd3dcb5dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     audio_path = \"F:\\\\Awais_data\\\\Datasets\\\\ASV21\\\\ASVspoof2021_DF_eval\\\\flac\\\\\"\n",
    "#     labels_file = \"F:\\\\Awais_data\\\\Datasets\\\\ASV21\\\\ASVspoof2021_DF_eval\\\\ASVspoof2021.DF.cm.eval.trl.txt.txt\"\n",
    "#     output_hidden_states_file = \"F:\\\\Awais_data\\\\Datasets\\\\ASV21\\\\Features\\\\SSL\\\\W2V_DF_hidden_states_features.npy\"\n",
    "#     output_features_last_cnn_layer_file = \"F:\\\\Awais_data\\\\Datasets\\\\ASV21\\\\Features\\\\SSL\\\\W2V_DF_features_last_cnn_layer.npy\"\n",
    "#     output_labels_file = \"F:\\\\Awais_data\\\\Datasets\\\\ASV21\\\\Features\\\\SSL\\\\W2V_DF_labels.npy\"\n",
    "\n",
    "#     # Read labels\n",
    "#     labels_dict = read_labels(labels_file)\n",
    "\n",
    "#     # Extract features and store labels\n",
    "#     hidden_states_list = []\n",
    "#     features_last_cnn_layer_list = []\n",
    "#     labels_list = []\n",
    "\n",
    "#     # Iterate over the labeled audio files\n",
    "#     for audio_name, label in tqdm(labels_dict.items(), desc=\"Extracting features\"):\n",
    "#         audio_file = os.path.join(audio_path, audio_name + \".flac\")\n",
    "\n",
    "#         # Check if file exists\n",
    "#         if not os.path.exists(audio_file):\n",
    "#             print(f\"File '{audio_file}' not found. Skipping...\")\n",
    "#             continue\n",
    "\n",
    "#         # Extract features\n",
    "#         hidden_states, features_last_cnn_layer = extract_features(audio_file, device, model, feature_extractor)\n",
    "\n",
    "#         # Store features and label if feature extraction was successful\n",
    "#         hidden_states_list.append(hidden_states)\n",
    "#         features_last_cnn_layer_list.append(features_last_cnn_layer)\n",
    "#         labels_list.append(label)\n",
    "\n",
    "#     # Convert lists to numpy arrays\n",
    "#     hidden_states_array = np.array(hidden_states_list)\n",
    "#     features_last_cnn_layer_array = np.array(features_last_cnn_layer_list)\n",
    "#     labels_array = np.array(labels_list)\n",
    "\n",
    "#     # Save features and labels to numpy files\n",
    "#     np.save(output_hidden_states_file, hidden_states_array)\n",
    "#     np.save(output_features_last_cnn_layer_file, features_last_cnn_layer_array)\n",
    "#     np.save(output_labels_file, labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207d5e22-896a-41d7-bd9c-382f332d6eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     audio_path = \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Train\\\\con_wav\"\n",
    "#     labels_file = \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\protocols\\\\PartialSpoof_LA_cm_protocols\\\\PartialSpoof.LA.cm.train.trl.txt\"\n",
    "#     output_hidden_states_file = \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Features\\\\training\\\\SSL\\\\XLSR_Train_hidden_states_features.npy\"\n",
    "#     output_features_last_cnn_layer_file = \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Features\\\\training\\\\SSL\\\\XLSR_Train_features_last_cnn_layer.npy\"\n",
    "#     output_labels_file = \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Features\\\\training\\\\SSL\\\\XLSR_Train_labels.npy\"\n",
    "\n",
    "#     # Read labels\n",
    "#     labels_dict = read_labels(labels_file)\n",
    "\n",
    "#     # Extract features and store labels\n",
    "#     hidden_states_list = []\n",
    "#     features_last_cnn_layer_list = []\n",
    "#     labels_list = []\n",
    "\n",
    "#     # Iterate over the labeled audio files\n",
    "#     for audio_name, label in tqdm(labels_dict.items(), desc=\"Extracting features\"):\n",
    "#         audio_file = os.path.join(audio_path, audio_name + \".wav\")\n",
    "\n",
    "#         # Check if file exists\n",
    "#         if not os.path.exists(audio_file):\n",
    "#             print(f\"File '{audio_file}' not found. Skipping...\")\n",
    "#             continue\n",
    "\n",
    "#         # Extract features\n",
    "#         hidden_states, features_last_cnn_layer = extract_features(audio_file, device, model, feature_extractor)\n",
    "\n",
    "#         # Store features and label if feature extraction was successful\n",
    "#         hidden_states_list.append(hidden_states)\n",
    "#         features_last_cnn_layer_list.append(features_last_cnn_layer)\n",
    "#         labels_list.append(label)\n",
    "\n",
    "#     # Convert lists to numpy arrays\n",
    "#     hidden_states_array = np.array(hidden_states_list)\n",
    "#     features_last_cnn_layer_array = np.array(features_last_cnn_layer_list)\n",
    "#     labels_array = np.array(labels_list)\n",
    "\n",
    "#     # Save features and labels to numpy files\n",
    "#     np.save(output_hidden_states_file, hidden_states_array)\n",
    "#     np.save(output_features_last_cnn_layer_file, features_last_cnn_layer_array)\n",
    "#     np.save(output_labels_file, labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9ced07-e3d1-4306-af7b-e69d5a8a04cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     audio_path = \"F:\\\\Awais_data\\\\Datasets\\\\asvspoof2019\\\\LA\\ASVspoof2019_LA_dev\\\\flac\\\\\"\n",
    "#     labels_file = \"F:\\\\Awais_data\\\\Datasets\\\\asvspoof2019\\\\LA\\ASVspoof2019_LA_cm_protocols\\\\ASVspoof2019.LA.cm.dev.trl.txt\"\n",
    "#     output_hidden_states_file = \"F:\\\\Awais_data\\\\Datasets\\\\asvspoof2019\\\\LA\\\\Features\\\\SSL\\\\W2V_dev_hidden_states_features.npy\"\n",
    "#     output_features_last_cnn_layer_file = \"F:\\\\Awais_data\\\\Datasets\\\\asvspoof2019\\\\LA\\\\Features\\\\SSL\\\\W2V_dev_features_last_cnn_layer.npy\"\n",
    "#     output_labels_file = \"F:\\\\Awais_data\\\\Datasets\\\\asvspoof2019\\\\LA\\\\Features\\\\SSL\\\\W2V_dev_labels.npy\"\n",
    "\n",
    "#     # Read labels\n",
    "#     labels_dict = read_labels(labels_file)\n",
    "\n",
    "#     # Extract features and store labels\n",
    "#     hidden_states_list = []\n",
    "#     features_last_cnn_layer_list = []\n",
    "#     labels_list = []\n",
    "\n",
    "#     # Iterate over the labeled audio files\n",
    "#     for audio_name, label in tqdm(labels_dict.items(), desc=\"Extracting features\"):\n",
    "#         audio_file = os.path.join(audio_path, audio_name + \".flac\")\n",
    "\n",
    "#         # Check if file exists\n",
    "#         if not os.path.exists(audio_file):\n",
    "#             print(f\"File '{audio_file}' not found. Skipping...\")\n",
    "#             continue\n",
    "\n",
    "#         # Extract features\n",
    "#         hidden_states, features_last_cnn_layer = extract_features(audio_file, device, model, feature_extractor)\n",
    "\n",
    "#         # Store features and label if feature extraction was successful\n",
    "#         hidden_states_list.append(hidden_states)\n",
    "#         features_last_cnn_layer_list.append(features_last_cnn_layer)\n",
    "#         labels_list.append(label)\n",
    "\n",
    "#     # Convert lists to numpy arrays\n",
    "#     hidden_states_array = np.array(hidden_states_list)\n",
    "#     features_last_cnn_layer_array = np.array(features_last_cnn_layer_list)\n",
    "#     labels_array = np.array(labels_list)\n",
    "\n",
    "#     # Save features and labels to numpy files\n",
    "#     np.save(output_hidden_states_file, hidden_states_array)\n",
    "#     np.save(output_features_last_cnn_layer_file, features_last_cnn_layer_array)\n",
    "#     np.save(output_labels_file, labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2641bb2a-e41c-4086-83a1-aed6d5110ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     audio_path = \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\dev\\\\con_wav\"\n",
    "#     labels_file = \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\protocols\\\\PartialSpoof_LA_cm_protocols\\\\PartialSpoof.LA.cm.dev.trl.txt\"\n",
    "#     output_hidden_states_file = \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Features\\\\training\\\\SSL\\\\XLSR_dev_hidden_states_features.npy\"\n",
    "#     output_features_last_cnn_layer_file = \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Features\\\\training\\\\SSL\\\\XLSR_dev_features_last_cnn_layer.npy\"\n",
    "#     output_labels_file = \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Features\\\\training\\\\SSL\\\\XLSR_dev_labels.npy\"\n",
    "\n",
    "#     # Read labels\n",
    "#     labels_dict = read_labels(labels_file)\n",
    "\n",
    "#     # Extract features and store labels\n",
    "#     hidden_states_list = []\n",
    "#     features_last_cnn_layer_list = []\n",
    "#     labels_list = []\n",
    "\n",
    "#     # Iterate over the labeled audio files\n",
    "#     for audio_name, label in tqdm(labels_dict.items(), desc=\"Extracting features\"):\n",
    "#         audio_file = os.path.join(audio_path, audio_name + \".wav\")\n",
    "\n",
    "#         # Check if file exists\n",
    "#         if not os.path.exists(audio_file):\n",
    "#             print(f\"File '{audio_file}' not found. Skipping...\")\n",
    "#             continue\n",
    "\n",
    "#         # Extract features\n",
    "#         hidden_states, features_last_cnn_layer = extract_features(audio_file, device, model, feature_extractor)\n",
    "\n",
    "#         # Store features and label if feature extraction was successful\n",
    "#         hidden_states_list.append(hidden_states)\n",
    "#         features_last_cnn_layer_list.append(features_last_cnn_layer)\n",
    "#         labels_list.append(label)\n",
    "\n",
    "#     # Convert lists to numpy arrays\n",
    "#     hidden_states_array = np.array(hidden_states_list)\n",
    "#     features_last_cnn_layer_array = np.array(features_last_cnn_layer_list)\n",
    "#     labels_array = np.array(labels_list)\n",
    "\n",
    "#     # Save features and labels to numpy files\n",
    "#     np.save(output_hidden_states_file, hidden_states_array)\n",
    "#     np.save(output_features_last_cnn_layer_file, features_last_cnn_layer_array)\n",
    "#     np.save(output_labels_file, labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e7b70-e2a1-40f7-91e0-f35743cc192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     audio_path = \"F:\\\\Awais_data\\\\Datasets\\\\asvspoof2019\\\\LA\\\\ASVspoof2019_LA_eval\\\\flac\\\\\"\n",
    "#     labels_file = \"F:\\\\Awais_data\\\\Datasets\\\\asvspoof2019\\\\LA\\ASVspoof2019_LA_cm_protocols\\\\ASVspoof2019.LA.cm.eval.trl.txt\"\n",
    "#     output_hidden_states_file = \"F:\\\\Awais_data\\\\Datasets\\\\asvspoof2019\\\\LA\\\\Features\\\\SSL\\\\W2V_eval_hidden_states_features.npy\"\n",
    "#     output_features_last_cnn_layer_file = \"F:\\\\Awais_data\\\\Datasets\\\\asvspoof2019\\\\LA\\\\Features\\\\SSL\\\\W2V_eval_features_last_cnn_layer.npy\"\n",
    "#     output_labels_file = \"F:\\\\Awais_data\\\\Datasets\\\\asvspoof2019\\\\LA\\\\Features\\\\SSL\\\\W2V_eval_labels.npy\"\n",
    "\n",
    "#     # Read labels\n",
    "#     labels_dict = read_labels(labels_file)\n",
    "\n",
    "#     # Extract features and store labels\n",
    "#     hidden_states_list = []\n",
    "#     features_last_cnn_layer_list = []\n",
    "#     labels_list = []\n",
    "\n",
    "#     # Iterate over the labeled audio files\n",
    "#     for audio_name, label in tqdm(labels_dict.items(), desc=\"Extracting features\"):\n",
    "#         audio_file = os.path.join(audio_path, audio_name + \".flac\")\n",
    "\n",
    "#         # Check if file exists\n",
    "#         if not os.path.exists(audio_file):\n",
    "#             print(f\"File '{audio_file}' not found. Skipping...\")\n",
    "#             continue\n",
    "\n",
    "#         # Extract features\n",
    "#         hidden_states, features_last_cnn_layer = extract_features(audio_file, device, model, feature_extractor)\n",
    "\n",
    "#         # Store features and label if feature extraction was successful\n",
    "#         hidden_states_list.append(hidden_states)\n",
    "#         features_last_cnn_layer_list.append(features_last_cnn_layer)\n",
    "#         labels_list.append(label)\n",
    "\n",
    "#     # Convert lists to numpy arrays\n",
    "#     hidden_states_array = np.array(hidden_states_list)\n",
    "#     features_last_cnn_layer_array = np.array(features_last_cnn_layer_list)\n",
    "#     labels_array = np.array(labels_list)\n",
    "\n",
    "#     # Save features and labels to numpy files\n",
    "#     np.save(output_hidden_states_file, hidden_states_array)\n",
    "#     np.save(output_features_last_cnn_layer_file, features_last_cnn_layer_array)\n",
    "#     np.save(output_labels_file, labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b437af-0d75-4df4-9b9d-4ac31e8a9e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     audio_path = \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\eval\\\\con_wav\"\n",
    "#     labels_file = \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\protocols\\\\PartialSpoof_LA_cm_protocols\\\\PartialSpoof.LA.cm.eval.trl.txt\"\n",
    "#     output_hidden_states_file = \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Features\\\\training\\\\SSL\\\\XLSR_eval_hidden_states_features.npy\"\n",
    "#     output_features_last_cnn_layer_file = \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Features\\\\training\\\\SSL\\\\XLSR_eval_features_last_cnn_layer.npy\"\n",
    "#     output_labels_file = \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Features\\\\training\\\\SSL\\\\XLSR_eval_labels.npy\"\n",
    "\n",
    "#     # Read labels\n",
    "#     labels_dict = read_labels(labels_file)\n",
    "\n",
    "#     # Extract features and store labels\n",
    "#     hidden_states_list = []\n",
    "#     features_last_cnn_layer_list = []\n",
    "#     labels_list = []\n",
    "\n",
    "#     # Iterate over the labeled audio files\n",
    "#     for audio_name, label in tqdm(labels_dict.items(), desc=\"Extracting features\"):\n",
    "#         audio_file = os.path.join(audio_path, audio_name + \".wav\")\n",
    "\n",
    "#         # Check if file exists\n",
    "#         if not os.path.exists(audio_file):\n",
    "#             print(f\"File '{audio_file}' not found. Skipping...\")\n",
    "#             continue\n",
    "\n",
    "#         # Extract features\n",
    "#         hidden_states, features_last_cnn_layer = extract_features(audio_file, device, model, feature_extractor)\n",
    "\n",
    "#         # Store features and label if feature extraction was successful\n",
    "#         hidden_states_list.append(hidden_states)\n",
    "#         features_last_cnn_layer_list.append(features_last_cnn_layer)\n",
    "#         labels_list.append(label)\n",
    "\n",
    "#     # Convert lists to numpy arrays\n",
    "#     hidden_states_array = np.array(hidden_states_list)\n",
    "#     features_last_cnn_layer_array = np.array(features_last_cnn_layer_list)\n",
    "#     labels_array = np.array(labels_list)\n",
    "\n",
    "#     # Save features and labels to numpy files\n",
    "#     np.save(output_hidden_states_file, hidden_states_array)\n",
    "#     np.save(output_features_last_cnn_layer_file, features_last_cnn_layer_array)\n",
    "#     np.save(output_labels_file, labels_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69efa762-1bef-454c-8d65-a1a9118c8050",
   "metadata": {},
   "source": [
    "After feature Extraction Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633899f1-35e2-417b-be7a-bacd1046d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4f8832-9b07-4225-aa17-348deacebfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions to compute metrics\n",
    "def compute_eer(fpr, tpr):\n",
    "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer\n",
    "\n",
    "# Load features and labels\n",
    "def load_features(file_path):\n",
    "    return np.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5095ceea-5a92-4fa0-b34e-017dca304763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_val, y_val, X_test, y_test):\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_val_prob = model.predict_proba(X_val)[:, 1]\n",
    "    y_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    val_auc = roc_auc_score(y_val, y_val_prob)\n",
    "    test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_test_prob)\n",
    "    test_eer = compute_eer(fpr, tpr)\n",
    "\n",
    "    return val_auc, test_auc, val_accuracy, test_accuracy, test_eer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6625461-b4d9-4c55-9ccb-565aee8511bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU execution\n",
    "# def evaluate_model(model, dataloader, device):\n",
    "#     model.eval()\n",
    "#     targets, outputs = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for data, target in dataloader:\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "#             output = model(data)\n",
    "#             targets.extend(target.cpu().numpy())\n",
    "#             outputs.extend(output.cpu().numpy())\n",
    "\n",
    "#     auc = roc_auc_score(targets, outputs)\n",
    "#     accuracy = accuracy_score(targets, np.round(outputs))\n",
    "\n",
    "#     fpr, tpr, _ = roc_curve(targets, outputs)\n",
    "#     eer = compute_eer(fpr, tpr)\n",
    "\n",
    "#     return auc, accuracy, eer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c797a975-7b21-4058-a1fc-14776558b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set paths to feature files\n",
    "# data_dir = \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Features\\\\training\\\\SSL\"\n",
    "# X_train_file = os.path.join(data_dir, \"Train_combined_features.npy\")\n",
    "# X_val_file = os.path.join(data_dir, \"Val_combined_features.npy\")\n",
    "# X_test_file = os.path.join(data_dir, \"Test_combined_features.npy\")\n",
    "# y_train_file = os.path.join(data_dir, \"Train_labels.npy\")\n",
    "# y_val_file = os.path.join(data_dir, \"Val_labels.npy\")\n",
    "# y_test_file = os.path.join(data_dir, \"Test_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df451da2-2506-4ba2-bb62-ac78cf40a50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths to feature files\n",
    "data_dir = \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Features\\\\training\\\\SSL\"\n",
    "X_train_hidden_file = os.path.join(data_dir, \"Train_hidden_states_features.npy\")\n",
    "X_val_hidden_file = os.path.join(data_dir, \"dev_hidden_states_features.npy\")\n",
    "X_test_hidden_file = os.path.join(data_dir, \"eval_hidden_states_features.npy\")\n",
    "X_train_cnn_file = os.path.join(data_dir, \"Train_features_last_cnn_layer.npy\")\n",
    "X_val_cnn_file = os.path.join(data_dir, \"dev_features_last_cnn_layer.npy\")\n",
    "X_test_cnn_file = os.path.join(data_dir, \"eval_features_last_cnn_layer.npy\")\n",
    "y_train_file = os.path.join(data_dir, \"Train_labels.npy\")\n",
    "y_val_file = os.path.join(data_dir, \"dev_labels.npy\")\n",
    "y_test_file = os.path.join(data_dir, \"eval_labels.npy\")\n",
    "\n",
    "# Load features and labels\n",
    "X_train_hidden = load_features(X_train_hidden_file)\n",
    "X_val_hidden = load_features(X_val_hidden_file)\n",
    "X_test_hidden = load_features(X_test_hidden_file)\n",
    "X_train_cnn = load_features(X_train_cnn_file)\n",
    "X_val_cnn = load_features(X_val_cnn_file)\n",
    "X_test_cnn = load_features(X_test_cnn_file)\n",
    "y_train = load_features(y_train_file)\n",
    "y_val = load_features(y_val_file)\n",
    "y_test = load_features(y_test_file)\n",
    "\n",
    "X_train_hidden = X_train_hidden.reshape(X_train_hidden.shape[0], -1)\n",
    "X_val_hidden = X_val_hidden.reshape(X_val_hidden.shape[0], -1)\n",
    "X_test_hidden = X_test_hidden.reshape(X_test_hidden.shape[0], -1)\n",
    "X_train_cnn = X_train_cnn.reshape(X_train_cnn.shape[0], -1)\n",
    "X_val_cnn = X_val_cnn.reshape(X_val_cnn.shape[0], -1)\n",
    "X_test_cnn = X_test_cnn.reshape(X_test_cnn.shape[0], -1)\n",
    "\n",
    "# Combine features\n",
    "X_train = X_train_hidden\n",
    "X_val = X_val_hidden\n",
    "X_test = X_test_hidden\n",
    "\n",
    "# Combine features\n",
    "# X_train = np.concatenate((X_train_hidden, X_train_cnn), axis=1)\n",
    "# X_val = np.concatenate((X_val_hidden, X_val_cnn), axis=1)\n",
    "# X_test = np.concatenate((X_test_hidden, X_test_cnn), axis=1)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afd9e39-4bc5-4ced-be3e-5a50ae36e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "# X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "# y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "# X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d28bdd9-9f3f-49bb-9018-8891467aa30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train SVM\n",
    "svm_model = SVC(probability=True, random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_val_auc, svm_test_auc, svm_val_accuracy, svm_test_accuracy, svm_test_eer = evaluate_model(svm_model, X_val, y_val, X_test, y_test)\n",
    "\n",
    "print(f\"SVM - AUC: {svm_test_auc}, Accuracy: {svm_test_accuracy}, EER: {svm_test_eer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15048e3c-3573-42c0-b5d7-80840995624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP model for GPU\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, input_size):\n",
    "#         super(MLP, self).__init__()\n",
    "#         self.layers = nn.Sequential(\n",
    "#             nn.Linear(input_size, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e30094-ea52-4700-af18-85a05b744297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define and train MLP\n",
    "# mlp_model = MLPClassifier(hidden_layer_sizes=(256, 128, 64), max_iter=1000, random_state=42)\n",
    "# mlp_model.fit(X_train, y_train)\n",
    "# mlp_val_auc, mlp_test_auc, mlp_val_accuracy, mlp_test_accuracy, mlp_test_eer = evaluate_model(mlp_model, X_val, y_val, X_test, y_test)\n",
    "\n",
    "# print(f\"MLP - AUC: {mlp_test_auc}, Accuracy: {mlp_test_accuracy}, EER: {mlp_test_eer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adbd3a1-0c65-4818-8241-c25749e506f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light CNN Model\n",
    "class LightCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LightCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * X_train.shape[1], 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51cdbbe-5c8c-4475-aee5-b52544e156ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lightcnn(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate_lightcnn(model, val_loader, test_loader, device):\n",
    "    model.eval()\n",
    "    val_targets, val_outputs = [], []\n",
    "    test_targets, test_outputs = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            val_targets.extend(target.cpu().numpy())\n",
    "            val_outputs.extend(output.cpu().numpy())\n",
    "\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_targets.extend(target.cpu().numpy())\n",
    "            test_outputs.extend(output.cpu().numpy())\n",
    "\n",
    "    val_auc = roc_auc_score(val_targets, val_outputs)\n",
    "    test_auc = roc_auc_score(test_targets, test_outputs)\n",
    "\n",
    "    val_accuracy = accuracy_score(val_targets, np.round(val_outputs))\n",
    "    test_accuracy = accuracy_score(test_targets, np.round(test_outputs))\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(test_targets, test_outputs)\n",
    "    test_eer = compute_eer(fpr, tpr)\n",
    "\n",
    "    return val_auc, test_auc, val_accuracy, test_accuracy, test_eer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520a776d-4114-406c-a12b-b54fd47235aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare DataLoader\n",
    "# def prepare_dataloader(X, y, batch_size=32):\n",
    "#     dataset = TensorDataset(X, y)\n",
    "#     return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# batch_size = 32\n",
    "# train_loader = prepare_dataloader(X_train_tensor, y_train_tensor, batch_size)\n",
    "# val_loader = prepare_dataloader(X_val_tensor, y_val_tensor, batch_size)\n",
    "# test_loader = prepare_dataloader(X_test_tensor, y_test_tensor, batch_size)\n",
    "\n",
    "# # Initialize and train MLP model\n",
    "# input_size = X_train.shape[1]\n",
    "# mlp_model = MLP(input_size).to(device)\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "\n",
    "# epochs = 10\n",
    "# for epoch in range(epochs):\n",
    "#     mlp_model.train()\n",
    "#     for data, target in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         output = mlp_model(data)\n",
    "#         loss = criterion(output, target)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# # Evaluate the model\n",
    "# mlp_val_auc, mlp_val_accuracy, mlp_val_eer = evaluate_model(mlp_model, val_loader, device)\n",
    "# mlp_test_auc, mlp_test_accuracy, mlp_test_eer = evaluate_model(mlp_model, test_loader, device)\n",
    "\n",
    "# print(f\"MLP - Validation AUC: {mlp_val_auc}, Validation Accuracy: {mlp_val_accuracy}, Validation EER: {mlp_val_eer}\")\n",
    "# print(f\"MLP - Test AUC: {mlp_test_auc}, Test Accuracy: {mlp_test_accuracy}, Test EER: {mlp_test_eer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc3236e-6bcf-4f6c-9be4-30f017daed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DataLoader\n",
    "def prepare_dataloader(X, y, batch_size=32):\n",
    "    tensor_X = torch.tensor(X, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
    "    tensor_y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)  # Binary classification\n",
    "    dataset = TensorDataset(tensor_X, tensor_y)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = prepare_dataloader(X_train, y_train, batch_size)\n",
    "val_loader = prepare_dataloader(X_val, y_val, batch_size)\n",
    "test_loader = prepare_dataloader(X_test, y_test, batch_size)\n",
    "\n",
    "# Train and evaluate LightCNN\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lightcnn_model = LightCNN().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(lightcnn_model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_lightcnn(lightcnn_model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "lightcnn_val_auc, lightcnn_test_auc, lightcnn_val_accuracy, lightcnn_test_accuracy, lightcnn_test_eer = evaluate_lightcnn(lightcnn_model, val_loader, test_loader, device)\n",
    "\n",
    "print(f\"LightCNN - AUC: {lightcnn_test_auc}, Accuracy: {lightcnn_test_accuracy}, EER: {lightcnn_test_eer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec2dee9-7b5a-4cfe-83ae-229a50107823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Class SVM Classifier\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Assuming that the positive class is labeled as 1\n",
    "# Transform y_train to be 1 for the positive class and -1 for the negative class\n",
    "y_train_one_class = np.where(y_train == 1, 1, -1)\n",
    "y_val_one_class = np.where(y_val == 1, 1, -1)\n",
    "y_test_one_class = np.where(y_test == 1, 1, -1)\n",
    "\n",
    "one_class_model = OneClassSVM(kernel=\"rbf\", gamma='scale', nu=0.5)\n",
    "one_class_model.fit(X_train[y_train == 1])  # Train only on the positive class\n",
    "\n",
    "# Evaluate the model\n",
    "val_scores = one_class_model.decision_function(X_val)\n",
    "test_scores = one_class_model.decision_function(X_test)\n",
    "\n",
    "val_auc = roc_auc_score(y_val_one_class, val_scores)\n",
    "test_auc = roc_auc_score(y_test_one_class, test_scores)\n",
    "\n",
    "val_accuracy = accuracy_score(y_val_one_class, np.sign(val_scores))\n",
    "test_accuracy = accuracy_score(y_test_one_class, np.sign(test_scores))\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test_one_class, test_scores)\n",
    "test_eer = compute_eer(fpr, tpr)\n",
    "\n",
    "print(f\"One-Class SVM - AUC: {test_auc}, Accuracy: {test_accuracy}, EER: {test_eer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803177d1-9810-49c3-97ff-489bb7a9d857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5385e49-4c79-4bc5-92bd-99c718c34e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de1e9c9-2984-4a76-bf1c-fd14892633c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54d70a4-80a7-4dce-89a3-3efd3d70bfcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab118004-0304-46a4-b077-c9f6a4bd4765",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
