{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a03c3c8-1e33-4633-afc6-06633c8c0cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bae1e9cc-aa46-4db4-bc39-15a60d03b877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mawais\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the pretrained Wav2Vec2 model and feature extractor\n",
    "model_name = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "model = Wav2Vec2Model.from_pretrained(model_name)\n",
    "min_duration = 4.0\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "644fc1d3-7be2-499d-8372-3ab82d493c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Model(\n",
       "  (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): Wav2Vec2LayerNormConvLayer(\n",
       "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_projection): Wav2Vec2FeatureProjection(\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "    (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "      (conv): ParametrizedConv1d(\n",
       "        1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "        (parametrizations): ModuleDict(\n",
       "          (weight): ParametrizationList(\n",
       "            (0): _WeightNorm()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (padding): Wav2Vec2SamePadLayer()\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "        (attention): Wav2Vec2Attention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Wav2Vec2FeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utilize all available GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "704c6f37-2048-4d55-9d5d-5a2096425a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_segment_labels(segment_labels_file):\n",
    "    return np.load(segment_labels_file, allow_pickle=True).item()\n",
    "\n",
    "def extract_features(audio_file, device, model, feature_extractor):\n",
    "    # Load and preprocess audio file\n",
    "    audio_input, _ = librosa.load(audio_file, sr=16000)\n",
    "    \n",
    "    # Extract input features for the model\n",
    "    input_values = feature_extractor(audio_input, sampling_rate=16000, return_tensors=\"pt\", padding=True).input_values\n",
    "    input_values = input_values.to(device)\n",
    "    \n",
    "    # Get model's hidden states and the last CNN layer features\n",
    "    with torch.no_grad():\n",
    "        if isinstance(model, torch.nn.DataParallel):\n",
    "            outputs = model.module(input_values)  # Use model.module to access the underlying model\n",
    "        else:\n",
    "            outputs = model(input_values)\n",
    "        last_cnn_layer = outputs.last_hidden_state\n",
    "    \n",
    "    return last_cnn_layer.squeeze(0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fecdad0-21e9-4032-9513-7cdd0f5af2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_for_classification(audio_dir, segment_labels_file, device, model, feature_extractor, max_files):\n",
    "    segment_labels_dict = load_segment_labels(segment_labels_file)\n",
    "    features_last_cnn_layer_list = []\n",
    "    labels_list = []\n",
    "    processed_files = 0\n",
    "\n",
    "    for audio_name, segment_labels in tqdm(segment_labels_dict.items(), desc=\"Extracting features\"):\n",
    "        if processed_files >= max_files:\n",
    "            break\n",
    "\n",
    "        audio_file = os.path.join(audio_dir, audio_name + \".wav\")\n",
    "        if not os.path.exists(audio_file):\n",
    "            print(f\"File '{audio_file}' not found. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        features_last_cnn_layer = extract_features(audio_file, device, model, feature_extractor)\n",
    "\n",
    "        # Ensure segment labels are numeric (in case they're strings)\n",
    "        if isinstance(segment_labels[0], str):\n",
    "            segment_labels = [float(label) for label in segment_labels]\n",
    "\n",
    "        # Assign segment-level labels to each frame\n",
    "        frame_labels = np.repeat(segment_labels, features_last_cnn_layer.shape[0] // len(segment_labels))\n",
    "\n",
    "        features_last_cnn_layer_list.append(features_last_cnn_layer)\n",
    "        labels_list.append(frame_labels)\n",
    "        processed_files += 1\n",
    "\n",
    "    # Flatten labels and check for unique values\n",
    "    all_labels_flat = np.concatenate(labels_list)\n",
    "    print(f\"Unique labels found: {np.unique(all_labels_flat)}\")\n",
    "    print(f\"Label counts: {np.bincount(all_labels_flat.astype(int))}\")\n",
    "\n",
    "    return features_last_cnn_layer_list, labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9a0f148-74a0-4b7c-ae74-1cab1b976646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(features):\n",
    "    return max([len(feat) for feat in features])\n",
    "\n",
    "\n",
    "def pad_to_max_length(features, labels, max_length):\n",
    "    \"\"\"\n",
    "    Pad or truncate features and labels to match the max length.\n",
    "    \n",
    "    Args:\n",
    "    - features: list of feature arrays (e.g., [feature1, feature2, ...])\n",
    "    - labels: list of corresponding labels (e.g., [label1, label2, ...])\n",
    "    - max_length: the length to which features and labels should be padded or truncated.\n",
    "    \n",
    "    Returns:\n",
    "    - padded_features: np.array of padded features\n",
    "    - padded_labels: np.array of padded or truncated labels\n",
    "    \"\"\"\n",
    "    padded_features = []\n",
    "    padded_labels = []\n",
    "\n",
    "    for i in range(len(features)):\n",
    "        feature = features[i]\n",
    "        label = labels[i]\n",
    "\n",
    "        # Padding or truncating features\n",
    "        if feature.shape[0] > max_length:\n",
    "            padded_feature = feature[:max_length]  # Truncate to max_length\n",
    "        else:\n",
    "            padding = np.zeros((max_length - feature.shape[0], feature.shape[1]))\n",
    "            padded_feature = np.vstack((feature, padding))  # Pad with zeros\n",
    "\n",
    "        padded_features.append(padded_feature)\n",
    "\n",
    "        # Ensure labels are scalar or truncate to a consistent length\n",
    "        if isinstance(label, (list, np.ndarray)):\n",
    "            if len(label) > max_length:\n",
    "                padded_label = label[:max_length]  # Truncate labels\n",
    "            else:\n",
    "                padded_label = np.pad(label, (0, max_length - len(label)), mode='constant')  # Pad with zeros\n",
    "        else:\n",
    "            padded_label = label  # If it's already a scalar, no need for padding\n",
    "\n",
    "        padded_labels.append(padded_label)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    padded_features = np.array(padded_features)\n",
    "\n",
    "    try:\n",
    "        padded_labels = np.array(padded_labels)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error while converting labels to np.array: {e}\")\n",
    "        print(f\"Shape of padded_labels: {[label.shape if isinstance(label, np.ndarray) else 'scalar' for label in padded_labels]}\")\n",
    "\n",
    "    return padded_features, padded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96ff414a-f10c-4454-965e-51dcd49284d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Extracting features:  12%|██████▌                                                 | 3000/25380 [00:44<05:33, 67.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels found: [0. 1.]\n",
      "Label counts: [ 30751 416388]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  12%|██████▊                                                 | 3000/24844 [00:48<05:52, 62.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels found: [0. 1.]\n",
      "Label counts: [ 31486 427665]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:   4%|██▎                                                     | 3000/71237 [01:36<36:23, 31.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels found: [0. 1.]\n",
      "Label counts: [    56 455189]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Wav2Vec2Model.from_pretrained('facebook/wav2vec2-base-960h').to(device)\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor()\n",
    "    \n",
    "    \n",
    "    # Process datasets\n",
    "    train_features, train_labels = extract_features_for_classification(\n",
    "        \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Train\\\\con_wav\\\\\",\n",
    "        \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\database_segment_labels\\\\database\\\\segment_labels\\\\train_seglab_0.16.npy\",\n",
    "        device, model, feature_extractor,max_files=3000\n",
    "    )\n",
    "    dev_features, dev_labels = extract_features_for_classification(\n",
    "        \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\dev\\\\con_wav\\\\\",\n",
    "        \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\database_segment_labels\\\\database\\\\segment_labels\\\\dev_seglab_0.16.npy\",\n",
    "        device, model, feature_extractor,max_files=3000\n",
    "    )\n",
    "    eval_features, eval_labels = extract_features_for_classification(\n",
    "        \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\eval\\\\con_wav\\\\\",\n",
    "        \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\database_segment_labels\\\\database\\\\segment_labels\\\\eval_seglab_0.16.npy\",\n",
    "        device, model, feature_extractor,max_files=3000\n",
    "    )\n",
    "\n",
    "    # Determine max length for padding\n",
    "    max_len = get_max_length(train_features)\n",
    "\n",
    "    # Pad features and labels\n",
    "    X_train, y_train = pad_to_max_length(train_features, train_labels, max_len)\n",
    "    X_val, y_val = pad_to_max_length(dev_features, dev_labels, max_len)\n",
    "    X_eval, y_eval = pad_to_max_length(eval_features, eval_labels, max_len)\n",
    "\n",
    "    # Convert to tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "    X_eval = torch.tensor(X_eval, dtype=torch.float32)\n",
    "    y_eval = torch.tensor(y_eval, dtype=torch.float32)\n",
    "\n",
    "    # # Create DataLoader\n",
    "    train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=8, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_val, y_val), batch_size=8, shuffle=False)\n",
    "    eval_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_eval, y_eval), batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24f4ade3-a306-4271-8451-eb530109ce07",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3000,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 140\u001b[0m\n\u001b[0;32m    137\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    139\u001b[0m    \u001b[38;5;66;03m# Process datasets\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m X_train, y_train_segmented \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_and_pad_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_len\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m X_val, y_val \u001b[38;5;241m=\u001b[39m process_and_pad_features(dev_features, dev_labels, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_len\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    142\u001b[0m X_eval, y_eval \u001b[38;5;241m=\u001b[39m process_and_pad_features(eval_features, eval_labels, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_len\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[11], line 65\u001b[0m, in \u001b[0;36mprocess_and_pad_features\u001b[1;34m(train_features, train_labels, max_len)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_and_pad_features\u001b[39m(train_features, train_labels, max_len):\n\u001b[1;32m---> 65\u001b[0m     X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mpad_to_max_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# Convert to tensors\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_train, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "Cell \u001b[1;32mIn[11], line 36\u001b[0m, in \u001b[0;36mpad_to_max_length\u001b[1;34m(features, labels, max_len)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Convert lists to numpy arrays after ensuring all elements are consistent in shape\u001b[39;00m\n\u001b[0;32m     35\u001b[0m padded_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39marray(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m padded_features])\n\u001b[1;32m---> 36\u001b[0m padded_labels \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpadded_labels\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m padded_features, padded_labels\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3000,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler, TensorDataset\n",
    "\n",
    "# Define the configuration dictionary\n",
    "config = {\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "    'num_epochs': 10,\n",
    "    'model_save_path': 'best_model.pth',\n",
    "    'max_len': 2000,  # Example maximum length\n",
    "}\n",
    "\n",
    "# Pad features and labels dynamically to max length\n",
    "def pad_to_max_length(features, labels, max_len):\n",
    "    padded_features = []\n",
    "    padded_labels = []\n",
    "\n",
    "    for feat, lbl in zip(features, labels):\n",
    "        feat_len = feat.shape[0]\n",
    "\n",
    "        if feat_len < max_len:\n",
    "            padding_feat = np.zeros((max_len - feat_len, feat.shape[1]))  # Pad features\n",
    "            padding_lbl = np.full((max_len - feat_len,), lbl[-1])  # Pad labels with the last value\n",
    "            feat_padded = np.vstack([feat, padding_feat])\n",
    "            lbl_padded = np.concatenate([lbl, padding_lbl])\n",
    "        else:\n",
    "            feat_padded = feat[:max_len]\n",
    "            lbl_padded = lbl[:max_len]\n",
    "\n",
    "        padded_features.append(feat_padded)\n",
    "        padded_labels.append(lbl_padded)\n",
    "\n",
    "    # Convert lists to numpy arrays after ensuring all elements are consistent in shape\n",
    "    padded_features = np.array([np.array(x) for x in padded_features])\n",
    "    padded_labels = np.array([np.array(x) for x in padded_labels])\n",
    "\n",
    "    return padded_features, padded_labels\n",
    "\n",
    "# Reshape and handle label segmentation dynamically\n",
    "def reshape_and_pad(y_train, X_train):\n",
    "    frames_per_segment = X_train.shape[1]  # Number of frames per segment\n",
    "    total_frames = y_train.shape[0]\n",
    "    remainder = total_frames % frames_per_segment\n",
    "\n",
    "    if remainder > 0:\n",
    "        # Calculate padding needed\n",
    "        padding_length = frames_per_segment - remainder\n",
    "        # Reshape padding to match the second dimension of y_train\n",
    "        padding = y_train[-1].repeat(padding_length, 1)  # Repeat last label with the same shape as y_train\n",
    "        y_train_padded = torch.cat([y_train, padding])\n",
    "        print(f\"Padding {padding_length} frames to y_train. New shape: {y_train_padded.shape}\")\n",
    "    else:\n",
    "        y_train_padded = y_train\n",
    "\n",
    "    # Reshape y_train to match segments\n",
    "    y_train_reshaped = y_train_padded.view(-1, frames_per_segment)\n",
    "    y_train_segmented = y_train_reshaped.flatten()  # Flatten for class balance\n",
    "\n",
    "    print(f\"Segmented y_train to shape: {y_train_segmented.shape}\")\n",
    "    return y_train_segmented\n",
    "\n",
    "# Process datasets dynamically\n",
    "def process_and_pad_features(train_features, train_labels, max_len):\n",
    "    X_train, y_train = pad_to_max_length(train_features, train_labels, max_len)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    \n",
    "    print(f\"Initial shape of X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    \n",
    "    # Reshape and handle label segmentation dynamically\n",
    "    y_train_segmented = reshape_and_pad(y_train, X_train)\n",
    "    \n",
    "    return X_train, y_train_segmented\n",
    "\n",
    "# Create a weighted sampler for class balancing\n",
    "def create_weighted_sampler(y_train_segmented):\n",
    "    class_counts = torch.bincount(y_train_segmented.long())\n",
    "    weights = 1.0 / class_counts.float()\n",
    "    sample_weights = weights[y_train_segmented.long()]\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    return sampler\n",
    "\n",
    "# Define a simple model (replace with your own model)\n",
    "class YourModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(YourModel, self).__init__()\n",
    "        self.fc = torch.nn.Linear(768, 2)  # Example linear layer (adjust as necessary)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, data_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for X_batch, y_batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch.long())\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "    \n",
    "    return total_loss / len(data_loader), precision, recall, f1, auc\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "       # Process datasets\n",
    "    X_train, y_train_segmented = process_and_pad_features(train_features, train_labels, config['max_len'])\n",
    "    X_val, y_val = process_and_pad_features(dev_features, dev_labels, config['max_len'])\n",
    "    X_eval, y_eval = process_and_pad_features(eval_features, eval_labels, config['max_len'])\n",
    "\n",
    "    # Ensure the size match after reshaping and padding\n",
    "    if X_train.shape[0] != y_train_segmented.shape[0]:\n",
    "        raise ValueError(\"Mismatch between the number of samples in X_train and y_train_segmented.\")\n",
    "    \n",
    "    # Create a weighted sampler for class balancing\n",
    "    sampler = create_weighted_sampler(y_train_segmented)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TensorDataset(X_train, y_train_segmented)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    eval_dataset = TensorDataset(X_eval, y_eval)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, sampler=sampler, batch_size=config['batch_size'])\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    eval_loader = DataLoader(eval_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    \n",
    "    # Model, optimizer, and loss function setup\n",
    "    model = YourModel().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_auc = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, val_precision, val_recall, val_f1, val_auc = evaluate_model(model, val_loader, criterion)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val AUC = {val_auc:.4f}\")\n",
    "        print(f\"Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # Save the best model based on validation loss and AUC\n",
    "        if val_loss < best_val_loss and val_auc > best_auc:\n",
    "            best_val_loss = val_loss\n",
    "            best_auc = val_auc\n",
    "            torch.save(model.state_dict(), config['model_save_path'])\n",
    "            print(f\"Best model saved with Val Loss = {val_loss:.4f}, Val AUC = {val_auc:.4f}\")\n",
    "\n",
    "    # Testing with the best saved model\n",
    "    print(\"Testing on Evaluation Data\")\n",
    "    model.load_state_dict(torch.load(config['model_save_path']))\n",
    "    test_loss, test_precision, test_recall, test_f1, test_auc = evaluate_model(model, eval_loader, criterion)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}\")\n",
    "    print(f\"Test F1: {test_f1:.4f}, Test AUC: {test_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f43a39-ac47-4e31-8ec9-65f731edd3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a20ac1-b954-4014-9128-47e5977faaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameLevelClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FrameLevelClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebebafd-4fb6-4aea-8bed-8a399c541213",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'hidden_dim': 1024,\n",
    "    'output_dim': 1,  # Binary classification\n",
    "    'num_epochs': 50,\n",
    "    'learning_rate': 0.0001,\n",
    "    'model_save_path': 'best_model.pth'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178da196-a7b8-4d3d-9594-3591b52952ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = FrameLevelClassifier(X_train.shape[2], config['hidden_dim'], config['output_dim']).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c31f7a-1021-4d36-82db-c94fd85ad62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking best model based on AUC and validation loss\n",
    "best_auc = 0\n",
    "best_val_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9017d22e-74e1-43f1-93b0-fa8128fd3edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated metrics evaluation function to handle errors\n",
    "def evaluate_metrics(true_labels, pred_labels):\n",
    "    # Ensure predictions are binary\n",
    "    unique_labels = np.unique(pred_labels)\n",
    "    \n",
    "    # Handle binary case for precision, recall, and F1\n",
    "    precision = precision_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "    recall = recall_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "    \n",
    "    # AUC calculation only if both classes are present\n",
    "    auc = None\n",
    "    if len(unique_labels) == 2:\n",
    "        auc = roc_auc_score(true_labels, pred_labels, average='weighted', multi_class='ovo')\n",
    "    else:\n",
    "        print(\"AUC calculation skipped: Only one class present in predictions.\")\n",
    "    \n",
    "    # Calculate EER (Equal Error Rate)\n",
    "    fpr, tpr, thresholds = roc_curve(true_labels.ravel(), pred_labels.ravel())\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = thresholds[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer = fpr[np.nanargmin(np.absolute((fnr - fpr)))]  # EER is the point where FPR == FNR\n",
    "    \n",
    "    return precision, recall, f1, auc, eer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bea54ef-b8f2-4ddd-99cd-b8dd36eb27c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch).squeeze()\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c766a33f-b132-4925-bd5c-37731081d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify your evaluation loop to handle missing AUC calculations\n",
    "def evaluate_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            all_outputs.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "            all_labels.append(y_batch.cpu().numpy())\n",
    "\n",
    "    # Flatten lists\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    # Binarize outputs\n",
    "    all_outputs_bin = (all_outputs > 0.5).astype(int)\n",
    "\n",
    "    # Ensure true labels are binary for metrics calculation\n",
    "    all_labels_bin = (all_labels > 0.5).astype(int)\n",
    "\n",
    "    precision, recall, f1, auc, eer = evaluate_metrics(all_labels_bin, all_outputs_bin)\n",
    "    return total_loss / len(loader), precision, recall, f1, auc, eer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdf41fb-3806-479e-8d85-07d359d0b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(config['num_epochs']):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_precision, val_recall, val_f1, val_auc, val_eer = evaluate_model(model, val_loader, criterion)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val AUC = {val_auc:.4f}, Val EER = {val_eer:.4f}\")\n",
    "    print(f\"Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Save best model based on validation loss and AUC\n",
    "    if val_loss < best_val_loss and (val_auc is None or val_auc > best_auc):\n",
    "        best_val_loss = val_loss\n",
    "        best_auc = val_auc\n",
    "        torch.save(model.state_dict(), config['model_save_path'])\n",
    "        print(f\"Best model saved with Val Loss = {val_loss:.4f}, Val AUC = {val_auc:.4f}, Val EER = {val_eer:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb10a5-8a6d-494e-9bbd-58a3b6132acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Testing with best saved model\n",
    "print(\"Testing on Evaluation Data\")\n",
    "model.load_state_dict(torch.load(config['model_save_path']))\n",
    "test_loss, test_precision, test_recall, test_f1, test_auc, test_eer = evaluate_model(model, eval_loader, criterion)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}\")\n",
    "print(f\"Test F1: {test_f1:.4f}, Test AUC: {test_auc:.4f}, Test EER: {test_eer:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98007a66-4389-4687-b3d5-103a357ae9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89680bc-b0ab-455c-814d-ead8efe8aa26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4490e1-0066-4644-9a0d-fa3ce248d375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87f62ee-b07f-40d3-a418-2ee1b90aca03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3383782d-de4c-473f-ba95-c0e0f3f72661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9d6b9d-0494-4a94-9914-efd909ed3384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98143fc1-7609-43d3-a2df-e308e5c70c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b430fd-9140-410b-9145-33f7f895e214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10322fbf-e108-49c9-b5c7-0a1b552f6e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9440ef04-db2d-47de-968d-24a55d3ba178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456636b4-5a66-484a-a3dd-545284703c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8120939-8375-4dc3-9e41-06274d5f1fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c757b28a-2ca0-4f6a-a7e7-5ebcee3ea1cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542b1174-7239-4127-9883-e0b78392e1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
