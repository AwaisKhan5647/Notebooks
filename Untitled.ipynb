{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc97b2a2-e7c3-40cc-9499-239788a4a4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e45cfb9d-7713-4dc0-b48c-bf1030891609",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8590883b-96aa-4c11-bf5e-137761ab8e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic audio sample (4 seconds of random noise, sampled at 16000 Hz)\n",
    "sample_rate = 16000\n",
    "duration = 4  # 4 seconds\n",
    "audio = np.random.randn(duration * sample_rate).astype(np.float32)\n",
    "\n",
    "# Dummy labels (one label per segment, just for demonstration)\n",
    "segment_length = 400      # 400 samples per segment\n",
    "hop_length = 160          # Overlap of 160 samples\n",
    "num_segments = (len(audio) - segment_length) // hop_length + 1\n",
    "dummy_labels = np.random.randint(0, 2, size=num_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13dfa847-080b-4f9b-9b2a-53756a0e573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature extraction function\n",
    "def extract_segment_embeddings(audio, segment_length=400, hop_length=160):\n",
    "    segments = [audio[i:i+segment_length] for i in range(0, len(audio) - segment_length, hop_length)]\n",
    "    embeddings = []\n",
    "    \n",
    "    for segment in segments:\n",
    "        # Preprocess each segment\n",
    "        inputs = processor(segment, sampling_rate=sample_rate, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # Extract embeddings without computing gradients\n",
    "        with torch.no_grad():\n",
    "            hidden_states = model(**inputs).last_hidden_state\n",
    "        \n",
    "        # Average across time dimension to obtain a 1D embedding for each segment\n",
    "        segment_embedding = torch.mean(hidden_states, dim=1).squeeze()\n",
    "        embeddings.append(segment_embedding.cpu().numpy())\n",
    "    \n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6abd3ecc-7154-4154-9314-db36a296f320",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Extract segment embeddings and print feature shape\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m segment_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mextract_segment_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of segment embeddings:\u001b[39m\u001b[38;5;124m\"\u001b[39m, segment_embeddings\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Display the shape of features and the dummy labels\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m, in \u001b[0;36mextract_segment_embeddings\u001b[1;34m(audio, segment_length, hop_length)\u001b[0m\n\u001b[0;32m      4\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m segment \u001b[38;5;129;01min\u001b[39;00m segments:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Preprocess each segment\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m(segment, sampling_rate\u001b[38;5;241m=\u001b[39msample_rate, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Extract embeddings without computing gradients\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'processor' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract segment embeddings and print feature shape\n",
    "segment_embeddings = extract_segment_embeddings(audio)\n",
    "print(\"Shape of segment embeddings:\", segment_embeddings.shape)\n",
    "\n",
    "# Display the shape of features and the dummy labels\n",
    "print(\"Shape of dummy labels:\", dummy_labels.shape)\n",
    "print(\"Features (first segment):\", segment_embeddings[0])\n",
    "print(\"Dummy labels:\", dummy_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "763b7a64-851d-4026-b46d-10f8918051ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of segment embeddings: (398, 1024)\n",
      "Shape of dummy labels: (398,)\n",
      "Features (first segment): [-0.03088856  0.21337077  0.00226705 ... -0.08575208  0.01292966\n",
      " -0.02562373]\n",
      "Dummy labels: [0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 0 0 1 0 1 0\n",
      " 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 1\n",
      " 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1\n",
      " 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 1\n",
      " 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0\n",
      " 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0\n",
      " 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 0\n",
      " 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 1 1\n",
      " 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "\n",
    "# Load the feature extractor and model\n",
    "# feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
    "# model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
    "\n",
    "model_name = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "model = Wav2Vec2Model.from_pretrained(model_name)\n",
    "\n",
    "# Create a synthetic audio sample (4 seconds of random noise, sampled at 16000 Hz)\n",
    "sample_rate = 16000\n",
    "duration = 4  # 4 seconds\n",
    "audio = np.random.randn(duration * sample_rate).astype(np.float32)\n",
    "\n",
    "# Dummy labels (one label per segment, just for demonstration)\n",
    "segment_length = 400      # 400 samples per segment\n",
    "hop_length = 160          # Overlap of 160 samples\n",
    "num_segments = (len(audio) - segment_length) // hop_length + 1\n",
    "dummy_labels = np.random.randint(0, 2, size=num_segments)\n",
    "\n",
    "# Define the feature extraction function\n",
    "def extract_segment_embeddings(audio, segment_length=400, hop_length=160):\n",
    "    segments = [audio[i:i+segment_length] for i in range(0, len(audio) - segment_length, hop_length)]\n",
    "    embeddings = []\n",
    "    \n",
    "    for segment in segments:\n",
    "        # Preprocess each segment using the feature extractor\n",
    "        inputs = feature_extractor(segment, sampling_rate=sample_rate, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # Extract embeddings without computing gradients\n",
    "        with torch.no_grad():\n",
    "            hidden_states = model(**inputs).last_hidden_state\n",
    "        \n",
    "        # Average across time dimension to obtain a 1D embedding for each segment\n",
    "        segment_embedding = torch.mean(hidden_states, dim=1).squeeze()\n",
    "        embeddings.append(segment_embedding.cpu().numpy())\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Extract segment embeddings and print feature shape\n",
    "segment_embeddings = extract_segment_embeddings(audio)\n",
    "print(\"Shape of segment embeddings:\", segment_embeddings.shape)\n",
    "\n",
    "# Display the shape of features and the dummy labels\n",
    "print(\"Shape of dummy labels:\", dummy_labels.shape)\n",
    "print(\"Features (first segment):\", segment_embeddings[0])\n",
    "print(\"Dummy labels:\", dummy_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7e3f44-8a55-4f8d-93bc-fb64c261b35d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b89d455-8458-4017-92d0-273d1b904441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          FileID  Wav2Vec2_Feature_1  Wav2Vec2_Feature_2  Wav2Vec2_Feature_3  \\\n",
      "0  CON_T_0000000           -0.031782            0.207258            0.001993   \n",
      "1  CON_T_0000000           -0.031079            0.209909            0.002559   \n",
      "2  CON_T_0000000           -0.031795            0.207865            0.002931   \n",
      "3  CON_T_0000000           -0.032433            0.206759            0.003525   \n",
      "4  CON_T_0000000           -0.031276            0.214853            0.002343   \n",
      "\n",
      "   Wav2Vec2_Feature_4  Wav2Vec2_Feature_5  Wav2Vec2_Feature_6  \\\n",
      "0           -0.014273           -0.040113            0.030700   \n",
      "1           -0.014819           -0.041912            0.031210   \n",
      "2           -0.014959           -0.042345            0.032232   \n",
      "3           -0.014638           -0.044052            0.032079   \n",
      "4           -0.013823           -0.040726            0.029733   \n",
      "\n",
      "   Wav2Vec2_Feature_7  Wav2Vec2_Feature_8  Wav2Vec2_Feature_9  ...  \\\n",
      "0            0.044732            0.028875            0.000828  ...   \n",
      "1            0.044062            0.029143            0.000116  ...   \n",
      "2            0.042668            0.028926            0.000521  ...   \n",
      "3            0.042585            0.028828            0.000006  ...   \n",
      "4            0.045856            0.027840            0.000208  ...   \n",
      "\n",
      "   Wav2Vec2_Feature_1017  Wav2Vec2_Feature_1018  Wav2Vec2_Feature_1019  \\\n",
      "0               0.102762               0.030237               0.008492   \n",
      "1               0.102146               0.031782               0.009117   \n",
      "2               0.103428               0.031916               0.008987   \n",
      "3               0.104801               0.032802               0.009140   \n",
      "4               0.103255               0.034509               0.007764   \n",
      "\n",
      "   Wav2Vec2_Feature_1020  Wav2Vec2_Feature_1021  Wav2Vec2_Feature_1022  \\\n",
      "0               0.142038              -0.159357              -0.086368   \n",
      "1               0.140877              -0.161140              -0.086679   \n",
      "2               0.142912              -0.157937              -0.087594   \n",
      "3               0.142271              -0.158701              -0.086211   \n",
      "4               0.141978              -0.166207              -0.085452   \n",
      "\n",
      "   Wav2Vec2_Feature_1023  Wav2Vec2_Feature_1024  Label  Polarity  \n",
      "0               0.013759              -0.025011      1  positive  \n",
      "1               0.013361              -0.025031      1  positive  \n",
      "2               0.014670              -0.025756      1  positive  \n",
      "3               0.013331              -0.025384      0  negative  \n",
      "4               0.011472              -0.024465      0  negative  \n",
      "\n",
      "[5 rows x 1027 columns]\n",
      "Successfully saved the modified DataFrame to C:\\Notebooks\\rrl_source\\dataset_raw\\train_segment_W2V2.csv\n"
     ]
    }
   ],
   "source": [
    "# Replace with your desired output file name\n",
    "import pandas as pd\n",
    "\n",
    "# Define the input and output CSV file paths\n",
    "input_csv_file = 'C:\\\\Notebooks\\\\rrl_source\\\\dataset_raw\\\\train_segment_Wav2Vec2.csv'  # Replace with your input file name\n",
    "output_csv_file = 'C:\\\\Notebooks\\\\rrl_source\\\\dataset_raw\\\\train_segment_W2V2.csv'  # Replace with your desired output file name\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(input_csv_file)\n",
    "print(df.head()) \n",
    "\n",
    "# Rename the feature columns\n",
    "feature_columns = [f'Wav2Vec2_Feature_{i}' for i in range(1, 1025)]  # Update range to 1 to 1024\n",
    "new_feature_columns = [f'W2V_F_{i}' for i in range(1, 1025)]  # Update range to 1 to 1024\n",
    "\n",
    "# Check if the expected columns exist in the dataframe\n",
    "if all(col in df.columns for col in feature_columns):\n",
    "    # Create a mapping dictionary for renaming\n",
    "    rename_dict = dict(zip(feature_columns, new_feature_columns))\n",
    "    \n",
    "    # Rename the columns\n",
    "    df.rename(columns=rename_dict, inplace=True)\n",
    "    \n",
    "    # Save the modified DataFrame to a new CSV file\n",
    "    df.to_csv(output_csv_file, index=False)\n",
    "    print(f'Successfully saved the modified DataFrame to {output_csv_file}')\n",
    "else:\n",
    "    print(\"Error: The expected feature columns do not exist in the input file.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349597be-f9f1-4144-b329-7b6d954267e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baaded3-be29-48c5-909b-69debb8105ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7d736d-c66e-4c75-a2b4-fce9cc7eaa16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
