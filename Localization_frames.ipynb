{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "007ab4cd-9f8f-4b01-9156-6ebe9f8f95ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a4cb585-ac99-4f76-b1e4-11044ed691f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mawais\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the pretrained Wav2Vec2 model and feature extractor\n",
    "model_name = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "model = Wav2Vec2Model.from_pretrained(model_name)\n",
    "min_duration = 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fed1f0b-3d3e-4c16-8fac-d57369b3875c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Model(\n",
       "  (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): Wav2Vec2LayerNormConvLayer(\n",
       "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_projection): Wav2Vec2FeatureProjection(\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "    (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "      (conv): ParametrizedConv1d(\n",
       "        1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "        (parametrizations): ModuleDict(\n",
       "          (weight): ParametrizationList(\n",
       "            (0): _WeightNorm()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (padding): Wav2Vec2SamePadLayer()\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "        (attention): Wav2Vec2Attention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Wav2Vec2FeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "633b77d5-70c3-456e-9e95-a1337d259f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_segment_labels(segment_labels_file):\n",
    "    return np.load(segment_labels_file, allow_pickle=True).item()\n",
    "\n",
    "def extract_features(audio_file, device, model, feature_extractor):\n",
    "    # Load and preprocess audio file\n",
    "    audio_input, _ = librosa.load(audio_file, sr=16000)\n",
    "    \n",
    "    # Extract input features for the XLSR model\n",
    "    input_values = feature_extractor(audio_input, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "    input_values = input_values.to(device)\n",
    "    \n",
    "    # Get model's hidden states and the last CNN layer features\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_values, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1]  # Last hidden state\n",
    "        last_cnn_layer = outputs.last_hidden_state  # Replace this with actual last CNN layer if available\n",
    "    \n",
    "    # Return as numpy arrays\n",
    "    return hidden_states.squeeze(0).cpu().numpy(), last_cnn_layer.squeeze(0).cpu().numpy()\n",
    "\n",
    "\n",
    "def extract_and_save_features(audio_dir, segment_labels_file, output_dir, chunk_size=40000, max_files=7000):\n",
    "    segment_labels_dict = load_segment_labels(segment_labels_file)\n",
    "\n",
    "    hidden_states_list = []\n",
    "    features_last_cnn_layer_list = []\n",
    "    labels_list = []\n",
    "    file_counter = 1\n",
    "    processed_files = 0\n",
    "\n",
    "    for audio_name, segment_labels in tqdm(segment_labels_dict.items(), desc=\"Extracting features\"):\n",
    "        # Stop processing if we've reached the max_files limit\n",
    "        if processed_files >= max_files:\n",
    "            break\n",
    "\n",
    "        audio_file = os.path.join(audio_dir, audio_name + \".wav\")\n",
    "        if not os.path.exists(audio_file):\n",
    "            print(f\"File '{audio_file}' not found. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Extract features\n",
    "        hidden_states, features_last_cnn_layer = extract_features(audio_file, device, model, feature_extractor)\n",
    "\n",
    "        # Assign segment-level labels to each frame\n",
    "        frame_labels = np.repeat(segment_labels, hidden_states.shape[0] // len(segment_labels))\n",
    "\n",
    "        # Store features and labels\n",
    "        hidden_states_list.append(hidden_states)\n",
    "        features_last_cnn_layer_list.append(features_last_cnn_layer)\n",
    "        labels_list.append(frame_labels)\n",
    "\n",
    "        processed_files += 1  # Increment the count of processed files\n",
    "\n",
    "        # Save in chunks\n",
    "        if len(hidden_states_list) >= chunk_size:\n",
    "            save_to_numpy(hidden_states_list, features_last_cnn_layer_list, labels_list, output_dir, file_counter)\n",
    "            hidden_states_list, features_last_cnn_layer_list, labels_list = [], [], []\n",
    "            file_counter += 1\n",
    "\n",
    "    if hidden_states_list:\n",
    "        save_to_numpy(hidden_states_list, features_last_cnn_layer_list, labels_list, output_dir, file_counter)\n",
    "        \n",
    "def extract_features(audio_file, device, model, feature_extractor):\n",
    "    # Load and preprocess audio file\n",
    "    audio_input, _ = librosa.load(audio_file, sr=16000)\n",
    "    \n",
    "    # Extract input features for the XLSR model\n",
    "    input_values = feature_extractor(audio_input, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "    input_values = input_values.to(device)\n",
    "    \n",
    "    # Get model's hidden states and the last CNN layer features\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_values, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1]  # Last hidden state\n",
    "        last_cnn_layer = outputs.last_hidden_state  # Replace this with actual last CNN layer if available\n",
    "    \n",
    "    # Return as numpy arrays\n",
    "    return hidden_states.squeeze(0).cpu().numpy(), last_cnn_layer.squeeze(0).cpu().numpy()\n",
    "\n",
    "# Main feature extraction function\n",
    "def extract_and_save_features(audio_dir, segment_labels_file, output_dir, chunk_size=40000, max_files=7000):\n",
    "    segment_labels_dict = load_segment_labels(segment_labels_file)\n",
    "\n",
    "    hidden_states_list = []\n",
    "    features_last_cnn_layer_list = []\n",
    "    labels_list = []\n",
    "    file_counter = 1\n",
    "    processed_files = 0\n",
    "\n",
    "    for audio_name, segment_labels in tqdm(segment_labels_dict.items(), desc=\"Extracting features\"):\n",
    "        # Stop processing if we've reached the max_files limit\n",
    "        if processed_files >= max_files:\n",
    "            break\n",
    "\n",
    "        audio_file = os.path.join(audio_dir, audio_name + \".wav\")\n",
    "        if not os.path.exists(audio_file):\n",
    "            print(f\"File '{audio_file}' not found. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Extract features\n",
    "        hidden_states, features_last_cnn_layer = extract_features(audio_file, device, model, feature_extractor)\n",
    "\n",
    "        # Assign segment-level labels to each frame\n",
    "        frame_labels = np.repeat(segment_labels, hidden_states.shape[0] // len(segment_labels))\n",
    "\n",
    "        # Store features and labels\n",
    "        hidden_states_list.append(hidden_states)\n",
    "        features_last_cnn_layer_list.append(features_last_cnn_layer)\n",
    "        labels_list.append(frame_labels)\n",
    "\n",
    "        processed_files += 1  # Increment the count of processed files\n",
    "\n",
    "        # Save in chunks\n",
    "        if len(hidden_states_list) >= chunk_size:\n",
    "            save_to_numpy(hidden_states_list, features_last_cnn_layer_list, labels_list, output_dir, file_counter)\n",
    "            hidden_states_list, features_last_cnn_layer_list, labels_list = [], [], []\n",
    "            file_counter += 1\n",
    "\n",
    "    if hidden_states_list:\n",
    "        save_to_numpy(hidden_states_list, features_last_cnn_layer_list, labels_list, output_dir, file_counter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e482c46-724b-475e-8542-1e32efdc8500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_numpy(hidden_states_list, features_last_cnn_layer_list, labels_list, output_dir, file_counter):\n",
    "    for i in range(len(hidden_states_list)):\n",
    "        hidden_states_array = np.array(hidden_states_list[i])\n",
    "        features_last_cnn_layer_array = np.array(features_last_cnn_layer_list[i])\n",
    "        labels_array = np.array(labels_list[i])\n",
    "\n",
    "        np.save(os.path.join(output_dir, f\"XLSR_PS_hidd_framelevel_feat{file_counter:02d}_{i:04d}.npy\"), hidden_states_array)\n",
    "        np.save(os.path.join(output_dir, f\"XLSR_PS_last_cnn_framelevel_feat{file_counter:02d}_{i:04d}.npy\"), features_last_cnn_layer_array)\n",
    "        np.save(os.path.join(output_dir, f\"XLSR_PS_labels{file_counter:02d}_{i:04d}.npy\"), labels_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9639060-353b-4847-b917-b629b599920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    base_output_path = \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Features\\\\Frame_level_features\"\n",
    "    \n",
    "    train_output_path = os.path.join(base_output_path, \"train\")\n",
    "    dev_output_path = os.path.join(base_output_path, \"dev\")\n",
    "    eval_output_path = os.path.join(base_output_path, \"eval\")\n",
    "    \n",
    "    os.makedirs(train_output_path, exist_ok=True)\n",
    "    os.makedirs(dev_output_path, exist_ok=True)\n",
    "    os.makedirs(eval_output_path, exist_ok=True)\n",
    "\n",
    "    extract_and_save_features(\"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Train\\\\con_wav\\\\\", \n",
    "                              \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\database_segment_labels\\\\database\\\\segment_labels\\\\train_seglab_0.16.npy\", \n",
    "                              train_output_path, max_files=25380)\n",
    "\n",
    "    extract_and_save_features(\"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\dev\\\\con_wav\\\\\", \n",
    "                              \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\database_segment_labels\\\\database\\\\segment_labels\\\\dev_seglab_0.16.npy\", \n",
    "                              dev_output_path, max_files=24000)\n",
    "\n",
    "    extract_and_save_features(\"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\eval\\\\con_wav\\\\\", \n",
    "                              \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\database_segment_labels\\\\database\\\\segment_labels\\\\eval_seglab_0.16.npy\", \n",
    "                              eval_output_path, max_files=71000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33d6cdc-8529-4e13-b6d5-ae6133f083cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da000f60-401b-4b6e-b6fc-f8bb7632fd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check the content of a sample file\n",
    "sample_features_file = \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Features\\\\Frame_level_features\\\\train\\\\XLSR_PS_hidd_framelevel_feat01_0000.npy\"\n",
    "sample_labels_file = \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Features\\\\Frame_level_features\\\\train\\\\XLSR_PS_labels01_0000.npy\"\n",
    "\n",
    "features = np.load(sample_features_file)\n",
    "labels = np.load(sample_labels_file)\n",
    "\n",
    "print(\"Features dtype:\", features.dtype)\n",
    "print(\"Labels dtype:\", labels.dtype)\n",
    "print(\"Features sample:\", features[:5])\n",
    "print(\"Labels sample:\", labels[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487353c0-3c55-4f18-a422-063c623e1f5a",
   "metadata": {},
   "source": [
    "Classifiaction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f07a5a-a1c3-40b3-b7f1-b4e193ca9fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7883d4f-e37b-4832-80e1-b69142252d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP model for frame-level classification\n",
    "class FrameLevelClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FrameLevelClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494756fc-06f0-4467-8978-1e4008157274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "config = {\n",
    "    'hidden_dim': 1024,\n",
    "    'output_dim': 1,  # Binary classification for each frame (real/spoof)\n",
    "    'num_epochs': 50,\n",
    "    'batch_size': 128,\n",
    "    'learning_rate': 0.0001,\n",
    "    'model_save_path': 'W2V_best_frame_level_model_PS' \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae0f559-c0f9-451a-b721-b3ea5710f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def load_data(features_file, labels_file):\n",
    "    X = np.load(features_file)\n",
    "    y = np.load(labels_file)\n",
    "\n",
    "    # Convert labels from string to float if needed\n",
    "    if np.issubdtype(y.dtype, np.str_):\n",
    "        y = y.astype(float)\n",
    "    \n",
    "    # Ensure labels are of the correct shape\n",
    "    if y.ndim == 1:\n",
    "        y = y.reshape(-1, 1)\n",
    "    \n",
    "    # Align features and labels\n",
    "    num_features = X.shape[0]\n",
    "    num_labels = y.shape[0]\n",
    "    \n",
    "    if num_features != num_labels:\n",
    "        print(f\"Warning: Number of features ({num_features}) does not match number of labels ({num_labels}).\")\n",
    "        min_len = min(num_features, num_labels)\n",
    "        X = X[:min_len]\n",
    "        y = y[:min_len]\n",
    "\n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Define DataLoader for training, validation, and test sets\n",
    "def get_data_loader(X, y, batch_size):\n",
    "    # Check tensor sizes\n",
    "    print(f\"X size: {X.size()}, y size: {y.size()}\")\n",
    "    assert X.size(0) == y.size(0), \"Size mismatch between features and labels\"\n",
    "    \n",
    "    dataset = TensorDataset(X, y)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return loader\n",
    "\n",
    "# Load datasets\n",
    "X_train, y_train = load_data(\"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Features\\\\Frame_level_features\\\\train\\\\XLSR_PS_hidd_framelevel_feat01_0000.npy\", \n",
    "                             \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Features\\\\Frame_level_features\\\\train\\\\XLSR_PS_labels01_0000.npy\")\n",
    "X_val, y_val = load_data(\"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Features\\\\Frame_level_features\\\\dev\\\\XLSR_PS_hidd_framelevel_feat01_0000.npy\", \n",
    "                         \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Features\\\\Frame_level_features\\\\dev\\\\XLSR_PS_labels01_0000.npy\")\n",
    "X_test, y_test = load_data(\"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Features\\\\Frame_level_features\\\\eval\\\\XLSR_PS_hidd_framelevel_feat01_0000.npy\", \n",
    "                           \"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\Features\\\\Frame_level_features\\\\eval\\\\XLSR_PS_labels01_0000.npy\")\n",
    "\n",
    "# Create DataLoaders for each dataset\n",
    "train_loader = get_data_loader(X_train, y_train, batch_size=32)  # Adjust batch size as needed\n",
    "val_loader = get_data_loader(X_val, y_val, batch_size=32)        # Adjust batch size as needed\n",
    "test_loader = get_data_loader(X_test, y_test, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1bfc6-4963-47ca-80f6-2eb02d14fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def pad_data(X, y, max_len):\n",
    "    num_features = X.shape[0]\n",
    "    num_labels = y.shape[0]\n",
    "\n",
    "    if num_features < max_len:\n",
    "        # Pad features\n",
    "        pad_size = max_len - num_features\n",
    "        X = np.pad(X, ((0, pad_size), (0, 0)), mode='constant', constant_values=0)\n",
    "        \n",
    "    if num_labels < max_len:\n",
    "        # Pad labels\n",
    "        pad_size = max_len - num_labels\n",
    "        y = np.pad(y, ((0, pad_size), (0, 0)), mode='constant', constant_values=-1)\n",
    "    \n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Example usage\n",
    "max_len = 142  # Use the maximum length from your dataset\n",
    "X_train, y_train = pad_data(X_train, y_train, max_len)\n",
    "X_val, y_val = pad_data(X_val, y_val, max_len)\n",
    "X_test, y_test = pad_data(X_test, y_test, max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e508042a-a621-49d9-b322-8b05c256e4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_shapes(X, y):\n",
    "    print(f\"Features shape: {X.shape}\")\n",
    "    print(f\"Labels shape: {y.shape}\")\n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        print(\"Warning: Number of features does not match number of labels after padding.\")\n",
    "    else:\n",
    "        print(\"Features and labels match after padding.\")\n",
    "\n",
    "check_shapes(X_train, y_train)\n",
    "check_shapes(X_val, y_val)\n",
    "check_shapes(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97860509-dca0-4f13-b70b-6e2307f6dfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dataloader(loader):\n",
    "    for batch in loader:\n",
    "        X_batch, y_batch = batch\n",
    "        print(f\"Batch X shape: {X_batch.shape}\")\n",
    "        print(f\"Batch y shape: {y_batch.shape}\")\n",
    "        if X_batch.shape[0] != y_batch.shape[0]:\n",
    "            print(\"Warning: Batch features and labels do not match.\")\n",
    "        break  # Check only the first batch\n",
    "\n",
    "check_dataloader(train_loader)\n",
    "check_dataloader(val_loader)\n",
    "check_dataloader(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16e1c69-4080-4f72-8404-1f9dc1233c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = FrameLevelClassifier(X_train.shape[1], config['hidden_dim'], config['output_dim']).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()  # For binary classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7646a32-77cd-4535-a9b4-11f756acd0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9885bfd5-2039-4867-a7df-137acb07fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        X_batch, y_batch = X_batch.cuda(), y_batch.float().cuda()  # Ensure target is float for BCEWithLogitsLoss\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch).squeeze()  # Remove any singleton dimensions\n",
    "        if outputs.dim() == 1:  # If outputs have shape [batch_size]\n",
    "            outputs = outputs.unsqueeze(1)  # Make shape [batch_size, 1]\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(tqdm(dataloader, desc=\"Evaluation\", leave=False)):\n",
    "            X_batch, y_batch = X_batch.cuda(), y_batch.float().cuda()\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            if outputs.dim() == 1:\n",
    "                outputs = outputs.unsqueeze(1)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "            all_outputs.extend(torch.sigmoid(outputs).cpu().numpy())  # Use sigmoid to convert logits to probabilities\n",
    "\n",
    "            # Debug information: print batch index and batch size\n",
    "            if batch_idx % 100 == 0:  # Print every 100 batches\n",
    "                print(f\"Processed batch {batch_idx}/{num_batches}, batch size: {len(X_batch)}\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_outputs = np.array(all_outputs)\n",
    "    \n",
    "    # Binary classification predictions\n",
    "    predictions = (all_outputs > 0.5).astype(int)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(all_labels, predictions)\n",
    "    \n",
    "    # Precision, Recall, F1\n",
    "    precision = precision_score(all_labels, predictions, zero_division=1)\n",
    "    recall = recall_score(all_labels, predictions)\n",
    "    f1 = f1_score(all_labels, predictions)\n",
    "    \n",
    "    # AUC\n",
    "    auc = roc_auc_score(all_labels, all_outputs)\n",
    "    \n",
    "    # EER Calculation\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_outputs)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n",
    "    eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, predictions)\n",
    "    \n",
    "    return total_loss / num_batches, accuracy, precision, recall, f1, auc, eer, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a8f2f4-8dc0-447b-b78c-5c4a11d75c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix\n",
    "\n",
    "# Early stopping parameters\n",
    "epochs_no_improve = 10\n",
    "n_epochs_stop = 10\n",
    "best_val_loss = float('inf')\n",
    "best_val_eer = float('inf')\n",
    "best_model_path = config['model_save_path']\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    print(f'Starting epoch {epoch+1}/{config[\"num_epochs\"]}')\n",
    "    train_loss = train_model(model, train_val_loader, criterion, optimizer)\n",
    "    \n",
    "    # Validate using the validation set\n",
    "    val_loss, val_accuracy, val_precision, val_recall, val_f1, val_auc, val_eer, cm = evaluate_model(model, create_dataloader(X_val, y_val, config['batch_size']), criterion)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{config[\"num_epochs\"]}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val Precision: {val_precision:.4f}, Val Recall: {val_recall:.4f}, Val F1: {val_f1:.4f}, Val AUC: {val_auc:.4f}, Val EER: {val_eer:.4f}')\n",
    "       \n",
    "    if val_eer < best_val_eer:\n",
    "        best_val_eer = val_eer\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        \n",
    "        # Save the current model\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f'Saved best model with Val EER: {val_eer:.4f} to {best_model_path}')\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if epochs_no_improve >= n_epochs_stop:\n",
    "        print(f'Early stopping at epoch {epoch+1}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9e1213-a163-40db-9584-2fd537b01f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "def test_model(model, test_loader, criterion):\n",
    "    print(\"\\nEvaluating the model on the test set:\")\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "    print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92c2ce8-e6f9-4a4c-a535-61abb71ec6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "test_loss, test_accuracy, test_precision, test_recall, test_f1, test_auc, test_eer, test_cm = evaluate_model(model, test_loader, criterion)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "print(f'Test Precision: {test_precision:.4f}')\n",
    "print(f'Test Recall: {test_recall:.4f}')\n",
    "print(f'Test F1: {test_f1:.4f}')\n",
    "print(f'Test AUC: {test_auc:.4f}')\n",
    "print(f'Test EER: {test_eer:.4f}')\n",
    "print(f'Test Confusion Matrix:\\n{test_cm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a86cb7-6ac0-4909-8960-7433c51b58ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618dcd39-161d-417f-bb27-7341d96fce86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a6f346-638d-4a29-afb1-db4ca47d7d03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0575884d-8e6d-4204-8d90-a036ca1cd186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a45e20b-6a33-44d0-a2c7-021a7eac16a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4c9ba4-f850-46db-8e02-493045612a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
