{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2363ef57-e4a7-453d-8cf7-7840dd4e8ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mawais\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25380\n",
      "22800\n",
      "2580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|                                                                         | 0/60 [00:00<?, ?it/s]C:\\Users\\mawais\\AppData\\Local\\Temp\\ipykernel_39432\\3674142855.py:59: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(y=windowed_segment, sr=sr)[0]\n",
      "Processing files: 100%|████████████████████████████████████████████████████████████████| 60/60 [00:18<00:00,  3.23it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Mismatch between number of files and predictions.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 184\u001b[0m\n\u001b[0;32m    181\u001b[0m selected_audio_file_ids \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(file)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m selected_audio_files]\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# Make sure all features and labels match correctly\u001b[39;00m\n\u001b[1;32m--> 184\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(selected_audio_files) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_pred), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch between number of files and predictions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Create the DataFrame with audio filenames, predictions, and scores\u001b[39;00m\n\u001b[0;32m    187\u001b[0m frame_level_predictions \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAudio Filename\u001b[39m\u001b[38;5;124m'\u001b[39m: selected_audio_file_ids,  \u001b[38;5;66;03m# Ensure these correspond to the files used in test\u001b[39;00m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m'\u001b[39m: y_pred,  \u001b[38;5;66;03m# These are the predictions for the test set\u001b[39;00m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScore\u001b[39m\u001b[38;5;124m'\u001b[39m: y_pred_prob  \u001b[38;5;66;03m# Probability scores\u001b[39;00m\n\u001b[0;32m    191\u001b[0m })\n",
      "\u001b[1;31mAssertionError\u001b[0m: Mismatch between number of files and predictions."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import librosa\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize paths and parameters\n",
    "model_name = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "model = Wav2Vec2Model.from_pretrained(model_name)\n",
    "\n",
    "train_audio_path = r\"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\train\\\\con_wav\\\\*.wav\"\n",
    "segment_label_path = r\"F:\\Awais_data\\Datasets\\PartialSpoof\\database_segment_labels\\database\\segment_labels\\train_seglab_0.16.npy\"\n",
    "utterance_label_path = r\"F:\\Awais_data\\Datasets\\PartialSpoof\\protocols\\PartialSpoof_LA_cm_protocols\\PartialSpoof.LA.cm.train.trl.txt\"\n",
    "save_path = r\"C:\\\\Notebooks\\\\rrl_source\\\\dataset_raw\\\\merge\\\\new\\\\\"\n",
    "\n",
    "window_size = 0.16 # Segment length in seconds\n",
    "hop_size = 0.16    # Frame shift in seconds\n",
    "extract_utterance_level = True  # Set to True for utterance-level, False for segment-level\n",
    "\n",
    "# Hann window function\n",
    "def hann_window(length):\n",
    "    return 0.5 * (1 - np.cos(2 * np.pi * np.arange(length) / (length - 1)))\n",
    "\n",
    "\n",
    "# Load segment-level or utterance-level labels\n",
    "def load_labels(label_file, utterance_level):\n",
    "    labels = {}\n",
    "    \n",
    "    if utterance_level:\n",
    "        # For utterance-level labels, we parse the label file and map the class labels to file_ids\n",
    "        with open(label_file, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' ')\n",
    "                if len(parts) >= 5:  # Assuming the line structure is consistent\n",
    "                    file_id = parts[1].strip()  # Extract the file_id\n",
    "                    label = parts[-1].strip()   # Extract the label (e.g., \"spoof\" or \"bonafide\")\n",
    "                    # Assign 0 for spoof, 1 for real/bonafide\n",
    "                    labels[file_id] = 0 if label == 'spoof' else 1\n",
    "    else:\n",
    "        # If working with segment-level labels\n",
    "        labels = np.load(label_file, allow_pickle=True).item()\n",
    "\n",
    "    return labels\n",
    "\n",
    "def extract_handcrafted_features(segment, sr, target_size=60):\n",
    "    windowed_segment = segment * hann_window(len(segment))\n",
    "    mfcc = librosa.feature.mfcc(y=windowed_segment, sr=sr, n_mfcc=13).mean(axis=1)\n",
    "    delta_mfcc = librosa.feature.delta(mfcc)\n",
    "    tempo = librosa.beat.tempo(y=windowed_segment, sr=sr)[0]\n",
    "    chroma = librosa.feature.chroma_stft(y=windowed_segment, sr=sr).mean(axis=1)\n",
    "    zcr = librosa.feature.zero_crossing_rate(windowed_segment).mean()\n",
    "    energy = librosa.feature.rms(y=windowed_segment).mean()\n",
    "    pitches, _ = librosa.core.piptrack(y=windowed_segment, sr=sr)\n",
    "    pitch = np.mean(pitches[pitches > 0]) if len(pitches[pitches > 0]) > 0 else 0\n",
    "    tempogram = librosa.feature.tempogram(y=windowed_segment, sr=sr).mean(axis=1)\n",
    "    downsampled_tempogram = tempogram[::int(np.ceil(len(tempogram) / 18))]\n",
    "    \n",
    "    features = np.concatenate((mfcc, delta_mfcc, [tempo], chroma, [zcr], [energy], [pitch], downsampled_tempogram))\n",
    "    \n",
    "    # Pad or truncate features to the target size\n",
    "    if len(features) < target_size:\n",
    "        features = np.pad(features, (0, target_size - len(features)), mode='constant')\n",
    "    elif len(features) > target_size:\n",
    "        features = features[:target_size]\n",
    "        return features\n",
    "\n",
    "# Wav2Vec2 feature extraction function\n",
    "def extract_wav2vec_features(segment, sr, target_size=1024):\n",
    "    if len(segment.shape) > 1:   # Ensure the segment is a 1D array and not a 2D array\n",
    "        segment = segment.flatten()  # Flatten it to 1D if needed\n",
    "    \n",
    "    inputs = feature_extractor(segment, sampling_rate=sr, return_tensors=\"pt\", padding=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)  # Extract the last hidden state and return it\n",
    "    wav2vec_features = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    \n",
    "    # Pad or truncate to fixed size (1024)\n",
    "    if len(wav2vec_features) < target_size:\n",
    "        wav2vec_features = np.pad(wav2vec_features, (0, target_size - len(wav2vec_features)), mode='constant')\n",
    "    elif len(wav2vec_features) > target_size:\n",
    "        wav2vec_features = wav2vec_features[:target_size]\n",
    "    \n",
    "    return wav2vec_features\n",
    "\n",
    "# Main processing function\n",
    "def process_dataset(audio_files, labels, utterance_level):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    for audio_file in tqdm(audio_files, desc=\"Processing files\"):\n",
    "        file_id = os.path.basename(audio_file).replace('.wav', '')\n",
    "        if file_id not in labels:\n",
    "            print('no file id exist')\n",
    "            continue\n",
    "\n",
    "        y, sr = librosa.load(audio_file, sr=16000)\n",
    "        if utterance_level:\n",
    "            label = labels[file_id]\n",
    "            handcrafted_features = extract_handcrafted_features(y, sr)\n",
    "            wav2vec_features = extract_wav2vec_features(y, sr)\n",
    "            combined_features = np.concatenate((handcrafted_features, wav2vec_features))\n",
    "            all_features.append(combined_features)\n",
    "            all_labels.append(label)\n",
    "        else:\n",
    "            segment_labels = labels[file_id]\n",
    "            segment_length = int(window_size * sr)\n",
    "            hop_length = int(hop_size * sr)\n",
    "            for i, seg_label in enumerate(segment_labels):\n",
    "                start = i * hop_length\n",
    "                end = start + segment_length\n",
    "                if end > len(y):\n",
    "                    break\n",
    "                segment = y[start:end]\n",
    "                handcrafted_features = extract_handcrafted_features(segment, sr)\n",
    "                wav2vec_features = extract_wav2vec_features(segment, sr)\n",
    "                combined_features = np.concatenate((handcrafted_features, wav2vec_features))\n",
    "                all_features.append(combined_features)\n",
    "                all_labels.append(seg_label)\n",
    "\n",
    "    return np.array(all_features), np.array(all_labels)\n",
    "\n",
    "# Load labels and process dataset for 100 spoof and 100 real audios\n",
    "labels = load_labels(utterance_label_path if extract_utterance_level else segment_label_path, extract_utterance_level)\n",
    "\n",
    "audio_files = glob(train_audio_path)\n",
    "print(len(audio_files))\n",
    "\n",
    "# Extract file IDs (for matching with labels)\n",
    "audio_file_ids = [os.path.basename(file).replace('.wav', '') for file in audio_files]\n",
    "# print(audio_file_ids[:10])  # Print first 10 file IDs\n",
    "\n",
    "# Now, match the files based on file_id and their labels\n",
    "spoof_files = [file for file in audio_files if os.path.basename(file).replace('.wav', '') in labels and labels[os.path.basename(file).replace('.wav', '')] == 0]\n",
    "print(len(spoof_files))\n",
    "\n",
    "real_files = [file for file in audio_files if os.path.basename(file).replace('.wav', '') in labels and labels[os.path.basename(file).replace('.wav', '')] == 1]\n",
    "print(len(real_files))\n",
    "\n",
    "# Select 100 spoof audios from the start and 100 real audios from the end\n",
    "selected_spoof_files = spoof_files[:30]\n",
    "selected_real_files = real_files[-30:]\n",
    "\n",
    "# Print names and labels of selected audios\n",
    "for audio_file in selected_spoof_files:\n",
    "    file_id = os.path.basename(audio_file).replace('.wav', '')\n",
    "    # print(f\"Selected spoof audio: {file_id} - Label: {labels[file_id]}\")\n",
    "\n",
    "for audio_file in selected_real_files:\n",
    "    file_id = os.path.basename(audio_file).replace('.wav', '')\n",
    "    # print(f\"Selected real audio: {file_id} - Label: {labels[file_id]}\")\n",
    "\n",
    "\n",
    "# Assuming you are only processing the selected spoof and real files for evaluation\n",
    "# selected_audio_files = selected_spoof_files + selected_real_files\n",
    "\n",
    "# Process the dataset\n",
    "features, labels = process_dataset(selected_audio_files, labels, extract_utterance_level)\n",
    "\n",
    "# After processing, ensure that features and labels are aligned correctly\n",
    "# assert len(features) == len(selected_audio_files), \"Mismatch between number of features and audio files\"\n",
    "\n",
    "# Split dataset into train, validation, and test sets (80-10-10 split)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Ensure that you only select predictions for the selected test audio files\n",
    "selected_audio_file_ids = [os.path.basename(file).replace('.wav', '') for file in selected_audio_files]\n",
    "\n",
    "# Make sure all features and labels match correctly\n",
    "assert len(selected_audio_files) == len(y_pred), \"Mismatch between number of files and predictions.\"\n",
    "\n",
    "# Create the DataFrame with audio filenames, predictions, and scores\n",
    "frame_level_predictions = pd.DataFrame({\n",
    "    'Audio Filename': selected_audio_file_ids,  # Ensure these correspond to the files used in test\n",
    "    'Prediction': y_pred,  # These are the predictions for the test set\n",
    "    'Score': y_pred_prob  # Probability scores\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "frame_level_predictions.to_csv(\"frame_level_predictions.csv\", index=False)\n",
    "\n",
    "print(\"Evaluation complete and results saved.\")\n",
    "\n",
    "# Standardize the features (important for MLP performance)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train an MLP classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(512, 256), max_iter=300, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = mlp.predict(X_test)\n",
    "y_pred_prob = mlp.predict_proba(X_test)[:, 1]  # Probabilities for ROC AUC and EER\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# EER calculation\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test, y_pred_prob)\n",
    "eer = thresholds[np.nanargmin(np.abs(fpr - (1 - tpr)))]\n",
    "\n",
    "# Save evaluation results to a .txt file\n",
    "with open(\"evaluation_results.txt\", \"w\") as f:\n",
    "    f.write(f\"Confusion Matrix:\\n{cm}\\n\\n\")\n",
    "    f.write(f\"ROC AUC: {roc_auc}\\n\")\n",
    "    f.write(f\"EER: {eer}\\n\")\n",
    "\n",
    "# Save frame-level predictions to CSV\n",
    "frame_level_predictions = pd.DataFrame({\n",
    "    'Audio Filename': selected_audio_files,  # Ensure these correspond to the files used in test\n",
    "    'Prediction': y_pred,  # These are the predictions for the test set\n",
    "    'Score': y_pred_prob  # Probability scores\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "frame_level_predictions.to_csv(\"frame_level_predictions.csv\", index=False)\n",
    "\n",
    "print(\"Evaluation complete and results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "33762d97-45a9-459d-943a-c007a2cd8cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected audio files: 60\n",
      "Number of features extracted: 60\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of selected audio files: {len(selected_audio_files)}\")\n",
    "print(f\"Number of features extracted: {len(features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "25a7ec9f-25d8-4436-9dec-36e791ebcde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_test: (3, 1084)\n",
      "Shape of y_pred: (3,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_pred: {y_pred.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0481f382-a444-4a55-9609-e0b750f23fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c6f108-b5ea-45d9-b7eb-18b1730f186f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9749b6ee-c6e6-43b0-9367-219783249661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Notebooks'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ed41237-96e0-4a19-b40f-b1c19c004031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25380\n",
      "22800\n",
      "2580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|                                                                         | 0/75 [00:00<?, ?it/s]C:\\Users\\mawais\\AppData\\Local\\Temp\\ipykernel_2792\\4132320622.py:57: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(y=windowed_segment, sr=sr)[0]\n",
      "Processing files: 100%|████████████████████████████████████████████████████████████████| 75/75 [14:49<00:00, 11.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[48  0]\n",
      " [ 0 27]]\n",
      "ROC AUC: 1.0000\n",
      "EER: 0.0000\n",
      "Evaluation results and predictions saved to C:\\Notebooks\\rrl_source\\dataset_raw\\merge\\new\\evaluation_results.txt and C:\\Notebooks\\rrl_source\\dataset_raw\\merge\\new\\frame_level_predictions.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import librosa\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize paths and parameters\n",
    "model_name = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "model = Wav2Vec2Model.from_pretrained(model_name)\n",
    "\n",
    "train_audio_path = r\"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\train\\\\con_wav\\\\*.wav\"\n",
    "segment_label_path = r\"F:\\Awais_data\\Datasets\\PartialSpoof\\database_segment_labels\\database\\segment_labels\\train_seglab_0.16.npy\"\n",
    "utterance_label_path = r\"F:\\Awais_data\\Datasets\\PartialSpoof\\protocols\\PartialSpoof_LA_cm_protocols\\PartialSpoof.LA.cm.train.trl.txt\"\n",
    "save_path = r\"C:\\\\Notebooks\\\\rrl_source\\\\dataset_raw\\\\merge\\\\new\\\\\"\n",
    "\n",
    "window_size = 0.16 # Segment length in seconds\n",
    "hop_size = 0.16    # Frame shift in seconds\n",
    "extract_utterance_level = True  # Set to True for utterance-level, False for segment-level\n",
    "\n",
    "# Hann window function\n",
    "def hann_window(length):\n",
    "    return 0.5 * (1 - np.cos(2 * np.pi * np.arange(length) / (length - 1)))\n",
    "\n",
    "# Load segment-level or utterance-level labels\n",
    "def load_labels(label_file, utterance_level):\n",
    "    labels = {}\n",
    "    \n",
    "    if utterance_level:\n",
    "        # For utterance-level labels, we parse the label file and map the class labels to file_ids\n",
    "        with open(label_file, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' ')\n",
    "                if len(parts) >= 5:  # Assuming the line structure is consistent\n",
    "                    file_id = parts[1].strip()  # Extract the file_id\n",
    "                    label = parts[-1].strip()   # Extract the label (e.g., \"spoof\" or \"bonafide\")\n",
    "                    # Assign 0 for spoof, 1 for real/bonafide\n",
    "                    labels[file_id] = 0 if label == 'spoof' else 1\n",
    "    else:\n",
    "        # If working with segment-level labels\n",
    "        labels = np.load(label_file, allow_pickle=True).item()\n",
    "\n",
    "    return labels\n",
    "\n",
    "def extract_handcrafted_features(segment, sr, target_size=60):\n",
    "    windowed_segment = segment * hann_window(len(segment))\n",
    "    mfcc = librosa.feature.mfcc(y=windowed_segment, sr=sr, n_mfcc=13).mean(axis=1)\n",
    "    delta_mfcc = librosa.feature.delta(mfcc)\n",
    "    # tempo = librosa.feature.rhythm.tempo(y=windowed_segment, sr=sr)[0]\n",
    "    tempo = librosa.beat.tempo(y=windowed_segment, sr=sr)[0]\n",
    "    chroma = librosa.feature.chroma_stft(y=windowed_segment, sr=sr).mean(axis=1)\n",
    "    zcr = librosa.feature.zero_crossing_rate(windowed_segment).mean()\n",
    "    energy = librosa.feature.rms(y=windowed_segment).mean()\n",
    "    pitches, _ = librosa.core.piptrack(y=windowed_segment, sr=sr)\n",
    "    pitch = np.mean(pitches[pitches > 0]) if len(pitches[pitches > 0]) > 0 else 0\n",
    "    tempogram = librosa.feature.tempogram(y=windowed_segment, sr=sr).mean(axis=1)\n",
    "    downsampled_tempogram = tempogram[::int(np.ceil(len(tempogram) / 18))]\n",
    "    \n",
    "    features = np.concatenate((mfcc, delta_mfcc, [tempo], chroma, [zcr], [energy], [pitch], downsampled_tempogram))\n",
    "    \n",
    "    # Pad or truncate features to the target size\n",
    "    if len(features) < target_size:\n",
    "        features = np.pad(features, (0, target_size - len(features)), mode='constant')\n",
    "    elif len(features) > target_size:\n",
    "        features = features[:target_size]\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Wav2Vec2 feature extraction function\n",
    "def extract_wav2vec_features(segment, sr, target_size=1024):\n",
    "    if len(segment.shape) > 1:   # Ensure the segment is a 1D array and not a 2D array\n",
    "        segment = segment.flatten()  # Flatten it to 1D if needed\n",
    "    \n",
    "    inputs = feature_extractor(segment, sampling_rate=sr, return_tensors=\"pt\", padding=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)  # Extract the last hidden state and return it\n",
    "    wav2vec_features = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    \n",
    "    # Pad or truncate to fixed size (1024)\n",
    "    if len(wav2vec_features) < target_size:\n",
    "        wav2vec_features = np.pad(wav2vec_features, (0, target_size - len(wav2vec_features)), mode='constant')\n",
    "    elif len(wav2vec_features) > target_size:\n",
    "        wav2vec_features = wav2vec_features[:target_size]\n",
    "    \n",
    "    return wav2vec_features\n",
    "\n",
    "# Main processing function\n",
    "def process_dataset(audio_files, labels, utterance_level):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    for audio_file in tqdm(audio_files, desc=\"Processing files\"):\n",
    "        file_id = os.path.basename(audio_file).replace('.wav', '')\n",
    "        if file_id not in labels:\n",
    "            print('no file id exist')\n",
    "            continue\n",
    "\n",
    "        y, sr = librosa.load(audio_file, sr=16000)\n",
    "        y = librosa.util.fix_length(y, size=int(4 * sr))  # Truncate or pad to 4 seconds\n",
    "  # Truncate or pad to 4 seconds\n",
    "        num_frames = int(4 / hop_size)\n",
    "        \n",
    "        if utterance_level:\n",
    "            label = labels[file_id]\n",
    "            for _ in range(num_frames):\n",
    "                handcrafted_features = extract_handcrafted_features(y, sr)\n",
    "                wav2vec_features = extract_wav2vec_features(y, sr)\n",
    "                combined_features = np.concatenate((handcrafted_features, wav2vec_features))\n",
    "                all_features.append(combined_features)\n",
    "                all_labels.append(label)\n",
    "        else:\n",
    "            segment_labels = labels[file_id]\n",
    "            segment_length = int(window_size * sr)\n",
    "            hop_length = int(hop_size * sr)\n",
    "            for i in range(num_frames):\n",
    "                start = i * hop_length\n",
    "                end = start + segment_length\n",
    "                segment = y[start:end]\n",
    "                handcrafted_features = extract_handcrafted_features(segment, sr)\n",
    "                wav2vec_features = extract_wav2vec_features(segment, sr)\n",
    "                combined_features = np.concatenate((handcrafted_features, wav2vec_features))\n",
    "                all_features.append(combined_features)\n",
    "                all_labels.append(segment_labels[i])\n",
    "\n",
    "    return np.array(all_features), np.array(all_labels)\n",
    "\n",
    "# Load labels and process dataset for 100 spoof and 100 real audios\n",
    "labels = load_labels(utterance_label_path if extract_utterance_level else segment_label_path, extract_utterance_level)\n",
    "\n",
    "audio_files = glob(train_audio_path)\n",
    "print(len(audio_files))\n",
    "\n",
    "# Extract file IDs (for matching with labels)\n",
    "audio_file_ids = [os.path.basename(file).replace('.wav', '') for file in audio_files]\n",
    "# print(audio_file_ids[:10])  # Print first 10 file IDs\n",
    "\n",
    "# Now, match the files based on file_id and their labels\n",
    "spoof_files = [file for file in audio_files if os.path.basename(file).replace('.wav', '') in labels and labels[os.path.basename(file).replace('.wav', '')] == 0]\n",
    "print(len(spoof_files))\n",
    "\n",
    "real_files = [file for file in audio_files if os.path.basename(file).replace('.wav', '') in labels and labels[os.path.basename(file).replace('.wav', '')] == 1]\n",
    "print(len(real_files))\n",
    "\n",
    "# Select 100 spoof audios from the start and 100 real audios from the end\n",
    "selected_spoof_files = spoof_files[:50]  # Selecting the first 100 spoof files\n",
    "selected_real_files = real_files[-25:]  # Selecting the last 100 real files\n",
    "\n",
    "\n",
    "# selected_spoof_files = spoof_files  # Use all available spoof files\n",
    "# selected_real_files = real_files  # Use all available real files\n",
    "\n",
    "\n",
    "# Print names and labels of selected audios\n",
    "for audio_file in selected_spoof_files:\n",
    "    file_id = os.path.basename(audio_file).replace('.wav', '')\n",
    "    # print(f\"Selected spoof audio: {file_id} - Label: {labels[file_id]}\")\n",
    "\n",
    "for audio_file in selected_real_files:\n",
    "    file_id = os.path.basename(audio_file).replace('.wav', '')\n",
    "    # print(f\"Selected real audio: {file_id} - Label: {labels[file_id]}\")\n",
    "\n",
    "# Combine the selected files\n",
    "selected_audio_files = selected_spoof_files + selected_real_files\n",
    "\n",
    "# Process the dataset\n",
    "features, labels = process_dataset(selected_audio_files, labels, extract_utterance_level)\n",
    "\n",
    "# Split dataset into train, validation, and test sets (80-10-10 split)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features (important for MLP performance)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train an MLP classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(512, 256), max_iter=300, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = mlp.predict(X_test)\n",
    "y_pred_prob = mlp.predict_proba(X_test)[:, 1]  # Probabilities for ROC AUC and EER\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# EER calculation\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]  # EER is where fpr = 1 - tpr\n",
    "\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"EER: {eer:.4f}\")\n",
    "\n",
    "# Save evaluation results to a .txt file\n",
    "evaluation_file = \"C:\\\\Notebooks\\\\rrl_source\\\\dataset_raw\\\\merge\\\\new\\\\evaluation_results.txt\"\n",
    "with open(evaluation_file, \"w\") as file:\n",
    "    file.write(f\"Confusion Matrix:\\n{cm}\\n\")\n",
    "    file.write(f\"ROC AUC: {roc_auc:.4f}\\n\")\n",
    "    file.write(f\"EER: {eer:.4f}\\n\")\n",
    "\n",
    "# Save frame-level predictions to a .csv file\n",
    "predictions_df = pd.DataFrame({\n",
    "    'File ID': selected_audio_files,\n",
    "    'Prediction': y_pred,\n",
    "    'Prediction Score': y_pred_prob\n",
    "})\n",
    "predictions_file = \"C:\\\\Notebooks\\\\rrl_source\\\\dataset_raw\\\\merge\\\\new\\\\frame_level_predictions.csv\"\n",
    "predictions_df.to_csv(predictions_file, index=False)\n",
    "\n",
    "print(f\"Evaluation results and predictions saved to {evaluation_file} and {predictions_file}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18eb7c5-38ab-4961-aa96-f3c047c225a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c2a789-34b4-40f5-b661-14e700e5034b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "ee8992a9-c62f-4354-a477-55527951d649",
   "metadata": {},
   "source": [
    "For GPU computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd0edefa-7c94-4db4-a2e2-81d7b30287ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25380\n",
      "['CON_T_0000000', 'CON_T_0000001', 'CON_T_0000002', 'CON_T_0000003', 'CON_T_0000004', 'CON_T_0000005', 'CON_T_0000006', 'CON_T_0000007', 'CON_T_0000008', 'CON_T_0000009']\n",
      "Number of spoof files: 20518\n",
      "Number of real files: 4862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|                                                                         | 0/20 [00:00<?, ?it/s]C:\\Users\\mawais\\AppData\\Local\\Temp\\ipykernel_2792\\3999501859.py:56: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(y=windowed_segment, sr=sr)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing at: 0\n",
      "Indexing at: 1\n",
      "Indexing at: 2\n",
      "Indexing at: 3\n",
      "Indexing at: 4\n",
      "Indexing at: 5\n",
      "Indexing at: 6\n",
      "Indexing at: 7\n",
      "Indexing at: 8\n",
      "Indexing at: 9\n",
      "Indexing at: 10\n",
      "Indexing at: 11\n",
      "Indexing at: 12\n",
      "Indexing at: 13\n",
      "Indexing at: 14\n",
      "Indexing at: 15\n",
      "Indexing at: 16\n",
      "Indexing at: 17\n",
      "Indexing at: 18\n",
      "Indexing at: 19\n",
      "Indexing at: 20\n",
      "Indexing at: 21\n",
      "Indexing at: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   5%|███▎                                                             | 1/20 [00:02<00:53,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing at: 23\n",
      "Indexing at: 24\n",
      "Indexing at: 0\n",
      "Indexing at: 1\n",
      "Indexing at: 2\n",
      "Indexing at: 3\n",
      "Indexing at: 4\n",
      "Indexing at: 5\n",
      "Indexing at: 6\n",
      "Indexing at: 7\n",
      "Indexing at: 8\n",
      "Indexing at: 9\n",
      "Indexing at: 10\n",
      "Indexing at: 11\n",
      "Indexing at: 12\n",
      "Indexing at: 13\n",
      "Indexing at: 14\n",
      "Indexing at: 15\n",
      "Indexing at: 16\n",
      "Indexing at: 17\n",
      "Indexing at: 18\n",
      "Indexing at: 19\n",
      "Indexing at: 20\n",
      "Indexing at: 21\n",
      "Indexing at: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  10%|██████▌                                                          | 2/20 [00:05<00:49,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing at: 23\n",
      "Indexing at: 24\n",
      "Indexing at: 0\n",
      "Indexing at: 1\n",
      "Indexing at: 2\n",
      "Indexing at: 3\n",
      "Indexing at: 4\n",
      "Indexing at: 5\n",
      "Indexing at: 6\n",
      "Indexing at: 7\n",
      "Indexing at: 8\n",
      "Indexing at: 9\n",
      "Indexing at: 10\n",
      "Indexing at: 11\n",
      "Indexing at: 12\n",
      "Indexing at: 13\n",
      "Indexing at: 14\n",
      "Indexing at: 15\n",
      "Indexing at: 16\n",
      "Indexing at: 17\n",
      "Indexing at: 18\n",
      "Indexing at: 19\n",
      "Indexing at: 20\n",
      "Indexing at: 21\n",
      "Indexing at: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  10%|██████▌                                                          | 2/20 [00:08<01:16,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing at: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 218\u001b[0m\n\u001b[0;32m    215\u001b[0m selected_audio_files \u001b[38;5;241m=\u001b[39m selected_spoof_files \u001b[38;5;241m+\u001b[39m selected_real_files\n\u001b[0;32m    217\u001b[0m \u001b[38;5;66;03m# Process the dataset\u001b[39;00m\n\u001b[1;32m--> 218\u001b[0m features, labels \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselected_audio_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_utterance_level\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m# Split dataset into train, validation, and test sets (80-10-10 split)\u001b[39;00m\n\u001b[0;32m    221\u001b[0m X_train, X_temp, y_train, y_temp \u001b[38;5;241m=\u001b[39m train_test_split(features, labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 146\u001b[0m, in \u001b[0;36mprocess_dataset\u001b[1;34m(audio_files, labels, utterance_level)\u001b[0m\n\u001b[0;32m    144\u001b[0m combined_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((handcrafted_features, wav2vec_features))\n\u001b[0;32m    145\u001b[0m all_features\u001b[38;5;241m.\u001b[39mappend(combined_features)\n\u001b[1;32m--> 146\u001b[0m all_labels\u001b[38;5;241m.\u001b[39mappend(\u001b[43msegment_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Move print statement inside the loop where `i` is defined\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndexing at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import librosa\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize paths and parameters\n",
    "model_name = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "model = Wav2Vec2Model.from_pretrained(model_name)\n",
    "\n",
    "train_audio_path = r\"F:\\\\Awais_data\\\\Datasets\\\\PartialSpoof\\\\train\\\\con_wav\\\\*.wav\"\n",
    "segment_label_path = r\"F:\\Awais_data\\Datasets\\PartialSpoof\\database_segment_labels\\database\\segment_labels\\train_seglab_0.16.npy\"\n",
    "utterance_label_path = r\"F:\\Awais_data\\Datasets\\PartialSpoof\\protocols\\PartialSpoof_LA_cm_protocols\\PartialSpoof.LA.cm.train.trl.txt\"\n",
    "save_path = r\"C:\\\\Notebooks\\\\rrl_source\\\\dataset_raw\\\\merge\\\\new\\\\\"\n",
    "\n",
    "window_size = 0.16 # Segment length in seconds\n",
    "hop_size = 0.16    # Frame shift in seconds\n",
    "extract_utterance_level = False  # Set to True for utterance-level, False for segment-level\n",
    "\n",
    "# Hann window function\n",
    "def hann_window(length):\n",
    "    return 0.5 * (1 - np.cos(2 * np.pi * np.arange(length) / (length - 1)))\n",
    "\n",
    "# Load segment-level or utterance-level labels\n",
    "def load_labels(label_file, utterance_level):\n",
    "    labels = {}\n",
    "    \n",
    "    if utterance_level:\n",
    "        # For utterance-level labels, we parse the label file and map the class labels to file_ids\n",
    "        with open(label_file, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' ')\n",
    "                if len(parts) >= 5:  # Assuming the line structure is consistent\n",
    "                    file_id = parts[1].strip()  # Extract the file_id\n",
    "                    label = parts[-1].strip()   # Extract the label (e.g., \"spoof\" or \"bonafide\")\n",
    "                    # Assign 0 for spoof, 1 for real/bonafide\n",
    "                    labels[file_id] = 0 if label == 'spoof' else 1\n",
    "    else:\n",
    "        # If working with segment-level labels\n",
    "        labels = np.load(label_file, allow_pickle=True).item()\n",
    "\n",
    "    return labels\n",
    "\n",
    "def extract_handcrafted_features(segment, sr, target_size=60):\n",
    "    windowed_segment = segment * hann_window(len(segment))\n",
    "    mfcc = librosa.feature.mfcc(y=windowed_segment, sr=sr, n_mfcc=13).mean(axis=1)\n",
    "    delta_mfcc = librosa.feature.delta(mfcc)\n",
    "    tempo = librosa.beat.tempo(y=windowed_segment, sr=sr)[0]\n",
    "    chroma = librosa.feature.chroma_stft(y=windowed_segment, sr=sr).mean(axis=1)\n",
    "    zcr = librosa.feature.zero_crossing_rate(windowed_segment).mean()\n",
    "    energy = librosa.feature.rms(y=windowed_segment).mean()\n",
    "    pitches, _ = librosa.core.piptrack(y=windowed_segment, sr=sr)\n",
    "    pitch = np.mean(pitches[pitches > 0]) if len(pitches[pitches > 0]) > 0 else 0\n",
    "    tempogram = librosa.feature.tempogram(y=windowed_segment, sr=sr).mean(axis=1)\n",
    "    downsampled_tempogram = tempogram[::int(np.ceil(len(tempogram) / 18))]\n",
    "    \n",
    "    features = np.concatenate((mfcc, delta_mfcc, [tempo], chroma, [zcr], [energy], [pitch], downsampled_tempogram))\n",
    "    \n",
    "    # Pad or truncate features to the target size\n",
    "    if len(features) < target_size:\n",
    "        features = np.pad(features, (0, target_size - len(features)), mode='constant')\n",
    "    elif len(features) > target_size:\n",
    "        features = features[:target_size]\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Wav2Vec2 feature extraction function\n",
    "def extract_wav2vec_features(segment, sr, target_size=1024):\n",
    "    if len(segment.shape) > 1:   # Ensure the segment is a 1D array and not a 2D array\n",
    "        segment = segment.flatten()  # Flatten it to 1D if needed\n",
    "    \n",
    "    inputs = feature_extractor(segment, sampling_rate=sr, return_tensors=\"pt\", padding=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)  # Extract the last hidden state and return it\n",
    "    wav2vec_features = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    \n",
    "    # Pad or truncate to fixed size (1024)\n",
    "    if len(wav2vec_features) < target_size:\n",
    "        wav2vec_features = np.pad(wav2vec_features, (0, target_size - len(wav2vec_features)), mode='constant')\n",
    "    elif len(wav2vec_features) > target_size:\n",
    "        wav2vec_features = wav2vec_features[:target_size]\n",
    "    \n",
    "    return wav2vec_features\n",
    "\n",
    "# Main processing function\n",
    "def process_dataset(audio_files, labels, utterance_level):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    window_size_samples = int(window_size * 16000)  # Convert segment length to samples\n",
    "    hop_length_samples = int(hop_size * 16000)  # Convert hop size to samples\n",
    "\n",
    "    for audio_file in tqdm(audio_files, desc=\"Processing files\"):\n",
    "        file_id = os.path.basename(audio_file).replace('.wav', '')\n",
    "        if file_id not in labels:\n",
    "            print(f\"No label found for file ID {file_id}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        y, sr = librosa.load(audio_file, sr=16000)\n",
    "        y = librosa.util.fix_length(y, size=int(4 * sr))  # Truncate or pad to 4 seconds (can be adjusted)\n",
    "\n",
    "        num_frames = int(4 / hop_size)  # Total number of frames for 4-second audio\n",
    "\n",
    "        if utterance_level:\n",
    "            # For utterance-level, apply the same label to all frames\n",
    "            label = labels[file_id]\n",
    "            for _ in range(num_frames):\n",
    "                handcrafted_features = extract_handcrafted_features(y, sr)\n",
    "                wav2vec_features = extract_wav2vec_features(y, sr)\n",
    "                combined_features = np.concatenate((handcrafted_features, wav2vec_features))\n",
    "                all_features.append(combined_features)\n",
    "                all_labels.append(label)\n",
    "        else:\n",
    "            # For segment-level processing\n",
    "            segment_labels = labels[file_id]\n",
    "            segment_length = int(window_size * sr)\n",
    "            hop_length = int(hop_size * sr)\n",
    "\n",
    "            # Pad or repeat segment labels to match the required number of frames\n",
    "            if len(segment_labels) < num_frames:\n",
    "                segment_labels = list(segment_labels) + list(segment_labels[:num_frames - len(segment_labels)])\n",
    "            elif len(segment_labels) > num_frames:\n",
    "                segment_labels = segment_labels[:num_frames]\n",
    "\n",
    "            for i in range(num_frames):\n",
    "                start = i * hop_length\n",
    "                end = start + segment_length\n",
    "                segment = y[start:end]\n",
    "\n",
    "                if len(segment) < segment_length:\n",
    "                    # Pad segment if it's shorter than expected\n",
    "                    segment = np.pad(segment, (0, segment_length - len(segment)), mode='constant')\n",
    "\n",
    "                handcrafted_features = extract_handcrafted_features(segment, sr)\n",
    "                wav2vec_features = extract_wav2vec_features(segment, sr)\n",
    "                combined_features = np.concatenate((handcrafted_features, wav2vec_features))\n",
    "                all_features.append(combined_features)\n",
    "                all_labels.append(segment_labels[i])\n",
    "\n",
    "                # Move print statement inside the loop where `i` is defined\n",
    "                print(f\"Indexing at: {i}\")\n",
    "\n",
    "    return np.array(all_features), np.array(all_labels)\n",
    "\n",
    "\n",
    "# Load labels and process dataset for 100 spoof and 100 real audios\n",
    "labels = load_labels(utterance_label_path if extract_utterance_level else segment_label_path, extract_utterance_level)\n",
    "\n",
    "audio_files = glob(train_audio_path)\n",
    "print(len(audio_files))\n",
    "\n",
    "# Extract file IDs (for matching with labels)\n",
    "audio_file_ids = [os.path.basename(file).replace('.wav', '') for file in audio_files]\n",
    "print(audio_file_ids[:10])  # Print first 10 file IDs\n",
    "\n",
    "spoof_files = []\n",
    "real_files = []\n",
    "for file in audio_files:\n",
    "    file_id = os.path.basename(file).replace('.wav', '')\n",
    "    \n",
    "    if file_id not in labels:\n",
    "        print(f\"File ID {file_id} not found in labels.\")\n",
    "        continue\n",
    "    \n",
    "    if extract_utterance_level:\n",
    "        # For utterance-level labels\n",
    "        label = labels[file_id]\n",
    "        if label == 0:\n",
    "            spoof_files.append(file)\n",
    "        elif label == 1:\n",
    "            real_files.append(file)\n",
    "    else:\n",
    "        # For segment-level labels\n",
    "        segment_labels = labels[file_id]\n",
    "        # Here, check for segment-level processing logic\n",
    "        if int(segment_labels[0]) == 0:\n",
    "            spoof_files.append(file)\n",
    "        elif int(segment_labels[0]) == 1:\n",
    "            real_files.append(file)\n",
    "\n",
    "print(f\"Number of spoof files: {len(spoof_files)}\")\n",
    "print(f\"Number of real files: {len(real_files)}\")\n",
    "# selected_spoof_files = spoof_files  # Use all available spoof files\n",
    "# selected_real_files = real_files  # Use all available real files\n",
    "\n",
    "# Select 100 spoof audios from the start and 100 real audios from the end\n",
    "selected_spoof_files = spoof_files[:10]  # Selecting the first 100 spoof files\n",
    "selected_real_files = real_files[-10:]  # Selecting the last 100 real files\n",
    "\n",
    "# Print names and labels of selected audios\n",
    "for audio_file in selected_spoof_files:\n",
    "    file_id = os.path.basename(audio_file).replace('.wav', '')\n",
    "    # print(f\"Selected spoof audio: {file_id} - Label: {labels[file_id]}\")\n",
    "\n",
    "for audio_file in selected_real_files:\n",
    "    file_id = os.path.basename(audio_file).replace('.wav', '')\n",
    "    # print(f\"Selected real audio: {file_id} - Label: {labels[file_id]}\")\n",
    "\n",
    "# Combine the selected files\n",
    "selected_audio_files = selected_spoof_files + selected_real_files\n",
    "\n",
    "# Process the dataset\n",
    "features, labels = process_dataset(selected_audio_files, labels, extract_utterance_level)\n",
    "\n",
    "# Split dataset into train, validation, and test sets (80-10-10 split)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features (important for MLP performance)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train an MLP classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(512, 256), max_iter=300, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = mlp.predict(X_test)\n",
    "y_pred_prob = mlp.predict_proba(X_test)[:, 1]  # Probabilities for ROC AUC and EER\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# EER calculation\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]  # EER is where fpr = 1 - tpr\n",
    "\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"EER: {eer:.4f}\")\n",
    "\n",
    "# Save evaluation results to a .txt file\n",
    "evaluation_file = \"C:\\\\Notebooks\\\\rrl_source\\\\dataset_raw\\\\merge\\\\new\\\\evaluation_results.txt\"\n",
    "with open(evaluation_file, \"w\") as file:\n",
    "    file.write(f\"Confusion Matrix:\\n{cm}\\n\")\n",
    "    file.write(f\"ROC AUC: {roc_auc:.4f}\\n\")\n",
    "    file.write(f\"EER: {eer:.4f}\\n\")\n",
    "\n",
    "# Save frame-level predictions to a .csv file\n",
    "predictions_df = pd.DataFrame({\n",
    "    'File ID': selected_audio_files,\n",
    "    'Prediction': y_pred,\n",
    "    'Prediction Score': y_pred_prob\n",
    "})\n",
    "predictions_file = \"C:\\\\Notebooks\\\\rrl_source\\\\dataset_raw\\\\merge\\\\new\\\\frame_level_predictions.csv\"\n",
    "predictions_df.to_csv(predictions_file, index=False)\n",
    "\n",
    "print(f\"Evaluation results and predictions saved to {evaluation_file} and {predictions_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356850ac-8afa-4bd9-90a7-93d287856942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
